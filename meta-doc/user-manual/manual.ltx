%{{{ LaTeX configuration

% vim:ts=4:sw=4:tw=75
\documentclass[a4paper,twoside]{article}

%\usepackage{fullpage}
\usepackage{times}
\usepackage{a4wide}
\usepackage{verbatim}
\usepackage{makeidx}
\usepackage{epsfig}
\usepackage{alltt}
\usepackage{moreverb}  
\usepackage{url}

\makeindex

\newcommand{\ASmetaenv}{{\sc Asf}+{\sc Sdf} Meta-En\-vir\-on\-ment}
\newcommand{\sdf}{{\sc Sdf}}
\newcommand{\asf}{{\sc Asf}}
\newcommand{\asfsdf}{{\sc Asf}+{\sc Sdf}}
\newcommand{\ATerm}{ATerm}
\newcommand{\ATerms}{ATerms}
\newcommand{\xemacs}{{\tt XEmacs}}

%% notion of Symbol was changed to AFun, which matches specification.
\newcommand{\Symbol}{AFun}

\newcommand{\ATtrue}{\mbox{\tt ATtrue}}
\newcommand{\ATfalse}{\mbox{\tt ATfalse}}
\newcommand{\main}{\mbox{\tt main}}
\newcommand{\ATinit}{\mbox{\tt ATinit}}
\newcommand{\ATprotect}{\mbox{\tt ATprotect}}
\newcommand{\ATprotectArray}{\mbox{\tt ATprotectArray}}

\newcommand{\toolbus}{\mbox{\tt ToolBus}}

% \example{init.c} will input file "examples/init.c" in verbatim mode.
\newcommand{\example}[1]{
    \noindent
    \hrulefill
    \begin{small}
    \verbatiminput{examples/#1}
    \end{small}
    \hrulefill
}

% NULL
\newcommand{\NULL}{{\tt NULL}}               

% Function definition
\newcommand{\Function}[4]{
    \vspace{0.3cm}
    \noindent
    \framebox[12.6cm][l]{{\bf function:} {\tt #1}} \vspace{0.1cm}\newline
    \label{#1}
    \noindent
    {\bf Summary:\hspace{0.2cm}} #4\newline
    {\bf Declaration:}
        {\tt #2} {\tt #1}({\tt #3});
    \index{#1@{\tt #1}}
}
                                        

% Macro definition
\newcommand{\Macro}[4]{
    \vspace{0.3cm}
    \noindent
    \framebox[12.6cm][l]{{\bf macro:} {\tt #1}} \vspace{0.1cm}\newline
    \label{#1}
    \noindent
    {\bf Summary:\hspace{0.2cm}} #4\newline
    {\bf Declaration:}
        {\tt #2} {\tt #1}({\tt #3})
    \index{#1@{\tt #1}}
}
                        

% Method definition
\newcommand{\Method}[4]{
    \vspace{0.3cm}
    \noindent
    \framebox[12.6cm][l]{{\bf method:} {\tt #1}} \vspace{0.1cm}\newline
    \label{#1}
    \noindent
    {\bf Summary:\hspace{0.2cm}} #4\newline
    {\bf Declaration:}
        {\tt #2} {\tt #1}({\tt #3});
    \index{#1@{\tt #1}}
}

% Throws exception (optional)
\newcommand{\Throws}[2]{

\noindent{\bf Throws:} {\tt #1} #2
}                  

% Function/Macro/Method description (optional)
\newcommand{\Describe}[1]{

    \noindent{\bf Description:} #1
}

\newcommand{\fignegspacebefore}{\vspace*{-0.25cm}}
\newcommand{\fignegspacebetween}{\vspace*{-0.25cm}}
\newcommand{\fignegspaceafter}{\vspace*{-0.5cm}}        

\newenvironment{Code}
        {\vspace*{-0.15cm}\begin{small}\begin{center}}
        {\end{center}\end{small}\vspace*{-0.2cm}}

\newenvironment{IncCode}
        {\begin{Code}\begin{small}}
        {\end{small}\end{Code}}      

\def\aterms{\mbox{ATerms}}
\def\aterm{\mbox{ATerm}}
\def\asfix{\mbox{\sc AsFix}}

\newcommand{\traversalprod}[2]{\ensuremath{f(S_1 ,\dots, S_{#1}) \rightarrow #2}}
\newcommand{\transformerprod}[1]{\ensuremath{f(S_1 ,\dots, S_{#1}) \rightarrow S_1}}
\newcommand{\accumulatorprod}[1]{\ensuremath{f(S_1 , S_2 ,\dots, S_{#1}) \rightarrow S_2}}
\newcommand{\combinationprod}[1]{\ensuremath{f(S_1 , S_2 ,\dots, S_{#1}) \rightarrow S_1 \# S_2}}
\newcommand{\accu}[0]{{\tt accu}}
\newcommand{\trafo}[0]{{\tt trafo}}
\newcommand{\accutrafo}[0]{{\tt accu, trafo}}
\newcommand{\traversal}[0]{{\tt traversal}}
\newcommand{\innerm}[0]{{\tt bottom-up}}
\newcommand{\outerm}[0]{{\tt top-down}}
\newcommand{\suppress}[0]{{\tt suppress-syntax-generation}}
\newcommand{\generate}[0]{{\tt generate-syntax}}
%}}}
%{{{ Title page and table of contents

%----[ TITLE PAGE ]----           
\title{\ASmetaenv\ User Manual \\ $Revision$}
\author{M.G.J. van den Brand and P. Klint\\
        {\small\sl Centrum voor Wiskunde en Informatica (CWI),}\\
{\small\sl Kruislaan 413, 1098 SJ Amsterdam, The Netherlands}}

\date{\today}
\begin{document}
\maketitle    

\begin{abstract}
This is a preliminary user manual for the \ASmetaenv\ Release 1.
This is work under construction.
\end{abstract}

\tableofcontents
\newpage

%}}}

%{{{ Overview

%---- [ OVERVIEW ]---- 

\section{Overview}\label{overview} 

\subsection{When to use the \ASmetaenv?}

The \ASmetaenv\ is an interactive development environment
for the automatic generation of interactive systems for manipulating
programs, specifications, or other texts written in a formal
language. The generation process is controlled by a definition of the
target language, which typically includes such features as syntax,
pretty printing, type checking and execution of programs in the target
language. The \ASmetaenv\ can help you if:

\begin{itemize}

  \item You have to write a formal specification for some problem
  and you need interactive support to do this.

  \item You have developed your own (application) language and want to
  create an interactive environment for it.

  \item You have programs in some existing programming language and you
   want to analyze or transform them.
\end{itemize}

The \asfsdf\ formalism allows the definition of syntactic as well as
semantic issues. It can be used for the definition of languages (for
programming, for writing specifications, for querying databases, for
text processing, or for dedicated applications). In addition it can be
used for the formal specification of a wide variety of
problems. \asfsdf\ provides you with:

\begin{itemize}

  \item A general-purpose algebraic specification formalism based on equational logic. 

  \item Modular structuring of specifications.

  \item Integrated definition of lexical, context-free and abstract syntax. 

  \item User-defined syntax, allowing you to write specifications using your own notation. 

  \item Complete integration of the definition of syntax and
  semantics.

\end{itemize}

The \ASmetaenv\ offers: 

\begin{itemize}

  \item Syntax-directed editing of \asfsdf\ specifications.

  \item Incremental compilation and testing of specifications.

  \item Compilation of \asfsdf\ specifications into dedicated
interactive environments containing various tools such as a parser, 
a pretty printer, a syntax-directed editor, a
debugger, and an interpreter or compiler.

\end{itemize}

The advantages of creating interactive environments in this way are twofold:

\begin{itemize}

  \item \emph{Increased uniformity}. Similar tools for different
  languages often suffer from a lack of uniformity.  Generating tools
  from language definitions will result in a large increase in
  uniformity, with corresponding benefits for the user.

  \item \emph{Reduced implementation effort.} Preparing a language
  definition requires significantly less effort than developing an
  environment from scratch.

\end{itemize}

\subsection{Global Structure of the Meta-Environment}

You can create new specifications or modify and test existing ones
using the Meta-Environment. Specifications consist of a series of
modules, and individual modules can be edited by invoking a module
editor. All editing in the Meta-environment is done by creating
instances of a \emph{generic syntax-directed editor}.

After each editing operation on a module its \emph{implementation} is updated
immediately. It consists of a parser, a pretty
printer, and a term rewriting system which are all derived from the
module automatically.

A module can be tested by invoking a \emph{term editor} to create and
evaluate terms defined by the module. Term editors use the syntax of
the module for parsing the textual representation of terms and for
converting them to internal format (abstract syntax trees). The
equations of the module are then used to reduce the terms into normal
form. This result is, in its turn, converted back to textual form by
pretty printing it.

%%The reduction of a term can be monitored by using EDB (Equation
%%Debugger), a debugging system that allows you to reduce a term
%%step-by-step or to place breakpoints for interrupting the reduction
%%process at specific moments.

%%Finally, term editors can be customized by adding \emph{buttons} whose
%%activation starts the evaluation of a function which is defined in the
%%specification. In this way you can customize the user-interface of the
%%application by adding, for instance, a typechecking or evaluation
%%button to a term editor. (** check; not in Release 1.**)

\subsection{About this Manual}

This manual is intended for those courageous users that want to try
out the brand new implementation of the \ASmetaenv. This manual is
still under development and we welcome all feed back and comments.

%%In the sequel we will use \ASmetaenv\ to address the new
%%system and old \ASmetaenv\ when addressing the old system.

The focus of this manual will be on using the system to write a
specification like a type checker or evaluator for the toy language
PICO. It follows the user-interface to explain the capabilities of the
system.  Topics that will be addressed include:
\begin{itemize}
\item How to start the system, and how to exit it.
\item How to create, open and save a specification.
\item How to edit the syntax and/or equations part of a module.
\item How to edit a term.
\item How to evaluate a term.
\item How to compile a specification.
\item How to parse a term outside the \ASmetaenv.
\item How to rewrite a term using a compiled specification outside the
\ASmetaenv.
\item How to unparse parsed and/or normalized terms.
\end{itemize}

\noindent We do \emph{not} explain in detail:

\begin{itemize}
\item The formalism \asfsdf. See, however, Section~\ref{ASF+SDF} for a quick
  introduction to \asfsdf.

\item The architectural and implementational aspects of the system.
You can find a brief sketch in Section~\ref{SEC:TechnologyandArchitecture}.

\item The stand-alone usage of various parts of the system.
Section~\ref{SEC:Components} briefly describes the most important components.
\end{itemize}

\subsection{Downloading the \ASmetaenv}

You can download the \ASmetaenv\ from the following location:
\begin{verbatim}
   http://www.cwi.nl/projects/MetaEnv/
\end{verbatim}

\noindent It provides links to the software as well as to related documents.

\subsection{Further Reading}

There are many publications about the \ASmetaenv\ itself, about the
implementation techniques used, and about applications.  See
Section~\ref{SEC:TechnologyandArchitecture} for an overview of
architecture and implementation techniques.  We give here a brief
overview of selected publications:

\begin{description}

\item{Overviews:} \cite{HKKL85}, \cite{Kli93}, \cite{B*97}, \cite{B*01}.

\item {General ideas:} \cite{HK85b}, \cite{HK00}, \cite{H00}.

\item{ASF:} \cite{BHK89}.

\item{SDF:} \cite{HHKR89}, \cite{Vis97}.

\item{ASF+SDF:} \cite{BHK89}, \cite{DHK96}.

\item{Parser generation and parsing}: \cite{Rek87b}, \cite{HKR90}, \cite{RK91}, \cite{R92}, \cite{Vis97}.

\item{Pretty printing:} \cite{BV96}, \cite{Jon00}.

\item{Rewriting and Compilation:} \cite{BKO99}, \cite{vdBHK00}, \cite{vdBV00},
\cite{BKV01}.

\item{ToolBus}: ~\cite{BK98}.

\item{ATerms:} \cite{BJKO00}.

\item{Applications:} \cite{BDKKM96},\cite{BS00}, \cite{BR00}.

\item{Generic debugging}: 

\item{User manuals:} \cite{ATmanual}, \cite{TBmanual},
\cite{MEmanual}.

\end{description}




\newpage
\section{Starting the System}

The \ASmetaenv\ can be invoked via the command {\tt meta}.  As a
result, the \ASmetaenv\ main window pops up. This is shown in
Figure~\ref{FIG:meta-start}.


The {\tt meta} command has the following options, which may come in handy
later on. As a novice user, you may want to skip the remainder of this section
and continue with Section~\ref{MainWindow}.

\begin{itemize}

\item {\tt -c \emph{dir}} instructs the \asfsdf\ compiler to generate
C files in the directory \emph{dir}. Note that this directory is also
controlled by the environment variable {\tt COMPILER\_OUTPUT}.

\item {\tt -C \emph{file}} specifies the configuration file to be used,
in the default case the configuration file {\tt meta.conf} is used.

\item {\tt -d} starts the \ASmetaenv\ in debug mode. As a result,
an interactive viewer (``ToolBus viewer'') will be started that 
allows the study of the internal behaviour of the system.
The viewer is shown in Figure~\ref{FIG:viewer}.

\item {\tt -h } shows help information for the {\tt meta} command.

\item {\tt -T \emph{port}} controls the communication ports that will
be used for communication between the components of the \ASmetaenv.
Note that these ports are also controlled by the environment variable
{\tt TB\_PORT}.  The default value is {\tt 8999}, but this port may be
in use by someone else (or by an aborted previous run of the
\ASmetaenv). In that case, it is advisable to use other values in the
range 9000 and up.

\item {\tt -v} runs the \ASmetaenv\ in verbose mode.

\item {\tt -V} shows the version number of the \ASmetaenv\ you are running.

\end{itemize}

Search paths can be initialized by creating a file ``meta.conf'' in
the directory from which the {\tt meta} command is initiated.  This
file may contain a list of absolute and/or relative path names (each
on a separate line) that will be searched when opening modules.  For
instance, in the {\tt pico} directory (see Section~\ref{GuidedTour})
you will find an example {\tt meta.conf} file which only contains the
path `{\tt .}', i.e., only the current directory will be searched.

\begin{figure}[tb]

  \centerline{\epsfig{file=meta-start.ps,width=12cm}}
  \caption{\label{FIG:meta-start} Main window of \ASmetaenv}

\end{figure}
\begin{figure}[tb]

  \centerline{\epsfig{file=viewer.ps,width=14cm}}
  \caption{\label{FIG:viewer} ToolBus viewer}

\end{figure}

\newpage
\section{The Main Window} \label{MainWindow}

The main window of the \ASmetaenv\ immediately after starting of the
system was already shown in Figure~\ref{FIG:meta-start}.  After
loading a specification it may look like Figure~\ref{FIG:meta-pico}.
The main window consists of the following parts:

\begin{itemize}
\item At the top of the window is a menu bar that contains the following menus:
  \begin{itemize}

   \item {\tt File}: for creating, opening, saving and printing
    specifications as well as for leaving the \ASmetaenv.
    The {\tt File} menu is described in Section~\ref{FileMenu}.

   \item {\tt Edit}: for setting user preferences.
      The {\tt Edit} menu is described in Section~\ref{EditMenu}.

   \item {\tt Graph}: for setting options for the display of 
        the graph in the import pane (see below).
       The {\tt Graph} menu is described in Section~\ref{GraphMenu}.

   \item {\tt Debug}: for turning on/off the display of diagnostic messages.
    The {\tt Debug} menu is described in Section~\ref{DebugMenu}.

   \item {\tt Help} for various forms of information and online help.
    The {\tt Help} menu is described in Section~\ref{HelpMenu}.

  \end{itemize}
  
\item The \emph{import pane}: A graphical canvas (empty in
  Figure~\ref{FIG:meta-start}, but containing rectangles and arrows in
  Figure~\ref{FIG:meta-pico}) at the left hand side of the window shows
  the import graph of the specification you are editing.  The import pane is
  described in Section~\ref{ImportPane}.
  
\item The \emph{module pane}: a vertical list (empty in
  Figure~\ref{FIG:meta-start}, but containing names like {\tt Layout}, {\tt
    Pico-Booleans}, ...  in Figure~\ref{FIG:meta-pico}) at the right/middle
  part of the window that shows the names of all modules in the current
  specification.  The module pane is described in Section~\ref{ModulePane}.
  
\item The \emph{button pane}: a vertical row of buttons at the right hand side
  of the window containing buttons like {\tt Edit Syntax} and {\tt Edit
    Equations}: they are used for common operations on the
  current specification like editing, saving or deleting a module, or
  compiling the complete specification. The button pane is described in
  Section~\ref{ButtonPane}.
  
\item A \emph{status bar} at the bottom of the window that shows the current
  activity of the system. Examples are: {\tt idle} (the system is doing
  nothing), {\tt parsing} (the system is performing a syntactic analysis of
  some module or term), and {\tt rewriting} (the system is rewriting a term).

\end{itemize}

\begin{figure}[t]
  \centerline{\epsfig{file=meta-pico.ps,width=12cm}}
  \caption{\label{FIG:meta-pico} Main window after loading the Pico specification}
\end{figure}

\newpage
\section{The Menus of the Main Window}
 
\subsection{The File menu} \label{FileMenu}

\begin{figure}[t]
  \centerline{\epsfig{file=file-menu.ps,width=3cm}}
  \caption{\label{FIG:file-menu} {\tt File} menu (main window)}
\end{figure}

The {\tt File} menu is used for creating, opening, saving and printing
specifications as well as for leaving the \ASmetaenv.  It is shown in
Figure \ref{FIG:file-menu}.

{\tt New}, {\tt Open} and {\tt Save} are used for creating
a new module, opening an existing one, respectively, saving a module.
In all three cases a dialog window appears as shown in
Figure~\ref{FIG:open-file}.

{\tt Clear} removes all modules that have been opened so far from the
Meta-Environment. If modules have been modified, you are explicitly
asked to save them. The same effect can be achieved by leaving the
Meta-Environment (using {\tt Quit}, see below) and starting a new
version of the Meta-Environment using the {\tt meta} command.

{\tt Export} allows the export of the import pane in various graphical
formats.  {\tt Print Setup} allows the customization of the command
used for printing.  Its dialog window is shown in
Figure~\ref{FIG:printer-setup}.  {\tt Print} prints the current import
pane.

{\tt Quit} ends the execution of the \ASmetaenv.

\begin{figure}[t]
  \centerline{\epsfig{file=printer-setup.ps,width=3cm}}
  \caption{\label{FIG:printer-setup} Dialog for printer setup}
\end{figure}



\begin{figure}[t]
  \centerline{\epsfig{file=open-file.ps,width=8cm}}
  \caption{\label{FIG:open-file} Dialog for opening a module ({\tt File} menu)}
\end{figure}

\subsection{The Edit menu} \label{EditMenu}

\begin{figure}[t]
  \centerline{\epsfig{file=edit-menu.ps,width=3cm}}
  \caption{\label{FIG:edit-menu} {\tt Edit} menu (main window)}
\end{figure}

The {\tt Edit} menu is used for setting the preferences of a user.
At this moment this functionality is not yet implemented and
this menu is disabled.

\subsection{The Graph Menu} \label{GraphMenu}

\begin{figure}[t]
  \centerline{\epsfig{file=graph-menu.ps,width=4cm}}
  \caption{\label{FIG:graph-menu} {\tt Graph} menu (main window)}
\end{figure}

The {\tt Graph} menu is used for setting options for the display of the graph
in the import pane.  It is shown in Figure \ref{FIG:graph-menu}.


{\tt View All} enlarges the import pane to enable the display of the whole
import graph.

{\tt Autoresize after loading} defines the size of the import pane
after loading a specification. When enabled (check box red), the
import pane will be enlarged to enable the display of the whole import
graph.  When disabled (check box empty), only part of the import graph
will be shown.

{\tt Animate while loading} defines the visualization of the import
graph while the system is loading a specification. When enabled, the
import graph will be updated each time a module from the specification
has been loaded in the system. When disabled, the new import graph
will only be shown when the loading of the specification is complete.


\subsection{The Debug Menu} \label{DebugMenu}

\begin{figure}[t]
  \centerline{\epsfig{file=debug-menu.ps,width=4cm}}
  \caption{\label{FIG:debug-menu}{\tt Debug}  menu (main window)}
\end{figure}

The {\tt Debug} menu is shown in Figure~\ref{FIG:debug-menu} and contains the
single check box {\tt Diagnostic messages}.  When enabled, diagnostic
messages will be displayed.  Otherwise the systems runs silently.

\subsection{The Help Menu} \label{HelpMenu}

\begin{figure}[t]
  \centerline{\epsfig{file=help-menu.ps,width=5cm}}
  \caption{\label{FIG:help-menu}{\tt Help}  menu (main window)}
\end{figure}

The {\tt Help} menu is shown in Figure~\ref{FIG:help-menu}.


{\tt About Meta-Environment} and {\tt About Graph Browser}
give background on the  \ASmetaenv\ and on the graph browser
that is part of the main window.

{\tt Mouse Operations} gives an overview of the meaning of
the available mouse operations.


{\tt Online Help} displays online help files in a {\tt Netscape
Navigator} window.

{\tt JitterBug Activated} opens in an active {\tt Netscape} window a page
to enter a bug report. This offers the users of the \ASmetaenv\ an online
facility to report bugs to the developers.

Both {\tt Online Help} and {\tt JitterBug Activated} assume that there
is a {\tt Netscape} running, other browsers are not yet supported.

\newpage
\section{The Panes of the Main Window}

The two left-most panes of the main window give two, alternative,
views on the same information: the \asfsdf\ specification that has
been loaded into the Meta-Environment.  Using one of these views, the
same set of operations is available either via a pop up menu or via the
buttons in the pane at the right-hand side of the main window.

\subsection{The Import Pane} \label{ImportPane}

The \emph{import pane} gives a graphical view of the specification by
displaying the \emph{import} relation between modules in the form of a
graph. A module $M_1$ imports another module $M_2$ if $M_1$ contains
an import statement of the form {\tt imports $M_2$}.

Each module is represented by a rectangle. An arrow between two
rectangles represents an import relation between the two corresponding
modules. Modules with a special status (e.g., imported by some other
module but never defined, or containing syntax errors) are represented
by a circle.

The import pane has the following interaction facilities:

\begin{itemize}
  
\item Different parts of the import graph can be displayed by using the
  horizontal or vertical scrollbar at the right and at the bottom of the
  import pane.

\item By clicking and holding the middle mouse button outside any
module, the import graph can be dragged across the import pane.

\item Clicking \emph{on} a module yields a pop up menu as shown in
Figure~\ref{FIG:module-menu}.  It contains the same entries as the
button pane of the main window (see Section \ref{ButtonPane}).

\item Clicking \emph{outside} any module yields a pop up menu as shown
in Figure~\ref{FIG:new-open}.  It contains the entries {\tt New} and
{\tt Open} for creating a new and opening an existing module, respectively. 
These two operations are also available from the {\tt File}
menu (see Section~\ref{FileMenu}).

\end{itemize}

\begin{figure}[t]
  \centerline{\epsfig{file=module-menu.ps,width=3cm}}
  \caption{\label{FIG:module-menu}Pop up menu for module operations (import pane)}
\end{figure}

\begin{figure}[t]
  \centerline{\epsfig{file=new-open.ps,width=2cm}}
  \caption{\label{FIG:new-open}Pop up menu for adding a module (import pane)}
\end{figure}

\subsection{The Module Pane} \label{ModulePane}

The import pane is particularly useful when you want to understand the
overall structure of a specification but it may become unwieldy for
very large specifications. For large specifications the module pane
may give you quicker access to the modules in the specification.
It presents a vertical, scrollable, list of the names of all the modules in
the specification.

The main purpose of the module pane is to select one or more modules from
the specification on which an operation from the button pane
(Section~\ref{ButtonPane}) is to be performed.  One module is selected
by clicking on the corresponding module name in the module pane.  More
than one module can be selected by clicking on a module name, keeping
the mouse button down and and then dragging the mouse to the last
desired module name.

After making the selection, an operation can be performed on all the
selected modules by pushing a button from the button pane.  For
instance, pushing the {\tt Edit Syntax} button will create editors for
the syntax of all the selected modules.

\subsection{The Button Pane of the Main Window} \label{ButtonPane}

The entries in the button pane (at the right-hand side of the main
window, see Figure~\ref{FIG:meta-start}) and in the module pop-up menu
(see Figure \ref{FIG:module-menu} and Section~\ref{ImportPane}) are
mostly identical in function, except that the latter may operate on
more than one module while the former operate on a single module only.

We describe them here together. The names of the entries in the
pop-up menu are given in parentheses when they differ from the corresponding button
in the button pane.

First, one or more modules are selected via the import pane or the
module pane. Next, one of the following operations can be applied to
them.

{\tt Edit Syntax}, {\tt Edit Equations},
and {\tt Edit Term} activate (structure) editors for
editing syntax, equations, or terms, respectively, for the selected module(s).
{\tt New Term} allows the creation of a new term (pop-up menu only).

{\tt Save} ({\tt Save Module}) saves the tree representation of the syntax definition
and the equations section of the selected module(s).
{\tt Revert} {\tt Revert Module}) removes modules and re-opens the removed modules, 
the effect is that modifications made to these modules are discarded.
{\tt Delete} ({\tt Delete Module}) removes modules.

{\tt Compile} ({\tt Compile Module...}) invokes the \asfsdf-compiler to generate C code.  If no module
or more than one module has been selected, a warning appears 
(Figure~\ref{FIG:compiler-warning}).  Otherwise, a dialog window appears 
(Figure~\ref{FIG:compiler-dir}) containing the current directory path 
where the output of the compiler
will be written. It can be modified as well.
Pushing the {\tt Cancel} button aborts the compilation. 
Section~\ref{compiling-specs}
gives more details on the compilation of \asfsdf\ specifications.

{\tt Dump} ({\tt Dump Equations...}) dumps the equation of the selected
module and all its imports. This feature is needed in order to run
the evaluator in a stand-alone way, see Section \ref{SEC:interpretationgofterms}.
{\tt Info} ({\tt Module Info}) gives detailed information for the selected modules (not yet implemented).

\begin{figure}[t]
  \centerline{\epsfig{file=compiler-warning.ps,width=4cm}}
  \caption{\label{FIG:compiler-warning}Warning ({\tt Compile module...})}
\end{figure}

\begin{figure}[t]
  \centerline{\epsfig{file=compiler-dir.ps,width=4cm}}
  \caption{\label{FIG:compiler-dir}Dialog for setting directory path ({\tt Compile Module...})}
\end{figure}

\newpage
\section{Editing Specifications}

The editors used to create and modify specifications and terms are
based on \xemacs, so some familiarity with this editor is assumed.
See \url{http://www.xemacs.org/Documentation/index.html}
for documentation on \xemacs.

There are two additions to the standard user-interface of \xemacs:

\begin{itemize}
\item The {\tt Meta-Environment} pull-down menu
(Figure~\ref{FIG:meta-env-menu1}) for applying the \asfsdf\ parser to
the complete text buffer ({\tt Parse}).
For term editors this menu also contains a
({\tt Reduce}) entry, that applies the evaluator to the text in
the editor (Figure~\ref{FIG:meta-env-menu2}).

\item The {\tt Move} pull-down menu (Figure~\ref{FIG:move-menu}) for
structured traversal of the syntax tree of the text in the editor.
Using the entries {\tt Left}, {\tt Righ}t, {\tt Up}, {\tt Down} the
user can navigate in the tree.
\end{itemize}

\begin{figure}[t]
  \centerline{\epsfig{file=move-menu.ps,width=1.2cm}}
  \caption{\label{FIG:move-menu}{\tt Move} menu (editor)}
\end{figure}

\subsection{Editing the Syntax Part of a Module}

An editor for editing the syntax part of a module can be activated by
pressing the {\tt Edit Syntax} button. An example is shown in
Figure~\ref{FIG:Bool-example.sdf}.  Initially the entire text is
highlighted and the message ``{\tt Focus sort is invalid}'' appears
in the status line at the bottom of the main window.

Via the entry {\tt Parse Buffer} in the {\tt Meta-Environment} menu of the
editor, the parser can be activated.  The parser is finished when the
status line in the main window displays ``{\tt Idle}'' again.

\paragraph{Note:} when parsing a large term it may take
some time for the editor to be active again. If the parse was
successful, the bottom line in the \xemacs\ window displays the message
{\tt Focus sort: None}. If the term contains an error, the cursor is
located at the position where the error was detected and the bottom line
in the \xemacs\ window displays the message
{\tt Parse error near cursor}.
Clicking on an arbitrary location in a correctly parsed term sets the focus
and changes the bottom line message.

\begin{figure}[t]
  \centerline{\epsfig{file=meta-env-menu1.ps,width=1.2cm}}
  \caption{\label{FIG:meta-env-menu1} {\tt Meta-Environment} menu (editor)}
\end{figure}

\begin{figure}[t]
  \centerline{\epsfig{file=meta-env-menu2.ps,width=1.6cm}}
  \caption{\label{FIG:meta-env-menu2} {\tt Meta-Environment} menu (term editor)}
\end{figure}

\subsection{Editing the Equations Section of a Module}

An editor for editing the equations section of a module is activated
via the button {\tt Edit Equations}.  An example is shown in Figure
\ref{FIG:Bool-example.asf}.  

The entry {\tt Parse} in the {\tt Meta-Environment} menu of the editor 
activate the parser for the
equations. It is possible that in order to parse the equations, a
parse table must be generated. This is visible through the status
message {\tt Generating Parse Table}.

\subsection{Editing Terms} \label{EditingTerms}

An editor for editing a term over a module is activated via the button
{\tt Edit Term}.  An example is shown in Figure
\ref{FIG:Bool-example.trm},

The entry {\tt Parse} in the {\tt Meta-Environment} menu of the editor 
activate the parser for this
term. It is possible that in order to parse the term, a parse table
must be generated. This is visible through the status message {\tt
Generating Parse Table}.

The entry {\tt Reduce} in the {\tt Meta-Environment} menu of the
editor activates the evaluator\footnote{We will also use interpreter,
rewriter, or reducer instead of evaluator, and equivalently we use 
evaluating, interpreting, rewriting, or reducing of a term}. The term is
reduced given the specified equations (if any). In order to reduce the
term it may be necessary to parse the equations of various modules and
to initialize the evaluator with this set of equations.

\newpage
\section{Executing Specifications} 
\label{ExecutingSpecifications}

Some (but not all!) \asfsdf\ specifications can be executed by interpreting
each equation as a (conditional) rewrite rule from left to right.

When you have created a term (using a term editor, see
Section~\ref{EditingTerms}), you can reduce it (by selecting the {\tt
Reduce} entry from the {\tt Meta-Environment} menu of the term
editor).  As a result, rewrite rules will be applied until a normal
form is reached (a term for which no applicable rule can be
found). This normal form is the result of the execution and is
displayed in a new term window.

The rewriting process is further described in Section~\ref{Equations}.

\newpage
\section{Guided Tour} \label{GuidedTour}

\begin{figure}[t]
  \centerline{\epsfig{file=meta-booleans.ps,width=12cm}}
  \caption{\label{FIG:meta-booleans}Main window after opening {\tt Pico-Booleans}}
\end{figure}

To help you get acquainted with the \ASmetaenv\ the system
contains two example specifications. The first is the one-module specification:
{\tt Bool-example}, and the second is the specification of the syntax, 
typechecker and dynamic semantics of the small programming language Pico.

This Guided Tour is meant to guide you through these specifications, and show
you the main features of the \ASmetaenv. Only global information
is given about these features but references are made to parts of the
user-manual where detailed information can be found.

\subsection{Before you start the Guided Tour}

The Meta-Environment is usually installed in a directory named {\tt
  meta-$version$}. For Release 1, this may, for instance, be {\tt meta-1.0.0}.
You will then find the files needed for this Guided Tour in the directory {\tt
  meta-1.0.0/demo/pico}. It is advisable to make your personal copy of this
directory.  In this Guided Tour we will use `{\tt pico}' to refer to your own
copy of the directory.

For each module in a specification two files exist: `{\tt
  $module$.sdf}' contains the syntax of $module$ and `{\tt
  $module$.asf}' contains the equations (semantics) of $module$. The
  directory {\tt pico} contains:

\begin{itemize}

\item Two files for the module {\tt Bool-example}.

\item Files for the Pico-specification.

\item Three examples of Pico-programs: `{\tt big.pico}', `{\tt fac.pico}',
  `{\tt small.pico}'.

\item Terms for typechecking and evaluating these Pico-programs.

\end{itemize}

\subsection{Beginning the Guided Tour}

\begin{itemize}

\item Go to directory {\tt pico}. 
\item Type the command {\tt meta}. The main window of the Meta-environment
  will appear as shown in Figure~\ref{FIG:meta-start}.
  
\item Add the module {\tt Bool-example} by selecting the File menu, and
  choosing the Open button. In a dialog window, the system asks you to give
  the name of the module to be opened. It presents a list of all files with
  extension `{\tt sdf}'. Click once on `{\tt Bool-example.sdf}' and then
  push the {\tt Open} button. This will load the module {\tt Module-example}
  (both its syntax and equations!) into the system.
  
\item Verify that module {\tt Bool-example} appears as a rectangle in
the import pane as well as in the module pane of the main window as
shown in Figure~\ref{FIG:meta-Bool-example}.

\end{itemize}

\begin{figure}[t]
  \centerline{\epsfig{file=meta-Bool-example.ps,width=12cm}}
  \caption{\label{FIG:meta-Bool-example}Main window after opening {\tt Bool-example}}
\end{figure}

\subsection{The Module Bool-example} \label{Bool-example}

One of the simplest specifications possible, and therefore frequently used as
an example, is the datatype of the Boolean values. It defines the constants
{\tt true} and {\tt false} and the functions \emph{and} and \emph{or}
 (written in infix notation using
the left-associative operators `{\tt \&}' and `{\tt |}', respectively) and
\emph{not} (written
in prefix notation using the function symbol `{\tt not}'). Here is the
specification:

\begin{verbatim}
module Bool-example
  exports
    sorts BOOL 
    lexical syntax
      [\ \t\n]           -> LAYOUT

    context-free syntax
      "true"             -> BOOL
      "false"            -> BOOL 
      BOOL "|" BOOL      -> BOOL {left}
      BOOL "&" BOOL      -> BOOL {left}
      "not" "(" BOOL ")" -> BOOL
      "(" BOOL ")"       -> BOOL {bracket}

  hiddens
    variables
      "Bool"[0-9\']* -> BOOL

    context-free priorities
      BOOL "&" BOOL -> BOOL >
      BOOL "|" BOOL -> BOOL

    equations

    [B1] true | Bool = true 
    [B2] false | Bool = Bool

    [B3] true & Bool = Bool 
    [B4] false & Bool = false

    [B5] not(false) = true 
    [B6] not(true) = false
\end{verbatim}

For an explanation of the \asfsdf\ features used in this specification we
refer to Section~\ref{ASF+SDF}.

\begin{figure}[t]
  \centerline{\epsfig{file=Bool-example.sdf2.ps,width=12cm}}
  \caption{\label{FIG:Bool-example.sdf}Editor for the syntax of {\tt Bool-example}}
\end{figure}

\begin{figure}[t]
  \centerline{\epsfig{file=Bool-example.asf.ps,width=12cm}}
  \caption{\label{FIG:Bool-example.asf}Editor for the equations of {\tt Bool-example}}
\end{figure}

\subsubsection{The Module Editor for {\tt Bool-example}}

\begin{itemize}
  
\item Select module {\tt Bool-example} from the module pane (the vertical list
  of module names that now only contains {\tt Bool-example}) by clicking on it
  once.
  
\item Push the button {\tt Edit Syntax} in the button pane at the right-hand
  side of the main window.  An editor will appear containing the syntax part
  of the {\tt Bool-example} specification: the \sdf\ section.  This editor is
  a version of the standard text editor Emacs extended with the menus {\tt
    Meta-Environment} and {\tt Move}. The result is shown in
  Figure~\ref{FIG:Bool-example.sdf}.
  
\item Push the button {\tt Edit Equations}.  This will reuse the already 
  existing
  instance of Emacs by adding a new buffer to it containing the semantic part
  of the {\tt Bool-example} specification: a list of conditional equations.
  Note that the syntax of the equations is determined by the syntax defined in
  the \sdf\ section. Use the {\tt Buffers} menu of Emacs to see which buffers
  are open and to switch between buffers.
The result is shown in Figure~\ref{FIG:Bool-example.asf}.

\end{itemize}


\begin{figure}[t]
  \centerline{\epsfig{file=Bool-example.trm.ps,width=12cm}}
  \caption{\label{FIG:Bool-example.trm}Term editor for {\tt Bool-example} after entering `{\tt true \& false}'}
\end{figure}

\begin{figure}[t]
  \centerline{\epsfig{file=Bool-example.trm2.ps,width=12cm}}
  \caption{\label{FIG:Bool-example.trm2} Same term editor as in Figure~\ref{FIG:Bool-example.trm} after parse}
\end{figure}

\begin{figure}[t]
  \centerline{\epsfig{file=Bool-example.trm3.ps,width=12cm}}
  \caption{\label{FIG:Bool-example.trm3}Term editor for {\tt Bool-example} after clicking on `{\tt false}'}
\end{figure}

\begin{figure}[t]
  \centerline{\epsfig{file=reduct.out.ps,width=12cm}}
  \caption{\label{FIG:reduct.out}New term editor with normal form of `{\tt true \& false}'}
\end{figure}

\subsubsection{A Term Editor for {\tt Bool-example}}

\begin{itemize}
\item Open a term-editor over module {\tt Bool-example} by first selecting
  module {\tt Bool-example} in the module pane, and then pushing the {\tt
    New Term} button.  A standard dialog window pops up.
    Enter any new filename, for instance, `{\tt my-term}'.

\item 
  Type the term `{\tt true \& false}' in this editor. Observe how the text
  that you type gets a different background color.
  This is called a \emph{focus}. The result is shown in Figure~\ref{FIG:Bool-example.trm}.

\item Click in the focus, this will move the cursor (a single character-sized
  rectangle) 
  
\item From menu {\tt Meta-Environment} click the {\tt Parse Buffer} button.  The text
  in the focus is now parsed and the different background color has 
  disappeared.
  The result is shown in Figure~\ref{FIG:Bool-example.trm2}.

\end{itemize}

All the text\emph{outside} the focus is (by definition) always syntactically
correct. The text \emph{inside} the focus is fresh text which may contain syntax
errors.

\begin{itemize}

\item Click on one of the characters of the word `{\tt false}'.  You
have selected `{\tt false}' as new focus and the blue background
reappears. This is shown in Figure~\ref{FIG:Bool-example.trm3}.

\item Click on the \emph{and} operator `{\tt \&}'.  The whole expression is
  now selected as focus.

\end{itemize}

The movements of the focus are \emph{syntax-directed}: when you click on any
character in the text, the smallest syntactic unit enclosing that character
will be selected and becomes the focus.


\begin{itemize}
  
\item Reduce the term in the term-editor by clicking the {\tt Reduce} button
  in the {\tt Meta-Environment} menu of the editor.  The result will appear in
  a new term editor window (Figure~\ref{FIG:reduct.out}).

\end{itemize}

\paragraph{Error-messages}

\begin{itemize}
  
\item Edit the term `{\tt true \& false}' such that the new term will be
  syntactically incorrect. For instance, type `{\tt true \& wrong}'. Force a
  parse of the term by selecting the {\tt Parse Buffer} button of the 
  {\tt Meta-Environment} menu.
  
  In the status line at the bottom of the edit window a message appears
  `{\tt Parse error near cursor}' and the cursor will be positioned in
  the word `{\tt wrong}'

\end{itemize}

\paragraph{Associativity, Priorities and Brackets}

\begin{itemize}
  
\item Erase the term in your term-editor and type a new term `{\tt true \&
    false \& true}'. 

\item Parse the term using the {\tt Parse Buffer} button.

\item  Try to find out how this term has been parsed by clicking
  on different parts of the term and studying the resulting focus.

\end{itemize}

The {\tt left} attribute in the \sdf\ definition indicates that the `{\tt \&}'
operator is left associative. The term will thus be parsed as `{\tt (true \&
  false) \& true}'. Clicking on the left or right {\tt \&} yields a focus that
corresponds with this parse.

\begin{itemize}
  
\item Erase the term in your term-window and type a new term `{\tt true |
    false \& true}'. 
\item Parse the term using the {\tt Parse Buffer} button.
  
\item Try to find out how this term has been parsed by clicking on various
  parts of the term and studying the resulting focus.

\end{itemize}

The {\tt context-free priorities} definitions in the \sdf\ 
definition state that the `{\tt
  \&}' operator binds stronger than the `{\tt |}' operator.

\begin{itemize}
  
\item Erase the term in the term-editor and type a new term `{\tt true \&
    false}'.  Click on `false', so that the focus is around `false' only. Then
  add `{\tt | true}' after `{\tt false}', so that the resulting term is `{\tt
    true \& false | true}'.  

\item Parse the term.

\item Click on the `{\tt \&}' symbol. Is this what you wanted? Probably not.
\end{itemize}
  
To resolve a priority conflict `{\tt (}' and `{\tt )}' which are defined as
brackets in the \sdf\ definition are put around the term `{\tt false | true}'.
Thus `{\tt true \& (false | true})' is more likely to express what you
intended.

\subsubsection{Modifying {\tt Bool-example}}

The \ASmetaenv\ is an \emph{incremental environment generator}. After
each edit operation on a module, its \emph{implementation} (i.e., scanner, parser and
term rewriting system) is updated immediately.

The editing of both the syntax section and the equations section of a module is
syntax-directed like the editing of terms in a term editor.

\paragraph{Modifying the Equations}

The equations section of a module begins with the keyword {\tt equations}
and is saved in files ending on `{\tt .asf}'.

\begin{itemize}

\item  Click in the equation section to investigate the focus behavior.
  
\item Change the equations, for instance replace in equation {\tt [B1]} the
  last part `{\tt = true}' by `{\tt = false}'. 
  
\item Study the effect on the reduction of terms in the term-editor.

\end{itemize}

\paragraph{Modifying the Syntax}

The syntax part of a module starts with the keyword {\tt module}
and  is saved in files ending on `{\tt .sdf}'.
Modifying the syntax causes the generated scanner and parser to be adapted.
After each edit operation in the \sdf\ section that is followed by a parse of
the \sdf\ section, the focus in both the equations section and the term editor
is extended to completely contain the text in these editors.

Modifying the context-free syntax:

\begin{itemize}
  
\item Change the syntax of the defined functions. E.g, replace `{\tt not}' by
  `{\tt negation}'.

\item Try to re-parse the equations. 
  
%%\item Investigate the expand menu in the term-editor. (type `{\tt <BOOL>}' and
%%  select {\tt expand}.)

\end{itemize}

Modifying the priorities: 

\begin{itemize}

\item Remove the priority declaration. 
  
\item Type the term `{\tt true \& false | true}' in the term-editor (or
  anything similar according to your current syntax). Try to parse this term.

\item Add the priority declarations again.

\end{itemize}

Modifying layout in the lexical section: 

\begin{itemize}

\item Remove the lexical syntax with the {\tt LAYOUT} definitions. 

\item Try parsing equations of {\tt Bool-example}.

\end{itemize}

\paragraph{Note:} Omitting the {\tt LAYOUT} definition is one of most common
errors made when writing a new specification; always make sure your syntax
definitions define at least spaces and newlines to be {\tt LAYOUT}.

\begin{itemize}
  
\item End the editing of your term and the module {\tt Bool-example} by
  selecting the {\tt Exit XEmacs} from the {\tt File} menu of the editor.  
  If you like, you can save the changes to module Bool-example.

\end{itemize}

If you save the changes the files `{\tt Bool-example.sdf}' and `{\tt
  Bool-example.asf}' will be modified.


\begin{itemize}
  
\item Delete the module {\tt Bool-example} from the specification.  Click on
  {\tt Bool-example} in the module pane and push the {\tt Delete} button. The
  module remains known to the system and its graphical representation is
  changed form a rectangle to an ellipse. It can be re-opened later on.
  
\item Leave the system by pushing the {\tt Quit} entry in the {\tt File} menu
  of the main window of the \ASmetaenv.

\end{itemize}

\begin{figure}[t]

  \centerline{\epsfig{file=meta-Pico-syntax.ps,width=12cm}}
  \caption{\label{FIG:Pico-syntax} Main window after opening {\tt Pico-syntax}}

\end{figure}

\subsection{The Pico Specification}

More features of the  \ASmetaenv\  can be studied by looking at the
Pico specification.


\begin{figure}[t]

  \centerline{\epsfig{file=Pico-Booleans.sdf2.ps,width=12cm}}
  \caption{\label{FIG:Pico-Booleans-sdf}Editor for syntax of {\tt Pico-Booleans}}

\end{figure}


\begin{figure}[t]

  \centerline{\epsfig{file=Pico-Booleans.asf.ps,width=12cm}}
  \caption{\label{FIG:Pico-Booleans-eqs}Editor for equations of {\tt Pico-Booleans}}

\end{figure}


\begin{itemize}

\item Open the module {\tt Pico-Booleans} and study the differences in
syntax and equations with {\tt Bool-example}. 

\end{itemize}

Editors for syntax and equations of {\tt Pico-Booleans} are show in
Figures \ref{FIG:Pico-Booleans-sdf} and \ref{FIG:Pico-Booleans-eqs}.


\begin{itemize}
  
\item Start the Meta-Environment: go to directory {\tt
    pico}, and type `{\tt meta}'.
  
\item Add the module {\tt Pico-syntax} by selecting the {\tt File} menu, and
  selecting the {\tt Open...} button.  In the dialog window that appears, click
  on {\tt Pico-syntax.sdf} and push the {\tt Open} button.
  
\item As you can see in both the import pane and the module pane, not only
  {\tt Pico-syntax} has been added, but also all modules that are directly or
  transitively imported by {\tt Pico-syntax}. See Figure~\ref{FIG:Pico-syntax}.

\end{itemize}

\subsubsection{The Module Editor for Pico-syntax}

\begin{itemize}
  
\item Open an editor for the syntax of {\tt Pico-syntax} (using the {\tt Edit
    Syntax} button).

\end{itemize}
  
  A Pico program consists of the word `{\tt begin}', a declaration section, a
  series of zero or more statements, and the word `{\tt end}'. The declaration
  section consists of the word `{\tt declare}', a list of zero or more tuples
  `{\em identifier} {\tt :} {\em type}' and a semi-colon `{\tt ;}'. 
  Types are `{\tt string}' and `{\tt natural}'.  
  There are three kinds of statements:
  assignments, if-then-else statements and while-loops. The Pico language
  has also expressions for adding and subtracting natural numbers and
  for concatenating strings.

\paragraph{Notes:}

\begin{itemize}
  
\item A module {\tt Layout} is imported, in which the 
sort {\tt LAYOUT} has been specified.
  
\item In the context-free section the list constructs 
`{\tt \{ID-TYPE ","\}*}' and `{\tt \{STATEMENT ","\}*}' are used. 
In fact, the sort {\tt SERIES}
  could have been left out entirely and be replaced by 
`{\tt \{STATEMENT ","\}*}' all through the specification. 
{\tt SERIES} is only used for
  abbreviation in the syntax rules. Also, variables over list constructs are
  declared.

\item  The Pico-syntax module contains no equations.

\end{itemize}

\subsubsection{A Term Editor for Pico-syntax}

\begin{itemize}
  
\item Open a term-editor for the Pico-program `{\tt small.pico}': select {\tt
    Pico-syntax} in the module pane and push the {\tt term} button in the
  button pane. A dialog window pops up and type `{\tt small.pico}' as name of
  the term.

\item Press the {\tt Parse Buffer} button in the {\tt Meta-Environment} menu of the
  editor. As a result, {\tt small.pico} is parsed.

\item Press the {\tt Reduce} button in the {\tt Meta-Environment} menu of the
  editor.

\end{itemize}

This has the following effects:

\begin{itemize}

\item  The term in the editor is parsed. 
  
\item All the equations that are valid for this editor are parsed and compiled
  into a rewrite system.  In this case that means the equations of the
  imported modules {\tt Pico-Booleans}, {\tt Pico-Integers}, {\tt Pico-Strings} and {\tt
    Pico-Types}. This takes some time.
  
\item The term in the editor is reduced. As no equation can be applied to
  reduce this term, the term itself is returned in the shell window from which
  the Meta-Environment has been started.
\end{itemize}

  
Reducing a term for the second time is notably faster: the equations have been
processed already. If you are curious what is going, have a look at the
status field at the bottom of the main window. It reveals the steps that
are necessary to arrive at a specification which can be interpreted.

\begin{itemize}
\item Verify this by pushing the {\tt Reduce} button once more.
\end{itemize}

\begin{itemize}

\item Load the module {\tt Pico-typecheck} in the Meta-Environment.

\item Open a term editor for the term {\tt smalltc.pico}.

\item Reduce this term.

\end{itemize}

This has the following effects:

\begin{itemize}

\item  The module {\tt Pico-typecheck} is added to the specification. 
  
\item The term in {\tt smalltc.pico} is identical to the one in 
  {\tt small.pico}, except that the program has been surrounded by `{\tt tcp(}'
  and `{\tt )}'.  The function {\tt tcp} (for type check program), applies the
  typing rules for the Pico language to its single argument: a complete Pico
  program. The result is {\tt true} or {\tt false}, depending on whether
  the Pico program is properly typed.
  
\item All equations of {\tt Pico-typecheck} and its imported
  modules are being compiled.
  
\item The term {\tt smalltc.pico} is reduced using the equations of {\tt
    Pico-typecheck}.

\end{itemize}
  
Typechecking a term for the second time is notably faster, the
modules have been added already and the equations have been compiled.

\begin{itemize}

\item Verify this, by pushing {\tt Reduce} once more.
  
\item Make some modifications to `{\tt smalltc.pico}' in the term-editor.
  Typecheck the modified program.
  
\item Open term-editors with other pico programs (`{\tt fac.pico}', `{\tt
    big.pico}') or create your own program. The corresponding applications of
  the type check function are in `{\tt factc.pico}' and `{\tt bigtc.pico}', 
  respectively.  Typecheck these programs.

\end{itemize}

The evaluation of Pico programs is achieved in a similar fashion. 
The evaluation rules are defined in the module `{\tt
  Pico-eval}'. Applications of the evaluation function `{\tt evp}' can be
found in `{\tt smallev.pico}', `{\tt facev.pico}', and `{\tt bigev.pico}.

\begin{itemize}
  
\item Repeat the steps described above for typechecking, now for the
  evaluation of Pico programs.

\end{itemize}

\subsubsection{More Exercises to Study the Pico Specification}

\begin{itemize}

\item Study other modules in the specification. The modules {\tt
    Pico-typecheck} and {\tt  Pico-eval} are explained in the next sections.
  
\item Add a repeat statement `{\tt repeat SERIES until EXP}' to {\tt
    Pico-syntax}, add typecheck equations to {\tt Pico-typecheck}, and
  eval-equations to {\tt Pico-eval}, for this new statement.

\item Add your own module to the specification. 

\item Make your own specification. Create a new directory for each
  specification.

\end{itemize}

\subsubsection{Module Pico-typecheck}

\begin{itemize}

\item Open an editor for the syntax of {\tt Pico-typecheck}.

\end{itemize}

The function `{\tt tcp}' is defined for typechecking Pico-programs.
Variants of this function exist for typechecking various parts of a Pico program. 
The
typechecking of the declarations yields a type-environment: a table of
identifiers and their types. This type-environment, and the `{\tt lookup}'
function is specified in the module {\tt Type-environments}. The typechecking
of statements uses a type-environment and yields a Boolean value.


\begin{itemize}

\item Open an editor for the equations of {\tt Pico-typecheck}.

\end{itemize}

The equations define how a Pico-program is typechecked. Equation {\tt
  [Tc1]} says that the typechecking of a program is `{\tt true}' if the
typechecking of the Series in the type-environments, `{\tt tcd(Decls)}', is
`{\tt true}'.

Equations {\tt [Tc2]} and {\tt [Tc3]} specify how a type-environment is
constructed, when the declarations are typechecked.

Equations {\tt [Tc3a]} and {\tt [Tc3b]} specify the typechecking of a,
possibly empty, list of statements. Equations {\tt [Tc4a]} through {\tt
  [default-Tc6]} specify how the three kinds of Statements are typechecked
using the information from the type-environment.

The rest of the equations deal with the typechecking of expressions.

\subsubsection{Module Pico-eval}

\begin{itemize}

\item Open an editor for the syntax of {\tt Pico-eval}.

\end{itemize}

The functions `{\tt evp}' and variants are defined for describing the dynamic semantics of
Pico. The result of evaluation is a value-environment: a table of identifiers
and values with the final values of the declared identifiers. (Note that Pico
does not have an output-statement.)


\begin{itemize}

\item Open an editor for the equations of {\tt Pico-eval}.

\end{itemize}

The equations define how a program is evaluated. Equation {\tt [Ev1]}
says that the evaluation of a program is the evaluation of the Series in the
value-environments, `{\tt evs(Decls)}'.

Equations {\tt [Ev2]} thorugh {\tt [Ev3c]}  specify how a
value-environment is constructed, when the declarations are evaluated.
Identifiers of type `{\tt natural}' get value `{\tt 0}', Identifiers of type
`{\tt string}' get value `{\tt ""}' (the empty-string).

Equations {\tt [Ev4a]} and {\tt [Ev4b]} specify the evaluation of a, possibly
empty, list of statements. Equations {\tt [Ev5a]} through {\tt [Ev5e]}
specify how the three kinds of statements are
evaluated using the information from the type-environment. Evaluating
statements means updating the value-environment.

The rest of the equations deal with the evaluation of expressions. Evaluating
an expression results in a value.

\newpage
\section{An Introduction to \asfsdf} \label{ASF+SDF}

\asfsdf\ is the result of the marriage of two formalisms \asf\ (Algebraic
Specification Formalism) and \sdf\ (Syntax Definition Formalism).  \asf\ is
based on the notion of a module consisting of a signature defining the
abstract syntax of functions and a set of conditional equations defining their
semantics. Modules can be imported in other modules. 
\sdf\ allows the simultaneous definition of concrete
(i.e., lexical and context-free) and abstract syntax and implicitly defines a
translation from text strings to abstract syntax trees.

The main idea of \asfsdf\ is to identify the abstract syntax defined by the
signature in \asf\ specifications with the abstract syntax defined 
implicitly by an \sdf\ specification, thus yielding a standard mapping 
from text to abstract syntax tree. This allows the 
association of semantics with (the tree representation of) text and 
introduces user-defined notation in specifications.

\asfsdf\ is therefore a modular specification formalism for the integrated
definition of syntax and semantics.

\subsection{Modules and Modular Structure}
\label{modules}

An \asfsdf\ specification consists of a sequence of module declarations. Each
module may define syntax rules as well as semantic rules and the notation used
in the semantic rules depends on the definition of syntax rules. The entities
declared in a module may be visible or invisible to other modules. A module
can use another module from the specification by importing it. As a result,
all visible names of the imported module become available in the importing
module.

The overall structure of a module is:

\begin{verbatim}
module <ModuleName>

  <ImportSection>*
 
  <ExportOrHiddenSection>*

equations
  <ConditionalEquation>*
\end{verbatim}

A module consists of a module header, followed by a list of zero or more
import sections, followed by zero or more hidden or exported
sections and an optional equations section that defines conditional equations.

Conceptually, a module is a single unit but for technical reasons the
syntax sections and the equations section are stored in physically
separate files.  For each module $M$ in a specification two files
exist: `{\tt $M$.sdf}' contains the syntax sections of $M$ and `{\tt
$M$.asf}' contains the equations section of $M$.

An {\tt <ExportOrHiddenSection>} is either an \emph{export section} or a
\emph{hidden section}. The former starts with the keyword {\tt exports} and
makes all entities in the section visible to other modules.  The latter starts
with the keyword {\tt hiddens} and makes all entities in the section local to
the module.

An {\tt <ExportOrHiddenSection>} has thus one of the two forms:

\begin{verbatim}
exports                               hiddens
  <Grammar>+                            <Grammar>+
\end{verbatim}
  
\noindent A {\tt <Grammar>} 
can be a definition of one of the following: 
\begin{itemize}
\item Imports (Section~\ref{Imports}).
\item Aliases (Section~\ref{Aliases}).
\item Sorts (Section~\ref{Sorts}).
\item Lexical syntax (Section~\ref{LexicalSyntax}).
\item Context-free syntax (Section~\ref{ContexFreeSyntax}).
%\item lexical variables;
%\item lexical priorities;
\item Priorities (Section~\ref{Priorities}).
\item Variables (Section~\ref{Variables}).
\end{itemize}

There are a number of related properties which have an effect across the
various grammar items, these items are intermixed with the discussion
of the grammars:
\begin{itemize}
\item Symbols (Section~\ref{Symbols}).
\item Attributes of Lexical and Context-free Functions (Section~\ref{Attributes}).
\item Disambiguation (Section~\ref{Disambiguation}).
\item Equations (Section~\ref{Equations}).
\end{itemize}

\noindent Each of these entities and properties
will now be described and illustrated by examples.

\subsection{Imports} \label{Imports}

Each {\tt <ImportSection>} starts with the keyword {\tt imports} followed
by one or more module names:

\begin{verbatim}
imports
  <ModuleName>+
\end{verbatim}

\begin{sloppypar}
\noindent Modules can be combined by importing one module in another. 
Imports can occur as {\tt <ImportSection>} at the topmost level of a module
or they can occur within an exports or hiddens section.
\end{sloppypar}

When importing modules at the topmost level of a module or when the import
section occurs within the scope of an exports keyword, all exported entities
of the imported module (and of all modules that are imported indirectly by it)
become available in the importing module. In addition, they are also exported
by the importing module.  However, if the import section occurs within the
scope of a hiddens keyword, the exported entities are only visible in the
importing module but they are not exported by the importing module.

\subsection{Symbols} \label{Symbols}

The elementary building block of \sdf\ syntax rules is the ``symbol''.  It is
comparable to terminals and non-terminals in other grammar definition
formalisms. The elementary symbols are:

\begin{itemize}
\item \emph{sort}: corresponds to a non-terminal, e.g., {\tt Bool}.  Sort
  names always start with a capital letter and may be followed by letters
  and/or digits.  Hyphens (``{\tt -}'') may be embedded in a sort name.

\item \emph{literal}: corresponds to a terminal, e.g.,  {\tt true} or  {\tt
    "\&"}.
  
\item \emph{character class}: corresponds to a set of characters, 
e.g., {\tt [a-z]}.
Character classes will be explained in
  Section~\ref{CharacterClasses}, they are mainly used when describing the
  lexical syntax of a language.
\end{itemize}

Starting with the elementary symbols, more complex symbols can be constructed
by way of operators that we will discuss now.
Examples of the use of the various operators will be given in the
Sections~\ref{LexicalSyntax} and~\ref{ContexFreeSyntax}.

\subsubsection{Option} \label{OptionOperator}

The postfix option operator {\tt ?} describes an optional part in a syntax
rule. For instance, {\tt ElsePart?}  defines zero or exactly one occurrence of
{\tt ElsePart}.

\subsubsection{Sequence} \label{SequenceOperator}

The sequence operator {\tt (...)} describes the grouping of two or more
symbols, e.g., {\tt (Bool "\&")}.  Sequences are mostly used to group symbols
together to form a more complex symbol using one of the available operators,
e.g., {\tt (Bool "\&")*}.  It has no effect to construct a sequence consisting
of a single symbol.  The empty sequence is represented as {\tt ()}.

\subsubsection{Repetition} \label{RepetitionOperator}

Repetition operators express that a symbol should occur several times.  In
this way it is possible to construct flat lists and therefore we usually refer
to repetitions as \emph{lists}.

Repetition operators come in two flavors, with and without separators.
Furthermore, it is possible to express the minimal number of repetitions of
the symbol: at least zero times ({\tt *}), at least one time ({\tt +}), or at
least $n$ times ({\tt n+}). Examples are:

\begin{itemize}

\item {\tt Bool*} (a list of zero or more {\tt Bool}s).

\item {\tt \{Bool ","\}+} (a list of one or more {\tt Bool}s separated by comma's).

\item {\tt Bool3+} (a list of at least 3 {\tt Bool}s).

\end{itemize}

Both the element and the separator can be an arbitrary symbol.  To avoid
confusion it is, however, advisable that the element and the separator are not
the same symbol, e.g., {\tt \{Bool Bool\}*}.

\subsubsection{Alternative} \label{AlternativeOperator}

The alternative operator {\tt |} expresses the choice between two symbols,
e.g., {\tt true | false} represents that either a {\tt true} symbol or a {\tt
  false} symbol may occur here.  The alternative operator is right associative
and binds stronger than any other operator on symbols. This is important
because {\tt Bool "," | Bool ";"} expresses {\tt Bool ("," | Bool) ";"}
instead of {\tt (Bool ",") | (Bool ";")}. So, in case of doubt use the
sequence operator in combination with the alternative operator.

\subsubsection{Tuple} \label{TupleOperator}

The tuple operator {\tt \#} describes the grouping of a sequence of symbols of
a fixed length into a tuple. The notation for tuples is {\tt < , , >}, i.e., a
comma-separated list of elements enclosed in angle brackets.  For example,
{\tt Bool \# Int \# Id} describes a tuple with three elements consisting of a
{\tt Bool}, an {\tt Int} and an {\tt Id} (in that order).  For instance, {\tt
  <true, 3, x>} is a valid example of such a tuple.  The tuple operator is
right associative and binds weaker than the alternative operator.

\subsubsection{Permutation} \label{PermutationOperator}

The permutation operator {\tt <<...>>} describes an arbitrary permutation of
a given sequence of symbols.  For example, {\tt <<Bool Int Id>>} represents a
permutation of the symbols {\tt Bool}, {\tt Int}, and {\tt Id}.  Valid
instantiations are, for instance, {\tt true 3 x} and {\tt x 3 true}.  There
are six valid instantiations for this permutation.

\subsubsection{Set} \label{SetOperator}

The set operator {\tt Set[...]} is an abbreviation for defining a set of
elements of a given symbol.  For example, {\tt Set[Int]} abbreviates the
definition {\tt "\{" \{Int ","\}* "\}"}. As a consequnce, {\tt \{1, 2, 3\}} is
a valid instantiation for the given definition.

\subsubsection{Function} \label{FunctionOperator}

The function operator {\tt (...=>...)} allows the definition of function
types. Left of {\tt =>} zero or more symbols may occur, right of {\tt =>}
exactly one symbol may occur.  For example, {\tt (Bool Int) => Int} represents
a function with two argument (of types {\tt Bool} and {\tt Int}, respectively)
and a result type {\tt Int}.

\subsection{Aliases} \label{Aliases}

In ordinary programming it is good practice to use named constants to
represent literals or constant values.  In \sdf\ it is good practice to give a
name (``alias'') to complicated symbols that occur repeatedly in the
specification.  An alias is thus a named abbreviation for a complicated
symbol.  For example,

\begin{verbatim}
aliases
  Bool # Int # Id -> Tuple3
\end{verbatim}
introduces the alias {\tt Tuple3} for the symbol {\tt Bool \# Int \# Id} and
instead of using {\tt Bool \# Int \# Id} one can use the alias {\tt Tuple3}.
During parse table generation the alias is replaced by the actual symbol.  It
is not allowed to give an alias for an alias or to redefine aliases.
For example, the following definitions are illegal:

\begin{verbatim}
aliases
  Tuple3          -> SuperTuple
  Bool # Int # Id -> Tuple3
\end{verbatim}
(An alias is defined for the alias {\tt Tuple3}.)

\begin{verbatim}
aliases
  Bool # Int # Id -> Tuple3
  Bool # Int      -> Tuple3
\end{verbatim}
(The alias {\tt Tuple3} is redefined.)

\subsection{Sorts} \label{Sorts}

Sorts are declared by listing their name in a sorts section of the form:

\begin{verbatim}
sorts
  <SortName>*
\end{verbatim}

\noindent Recall that a sort name should start with a capital letter and may be followed by 
letters and/or digits. Hyphens (`{\tt -}') may be embedded in sort names.
There is one predefined sort name ({\tt LAYOUT}). It is described in
Section~\ref{LexicalSyntax}.

In fact, sort declarations have a more liberal form and may contain
arbitrary symbols rather than just sort names:

\begin{verbatim}
sorts
  <Symbol>*
\end{verbatim} 

It is required that all sorts that occur in some symbol in the specification
are declared. This has also the effect that input sentences corresponding to
these sorts can be parsed. Technically speaking each sort can act as start
symbol for the underlying grammar. This is not true for arbitrary symbols.
However, declaring an arbitrary symbol has the effect that that symbol can
also act as start symbol.

\subsection{Lexical Syntax} \label{LexicalSyntax}
\label{lexical-syntax}

The lexical syntax describes the low level structure of text by means of
\emph{lexical tokens}. A lexical token consists of a sort name (used to
distinguish classes of tokens like identifiers and numbers), and the actual text
of the token. The lexical syntax also defines which substrings of the text are
layout symbols or comments and are to be skipped.

A lexical syntax contains a set of declarations for \emph{lexical functions},
each consisting of a regular expression and a result sort. All functions with
the same result sort together define the lexical syntax of tokens of that
sort. Regular expressions may contain any basic symbol and any symbol
operator as described in Section~\ref{Symbols}. 
Spaces are only significant inside strings and character classes.

The sort name {\tt LAYOUT} is predefined and may not be redeclared.  
{\tt LAYOUT} defines which parts of the text are \emph{layout symbols} (also
known as \emph{white space}) between lexical tokens and are to be skipped
during lexical analysis. It may only be used as result sort of lexical
functions (Section~\ref{LexicalFunctions}). When a string is matched by both a
LAYOUT function and by other non-LAYOUT functions, then the interpretation as
layout symbol is ignored. {\tt LAYOUT} is typically used for defining layout
and comment conventions.

Traditionally, lexical syntax and context-free syntax are treated
differently.  They are defined by different notations and implemented
by means of different techniques. \sdf\ provides a much more uniform
treatment.  In \sdf, the only significant difference between the two
is that no layout will be accepted while recognizing the members of
the left-hand side of a lexical function, whereas layout \emph{will}
be accepted between the members of the left-hand side of a
context-free function. At the implementation level, both are
implemented using a single parsing technique.

Technically, there exist only \emph{syntax} sections. Both lexical
syntax sections and context-free syntax sections are transformed into
such syntax sections after appropriate insertion of optional layout
between the elements of context-free functions. In rare cases,
the specification writer may want to control this process explicitly
and write syntax sections directly. This will not be discussed
in this manual, but further details can be found in \cite{Vis97}.

\subsubsection{Lexical Functions} \label{LexicalFunctions}

In their simplest form, declarations of lexical functions consist of a
sequence of zero or more symbols followed by `$\rightarrow$' and a
result symbol, say $L$.  A lexical function may be followed by a list
of attributes. The regular expression associated with $L$ consists of the
logical \emph{or} of all left-hand sides of lexical functions with result sort
$L$.  All sort names appearing in left-hand sides of declarations are replaced
by the regular expression associated with them. 
%%Circular dependencies between declarations in the lexical syntax are 
%%forbidden.\footnote{Does this limitation still apply?}  
Figure \ref{CODE:simplelex} shows an example of a
simple lexical function definition for defining the first three words that
Dutch children learn to read.  The three sorts {\tt Aap}, {\tt Noot} and {\tt
  Mies}, each recognize, respectively, the strings {\tt aap}, {\tt noot} and
{\tt mies}. The sort {\tt LeesPlank} (a reading-desk used in primary
education) recognizes the single string {\tt aapnootmies}.


\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module LeesPlank

imports Layout

exports
  sorts Aap Noot Mies
  lexical syntax
    "aap"         -> Aap
    "noot"        -> Noot
    "mies"        -> Mies
    Aap Noot Mies -> LeesPlank  
\end{boxedverbatim}
\end{IncCode}
\caption{Simple lexical functions}\label{CODE:simplelex}
\end{figure}   
      

\paragraph{Lexical constructor functions}
For each sort {\tt L} that appears as result sort in the lexical syntax a lexical
constructor function of the form {\tt l "(" CHAR+ ")" -> L} is automatically
added to the context-free syntax of the specification.  Here, `{\tt l}' is the
name of sort `{\tt L}' written in lower-case letters.  In this way, you can
get access to the characters of lexical tokens.

\subsubsection{Character Classes}\label{CharacterClasses}

Enumerations of characters occur frequently in lexical definitions. They can
be abbreviated by using character classes enclosed by `{\tt [}' and 
`{\tt ]}'. 
A character class contains a list of zero or more characters (which
stand for themselves) or character ranges such as, for instance, {\tt [0-9]}
as an abbreviation for the characters {\tt 0}, {\tt 1}, ..., {\tt 9}. 
In a character range of the form {\tt $c_1$-$c_2$} one of the following 
restrictions should apply:

\begin{itemize}
\item $c_1$ and $c_2$ are both lower-case letters and $c_2$ follows $c_1$ in
  the alphabet, or 
\item $c_1$ and $c_2$ are both upper-case letters and $c_2$ follows $c_1$ in
  the alphabet, or 
\item $c_1$ and $c_2$ are both digits and the numeric value of $c_2$ is
  greater than that of $c_1$, or 
\item $c_1$ and $c_2$ are both escaped non-printable characters and the 
character code of $c_2$ is greater than that of $c_1$.
\end{itemize}

Definitions for lower-case letter ({\tt LCLetter}), upper-case letters
({\tt UCLetter}), lower-case and upper-case letters ({\tt Letter}) and
digits ({\tt Digit}) are shown in Figure~\ref{CODE:LettersDigits1}.
Figure~\ref{CODE:LettersDigits2} gives
a definition of the sort {\tt LetterOrDigit} that recognizes a single letter
(upper-case or lower-case) or digit.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module LettersDigits1

imports Layout

exports
  sorts LCLetter UCLetter Letter Digit
  lexical syntax
    [a-z]    -> LCLetter
    [A-Z]    -> UCLetter
    [a-zA-Z] -> Letter
    [0-9]    -> Digit
\end{boxedverbatim}
\end{IncCode}
\caption{Defining letter (lower-case and upper-case) and digit}\label{CODE:LettersDigits1}
\end{figure}   

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module LettersDigits2

imports Layout

exports
  sorts LetterOrDigit
  lexical syntax
    [a-z]    -> LetterOrDigit
    [A-Z]    -> LetterOrDigit
    [0-9]    -> LetterOrDigit
\end{boxedverbatim}
\end{IncCode}
\caption{Defining a single letter or digit}\label{CODE:LettersDigits2}
\end{figure}   

\paragraph{Escape Conventions}

Characters with a special meaning in \asfsdf\ may cause problems when they are
needed as ordinary characters in the lexical syntax. The backslash character
(`{\tt \verb+\+}') is used as escape character for 
the quoting of special characters. You
should use `{\tt \verb+\+$c$}' whenever you need special 
character $c$ as ordinary character in a definition.
All individual characters in character classes, except digits and letters,
are {\em always} escaped with a backslash.

In literal strings, the following characters are special and should be
escaped:

\begin{itemize}
 \item {\tt "}: double quote 
\item \verb+\+: escape character.
\end{itemize}

You may use the following abbreviations in literals and in character classes:

\begin{itemize}

\item \verb+\n+: newline character 

\item \verb+\r+: carriage return

\item \verb+\t+: horizontal tabulation 

\item \verb+\+$nr$: a non-printable character with the decimal code $nr$.

\end{itemize}

\paragraph{Character Class Operators}

The following operators are available for character classes
\begin{itemize}
\item {\tt \~{}}: complement of character class. Accepts all characters not in the original class.
\item {\tt /}: difference of two character classes. Accepts all characters in
  the first class unless they are in the second class.
\item {\tt  /\verb+\+}: intersection of two character classes. Accepts all
  characters that are accepted by both character classes.
\item {\tt  \verb+\+/}: union of two character classes. Accepts all characters
  that are accepted by either character class.
\end{itemize}
\noindent The first operator is a unary operator, whereas the other three are
left-associative binary operators.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module LettersDigits3
exports
  sorts LetterOrDigit
  lexical syntax
    [a-z] \/ [A-Z] \/ [0-9]   -> LetterOrDigit
\end{boxedverbatim}
\end{IncCode}
\caption{Defining a single letter or digit using the alternative operator}\label{CODE:LettersDigits3}
\end{figure}

Figure~\ref{CODE:LettersDigits3} shows the definion of a single letter or digit
using the alternative operator {\tt  \verb+\+/}.
This definition is equivalent to the one given earlier in 
Figure~\ref{CODE:LettersDigits2}.
Another example is shown in Figure \ref{CODE:charclasses}.
This definition of characters contains
all possible characters, either by means of the ordinary representation
or via their decimal representation.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Characters

imports Layout

exports
  sorts AlphaNumericalEscChar DecimalEscChar EscChar L-Char
  lexical syntax
    "\\" ~[]                 -> AlphaNumericalEscChar

    "\\" [01] [0-9] [0-9]    -> DecimalEscChar
    "\\" "2" [0-4] [0-9]     -> DecimalEscChar
    "\\" "2" "5" [0-5]       -> DecimalEscChar

    AlphaNumericalEscChar    -> EscChar
    DecimalEscChar           -> EscChar

    ~[\0-\31\"\\] \/ [\t\n]  -> L-Char
    EscChar                  -> L-Char
\end{boxedverbatim}
\end{IncCode}
\caption{Example of character classes}\label{CODE:charclasses}
\end{figure}  

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Identifiers-repetition

imports Layout

exports
  sorts Letter DigitLetter
  lexical syntax
    [a-z]       -> Letter
    [a-z0-9]    -> DigitLetter

    Letter DigitLetter* -> Id
\end{boxedverbatim}
\end{IncCode}
\caption{Defining identifiers using the repetition operator {\tt *} }\label{CODE:repetition}
\end{figure}   

\subsubsection{Repetition}

Lexical tokens are often described by patterns that exhibit a certain
repetition. The operator described in Section~\ref{RepetitionOperator}
can be used to express repetitions.

Figure \ref{CODE:repetition} shows the use of the repetition
operator {\tt *} for defining identifiers consisting of a letter
followed by zero or more letters or digits.

\subsubsection{Option}

If zero or exactly one occurrence of a lexical token is desired the
option operator described in Section~\ref{OptionOperator} can be used.

The use of the option operator is illustrated in Figure \ref{CODE:option}
which defines identifiers consisting of one letter followed by one, optional,
digit. This definition accepts {\tt a} and {\tt z8}, but rejects {\tt ab} or {\tt
  z789}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Identifiers-optional

imports Layout

exports
  sorts Letter Digit
  lexical syntax
    [a-z]  -> Letter
    [0-9]  -> Digit

    Letter Digit? -> Id
\end{boxedverbatim}
\end{IncCode}
\caption{Defining a letter followed by an optional number using the option operator {\tt ?} }\label{CODE:option}
\end{figure}   

\subsubsection{Alternative}

Functions with the same result sort together define the lexical syntax of
tokens of that sort. The left-hand sides of these function definitions form
the alternatives for this function. Sometimes, it is more convenient to list
these alternatives explicitly in a single left-hand side or to list
alternative parts inside a left-hand side.  This is precisely the role of the
alternative operator described in Section~\ref{AlternativeOperator}.

The example  in Figure \ref{CODE:alternative1} shows how this operator
can be used.  It describes identifiers starting with an upper-case letter
followed by one of the following:
\begin{itemize}
\item  zero or more lower-case letters, 

\item zero or more upper-case letters, or

\item zero or more digits.
\end{itemize}

\noindent According to this definition, 
{\tt Aap}, {\tt NOOT}, and {\tt B49} are acceptable, but {\tt MiES}, {\tt
  B49a} and {\tt 007} are not.

Note that the relation between juxtaposition and alternative operator is best
understood by looking at the line defining {\tt Id}. A parenthesized
version of this same line would read as follows:
\begin{quote}
\begin{verbatim}
  UCLetter (LCLetter* | UCLetter* | Digit*) -> Id
\end{verbatim}
\end{quote}
As an aside, note that moving the {\tt *} outside the parentheses as in
\begin{quote}
\begin{verbatim}
  UCLetter (LCLetter | UCLetter | Digit)* -> Id
\end{verbatim}
\end{quote}
yields a completely different definition: it describes identifiers starting
with an uppercase letter followed by zero or more lower-case letters,
uppercase letters or digits. According to this definition {\tt MiES}, {\tt
  B49a} and {\tt 007} would, for instance, be acceptable.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Identifiers-alternative1

imports Layout

exports
  sorts UCLetter Digit
  lexical syntax
    [A-Z]   -> UCLetter
    [a-z]   -> LCLetter
    [0-9]   -> Digit

  UCLetter LCLetter* | UCLetter* | Digit* -> Id
\end{boxedverbatim}
\end{IncCode}
\caption{Example of alternative operator {\tt |} }\label{CODE:alternative1}
\end{figure}   

\noindent A slightly more readable definition that is equivalent to the original one
in Figure~\ref{CODE:alternative1} is shown in Figure~\ref{CODE:alternative2}.  In
any case, we recommend to use parentheses to make the scope of alternatives
explicit.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Identifiers-alternative2

imports Layout

exports
  sorts UCLetter LCLetterDigit
  lexical syntax
    [A-Z]   -> UCLetter
    [a-z]   -> LCLetter
    [0-9]   -> Digit

    (UCLetter LCLetter*) | (UCLetter UCLetter*) | (UCLetter Digit*) -> Id
\end{boxedverbatim}
\end{IncCode}
\caption{Example of alternative operator {\tt |} }\label{CODE:alternative2}
\end{figure}   

\subsubsection{Miscellaneous Operators}
The other operators described in Section~\ref{Symbols} are less frequently
used within lexical syntax definitions and will not be illustrated by
means of an example.


\subsubsection{Examples of Lexical Syntax Definitions}

We will present a number of non-trivial lexical syntax definitions
in order to get some ideas what can be specified using 
\sdf.

\paragraph{Defining Numbers}

Definitions of integers and real numbers are shown in Figure
\ref{CODE:numbers}. Note the use of the alternative operator in the
definitions of {\tt UnsignedInt} and {\tt Number}.  Also note the use
of the option operator in the definitions of {\tt SignedReal} and {\tt
UnsignedReal}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Numbers

imports Layout

exports
  sorts UnsignedInt SignedInt UnsignedReal Number 

  lexical syntax
    [0] | ([1-9][0-9]*)                           -> UnsignedInt

    [\+\-]? UnsignedInt                           -> SignedInt

    UnsignedInt "." UnsignedInt ([eE] SignedInt)? -> UnsignedReal 
    UnsignedInt [eE] SignedInt                    -> UnsignedReal

    UnsignedInt | UnsignedReal                    -> Number
\end{boxedverbatim}
\end{IncCode}
\caption{Lexical definition of Numbers}\label{CODE:numbers}
\end{figure}   

\paragraph{Defining Strings}

Figure \ref{CODE:string} gives the lexical definition of
strings which may contain escaped double quote characters.
It defines a {\tt StringChar} as either
\begin{itemize}
\item zero or more
arbitrary characters except double quote or newline, or
\item an escaped double quote, i.e., \verb+\"+.
\end{itemize}

\noindent A string consists of zero or more {\tt StringChar}s surrounded by
double quotes.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Strings

imports Layout

exports
  sorts String StringChar

  lexical syntax
    ~[\"\n]*              -> StringChar
    [\\][\"]              -> StringChar
    "\"" StringChar* "\"" -> String
\end{boxedverbatim}
\end{IncCode}
\caption{Lexical definition of String}\label{CODE:string}
\end{figure}   

\subsection{Context-free Syntax} \label{ContexFreeSyntax}

The context-free syntax describes the concrete and abstract syntactic
structure of sentences in a language. A context-free syntax contains a set of
declarations for \emph{context-free functions}, each consisting of zero or
more symbols followed by `$\rightarrow$' and a result symbol.
They may be followed by attributes that control how parentheses and brackets
affect the abstract syntax, by attributes that define the associativity of a
rule, or by attributes which influence the rewriting process, see
Section~\ref{Attributes}.  All functions with the same result sort together
define the alternatives for that symbol.

Elements of the left-hand side of a context-free function
are separated by an invisible non-terminal {\tt LAYOUT?} 
(optional {\tt LAYOUT}) in order to permit layout between these members.
This optional layout non-terminal is automatically inserted.

\subsubsection{Context-free Functions}

In their simplest form, declarations of context-free functions consist of a
sequence of zero or more symbols followed by `$\rightarrow$'
and a result symbol. All literal strings appearing in a context-free
function declaration are implicitly added to the lexical syntax. Consider the
language of coordinates and drawing commands presented in 
Figure \ref{CODE:simple-cf}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module DrawingCommands

imports Layout

exports
  sorts NAT COORD CMND 

  lexical syntax
    [0-9]+ -> NAT 

  context-free syntax
    "(" NAT "," NAT ")" -> COORD
    "line" "to" COORD   -> CMND 
    "move" "to" COORD   -> CMND
\end{boxedverbatim}
\end{IncCode}
\caption{Simple context-free syntax definition}\label{CODE:simple-cf}
\end{figure}   


An equivalent conventional BNF grammar (and not considering lexical syntax) 
of the grammar of Figure~\ref{CODE:simple-cf} is presented in Figure
\ref{CODE:simple-bnf}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
<COORD> ::= "(" <NAT> "," <NAT> ")" 
<CMND>  ::= "line" "to" <COORD> | "move" "to" <COORD>
\end{boxedverbatim}
\end{IncCode}
\caption{BNF definition of simple grammar}\label{CODE:simple-bnf}
\end{figure}   

When a literal in a context-free function consists only of lower-case letters
and digits and is not a keyword of \asfsdf, it need not be surrounded with
quotes. You may therefore write `{\tt move to COORD -> CMND}' instead of the
definition given in Figure~\ref{CODE:simple-cf}.

\subsubsection{Lists}

Context-free syntax often requires the description of
the repetition of a syntactic notion or of list structures (with or without
separators) containing a syntactic notion. The repetition operator
described in Section~\ref{RepetitionOperator} can be used for this
purpose.

Lists may be used in both the left-hand side and right-hand side of a
context-free function as well as in the right-hand side of a variable
declaration (see Section~\ref{Variables}).

Figure \ref{CODE:pascal-ids} shows how lists can be used to define 
the syntax of a list of identifiers (occurring in a declaration
in a Pascal-like language).

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Decls

imports Layout

exports
  sorts ID DECL TYPE 

  lexical syntax
    [a-z]+ -> ID 

  context-free syntax
    "decl" {ID ","}+ ":" TYPE -> DECL
    "integer"                 -> TYPE 
    "real"                    -> TYPE
\end{boxedverbatim}
\end{IncCode}
\caption{Syntax definition of a list of identifiers}\label{CODE:pascal-ids}
\end{figure}

\subsubsection{Chain Functions}

A context-free syntax may contain functions that do not add syntax, but serve
the sole purpose of including a smaller syntactic notion into a larger one. 
This notion is also known as {\em injections}. 
Injections are functions ``without a name'' and with one argument sort
like {\tt ID -> DATA}.
A typical example is the inclusion of identifiers in expressions or of natural
numbers in reals. Such a \emph{chain function} has one of the following forms:

\begin{itemize}

\item {\tt SMALL -> BIG} 
\item {\tt \{SMALL SEP\}* -> BIG} 
\item {\tt SMALL* -> BIG} 
\item {\tt \{SMALL SEP\}+ -> BIG} 
\item {\tt SMALL+ -> BIG}
\item {\tt \{SMALL SEP\}n+ -> BIG} 
\item {\tt SMALLn+ -> BIG}

\end{itemize}

Chain functions do not appear in the abstract syntax but correspond to a
\emph{subsort relation} between {\tt SMALL} and {\tt BIG}.
If {\tt SORT-A} is a subsort of {\tt SORT-B} then in the abstract syntax
tree a tree of sort {\tt SORT-A} can be put wherever a tree of
sort {\tt SORT-B} is required.
In Figure~\ref{CODE:inj-exp} the symbols {\tt NAT} and {\tt VAR}
are injected in {\tt EXP}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Exp

imports Layout

exports
  sorts NAT VAR EXP

  lexical syntax
    [0-9]+   -> NAT
    [XYZ]    -> VAR

  context-free syntax
    NAT                 -> EXP
    VAR                 -> EXP
    EXP "+" EXP         -> EXP
\end{boxedverbatim}
\end{IncCode}
\caption{Syntax definition in which injections are used}\label{CODE:inj-exp}
\end{figure}

\subsubsection{Miscellaneous Operators}

In Section~\ref{Symbols} a number of sophisticated operators, like
alternative, option, set, function, sequence, tuple, and permutation.
are discussed. These operators allow a concise manner of defining
grammars. There are a number of issues to take in consideration
when using this operators. 

\paragraph{Definition of lists}
In Figure~\ref{CODE:lists-usage} two different
lists are defined, {\tt LIST1} represents a list of naturals separated
by commas whereas {\tt LIST2} represents a list of naturals terminated
by commas.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Lists

imports Layout

exports
  sorts NAT LIST1 LIST2

  lexical syntax
    [0-9]+   -> NAT

  context-free syntax
    {NAT ","}+   -> LIST1
    (LIST ",")+  -> LIST2
\end{boxedverbatim}
\end{IncCode}
\caption{Syntax definition in which two variants of lists are
defined}\label{CODE:lists-usage}
\end{figure}

\paragraph{Alternative alternatives}
The alternatives of symbols can be defined in two different ways, 
see Figure~\ref{CODE:alternative-alternatives} for an example.

The definition of the binary operators {\tt "|"} and {\tt "\&"} can
be made more concise as shown by {\tt Bool2}, however, it is now
impossible to express that {\tt "\&"} has a higher priority
than {\tt "|"}, see Section~\ref{Priorities} for more details on
priority definitions.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Bool

imports Layout

exports
  sorts Bool1 Bool2

  context-free syntax
    true            -> Bool1
    false           -> Bool1
    Bool1 "|" Bool1 -> Bool1 {left}
    Bool1 "&" Bool1 -> Bool1 {left}

    true | false            -> Bool2
    Bool2 ("|" | "&") Bool2 -> Bool2 {left}
\end{boxedverbatim}
\end{IncCode}
\caption{Syntax definition in which two ways of alternatives are 
shown}\label{CODE:alternative-alternatives}
\end{figure}

\subsection{Attributes of Lexical and Context-free Functions} \label{Attributes}

The definition of a lexical or context-free functions may be followed by
\emph{attributes} that define additional (syntactic or semantic) properties of
that function.  The following syntax-related attributes exist:

\begin{itemize}

\item {\tt bracket} allows the definitions of parenthesis and other
kinds of brackets that are mostly used for overruling the priorities
of operators in expressions (Section~\ref{BracketFunctions}).

\item {\tt left}, {\tt right}, {\tt non-assoc}, and {\tt assoc}
are used to define the associativity of functions 
(Section~\ref{Priorities}).

\item {\tt prefer} is used to indicate that the attributed function should
always be preferred over other functions (without this attribute)
in certain cases of syntactic ambiguity (Section~\ref{PreferAvoidReject}).

\item {\tt avoid} is used to indicate that a function should
only be used as a last resort in certain cases of
syntactic ambiguity (Section~\ref{PreferAvoidReject}).


\item {\tt reject} can be used to explicitly forbid certain syntactic
  constructs (Section~\ref{PreferAvoidReject}).

\end{itemize}

\noindent The remaining attributes define semantic properties of a function:

\begin{itemize}

\item {\tt memo} declares a function to be a \emph{memo function} for
which all calls and results will be cached during evaluation
(Section~\ref{MemoFunctions}).

%%\item {\tt delay} is used to influence the evaluation order of the
%%arguments of a function (Section~\ref{Delay}).

\item {\tt traversal} is used to declare so-called traversal functions
that greatly simply the specification of functions that have to visit
(parts of) a term (Section~\ref{Traversal}).

\end{itemize}

\subsection{Priorities} \label{Priorities}

The context-free syntax defined in an \asfsdf\ specification may be 
ambiguous: there are sentences which have more than one associated tree. 
The common example is the arithmetic expression in which definitions 
of the priority or associativity of operators are missing. There are 
three mechanisms for defining associativity and priority:

\begin{itemize}

\item Relative priorities of functions (defined in 
the `{\tt context-free priorities}' section, see Section~\ref{RelativePriorities}). 

\item Associativity of functions (defined as an attribute following 
the function declaration, see Section~\ref{AssociativeFunctions}).

\item Associativity of groups of functions (defined in 
the `{\tt context-free priorities}' section, see Section~\ref{GroupAssoc}).

\end{itemize}

Closely related with priorities are brackets that can be used to
overrule priorities.  We will first describe bracket functions,
and then the various forms of defining priorities.

\subsubsection{Bracket Functions} \label{BracketFunctions}

A bracket function has the form `{\tt $open$ $S$ $close$ -> $S$}' where $open$
and $close$ are literals acting as opening and closing parenthesis for sort
$S$. Examples are `{\tt (}' and `{\tt )}' in arithmetic expressions.
In most cases, such brackets are only
introduced for grouping and disambiguation, but have no further meaning. By
adding the attribute {\tt bracket} to the function declaration, it will not be
included in the abstract syntax. Figure \ref{CODE:bracket-expr} shows the
definition of a bracket function for the sort {\tt EXPR}.

Since brackets are necessary for overruling the priority and associativity of
functions,  it is required that bracket 
functions are declared for the argument and result sorts of

\begin{itemize}

\item all functions appearing in priority declarations, and
  
\item all functions having one of the attributes {\tt left}, {\tt right}, 
{\tt assoc}, or {\tt non-assoc}.

\end{itemize}

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module BracketExpr

imports Layout

exports
  sorts EXPR

  lexical syntax
    [0-9]+ -> EXPR

  context-free syntax
    "(" EXPR ")" -> EXPR {bracket}
\end{boxedverbatim}
\end{IncCode}
\caption{Syntax definition with a bracket function}\label{CODE:bracket-expr}
\end{figure}

\subsubsection{Relative Priorities}\label{RelativePriorities}

The relative priority of two functions is defined 
in the `{\tt context-free priorities}' section 
by including {\tt $F$ > $G$},
where $F$ and $G$ are 
as written in the context-free grammar. Functions with a higher 
priority bind more strongly than functions with lower priorities and 
the nodes corresponding to them should
thus appear at lower levels in the tree than nodes corresponding to functions
with lower priorities. Lists of functions may be used in a priority
declaration: {\tt $F$ > \{$G$, $H$\}} is an 
abbreviation for {\tt $F$ > $G$, $F$ > $H$}.
Note that this tells us nothing about the priority relation between $G$ and $H$. 

\subsubsection{Associative Functions} \label{AssociativeFunctions}

Associativity attributes can be attached to binary functions of the form 
`{\tt $S$ $op$ $S$ -> $S$}', where $op$ is a symbol or empty. 
Without associativity attributes, nested occurrences of such 
functions immediately lead to ambiguities, as is shown by the 
sentence `{\tt S-string op S-string op S-string}' where 
`{\tt S-string}' is a string produced by symbol $S$. 
The particular associativity 
associated with $op$ determines the indented interpretation of such sentences.
  
We call two occurrences of functions $F$ and $G$ \emph{related}, when the node
corresponding to $F$ has a node corresponding to $G$ as first or last child.
The associativity attributes define how to accept or reject trees containing
related occurrences of the same function, $F$:

\begin{itemize}

\item {\tt left}: related occurrences of $F$ associate from left to right. 

\item {\tt right}: related occurrences of $F$ associate from right to left. 

\item{\tt  assoc}: related occurrences of $F$ associate from left to right.

\item {\tt non-assoc}: related occurrences of $F$ are not allowed.

\end{itemize}

Currently, there is no syntactic or semantic difference between `{\tt left}'
and `{\tt assoc}', but we may change the semantics of the `{\tt assoc}'
attribute in the future.

Figure \ref{CODE:simple-prio} gives an example of a definition of
simple arithmetic expressions with the usual priorities and
associativities.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module SimpleExpr

imports Layout

exports
  sorts E 

  lexical syntax
    [0-9]+ -> E 

  context-free syntax
    E "+" E   -> E {left}
    E "*" E   -> E {left}
    "(" E ")" -> E {bracket}

  context-free priorities
    E "*" E -> E > 
    E "+" E -> E
\end{boxedverbatim}
\end{IncCode}
\caption{Simple context-free priority definition}\label{CODE:simple-prio}
\end{figure}   

\subsubsection{Groups of Associative Functions} \label{GroupAssoc}

Groups of associative functions define how to accept or reject trees
containing related occurrences of different functions with the same priority.
They are defined by prefixing a list of context-free functions in a priority
declaration with one of the following attributes:

\begin{itemize}

\item {\tt left}: related occurrences of $F$ and $G$ associate from left to right. 
\item {\tt right}: related occurrences of $F$ and $G$ associate from right to left.
\item {\tt non-assoc}: related occurrences of $F$ and $G$ are not allowed.

\end{itemize}

\noindent where $F$ and $G$ are functions appearing in the list.
See Figure \ref{CODE:complex-prio} for an example of the use of grouped
associativity.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module ComplexExpr

imports Layout

exports
  sorts E 

  lexical syntax
    [0-9]+ -> E

  context-free syntax
    E "+" E   -> E {left}
    E "-" E   -> E {non-assoc}
    E "*" E   -> E {left}
    E "/" E   -> E {non-assoc}
    E "^" E   -> E {right}
    "(" E ")" -> E {bracket}

  context-free priorities
    E "^" E -> E > 
    {non-assoc: E "*" E -> E
                E "/" E -> E} >
    {left: E "+" E -> E
           E "-" E -> E}
\end{boxedverbatim}
\end{IncCode}
\caption{More complex associativity and priority definitions}\label{CODE:complex-prio}
\end{figure}   

\subsubsection{Restrictions} \label{Restrictions} \label{ContextFreeRestrictions}
\label{LexicalRestrictions}

Lexical syntax can be highly ambiguous.  Consider a simple lexical definition
for identifiers like the one in Figure~\ref{CODE:repetition}.  When
recognizing the text {\tt abc}, what should we return: {\tt a}, {\tt ab}
or,
{\tt abc}? In Section \ref{lex-ambiguity} we will discuss the strategy
\emph{Prefer Longest Match} for resolving this kind of ambiguity.

Here, we describe the notion of \emph{restrictions} that enable the formulation
of this and other lexical disambiguation strategies.

A restriction limits the \emph{lookahead} for a given symbol; it indicates
that a symbol may not be followed by a character from a given character class.
A lookahead may consist of more than one character class.  Restrictions come
in two flavors:

\begin{itemize}
\item lexical restrictions;
\item context-free restrictions.
\end{itemize}

\noindent The general form of a restriction is 

\begin{verbatim}
<Symbol>+ -/- <Lookaheads>
\end{verbatim}
 
\noindent In case of lexical restrictions {\tt <Symbol>} may be   
either a literal or a sort.  In case of context-free restrictions only a sort
is allowed.
The restriction operator {\tt -/-} should be read as ``may not be
followed by''.
Before the restriction operator {\tt -/-} a list of symbols
is given for which the restriction holds, see Figure \ref{CODE:functional}
for an example, both {\tt let} and {\tt in} may not be followed
by a letter.

{\tt <Lookaheads>} are slightly more complex.  The most compact way is to
give the \sdf\ definition of the {\tt <Lookaheads>} and illustrate their use by
means of some examples.

\begin{verbatim}
context-free syntax
  CharClass                    -> Lookahead
  CharClass "." Lookaheads     -> Lookahead
  Lookahead                    -> Lookaheads
  Lookaheads "|" Lookaheads    -> Lookaheads {right}
  "(" Lookaheads ")"           -> Lookaheads {bracket}
  "[[" {Lookahead ","}* "]]"   -> Lookaheads 
\end{verbatim}

Our first example\footnote{Taken from~\cite{Vis97}} (Figure
\ref{CODE:functional}) shows how lexical restrictions can be used to prevent
the recognition of erroneous expressions in a small functional language.

The lexical restriction deals with the possible confusion between
the reserved words {\tt let} and {\tt in} and variables (of sort {\tt Var}).
It forbids the recognition of, for instance, {\tt let} as part
of {\tt letter}. Without this restriction {\tt letter} would be recognized
as the keyword {\tt let} followed by the variable  {\tt ter}.

The context-free restriction forbids that a variable is directly
followed by a letter. 

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Functional

imports Layout

exports
  sorts Var Term
  lexical syntax
    [a-z]+ -> Var
  context-free syntax
    Var                          -> Term
    Term Term                    -> Term {left}
    "let" Var "=" Term "in" Term -> Term

  lexical restrictions
    "let" "in" -/- [a-z]

  context-free restrictions
    Var -/- [a-z]
\end{boxedverbatim}
\end{IncCode}
\caption{Using restrictions in the definition of a simple functional language}\label{CODE:functional}
\end{figure}   

The second example illustrates the use of restrictions to define a
`safe' way of layout.  Recall from Section~\ref{LexicalSyntax} that optional
layout may be recognized between the members of the left-hand side of a
context-free syntax rule.

However, if a such a member recognizes the empty string, this gives rise to a
lexical ambiguity, (Section \ref{lex-ambiguity}). This problem is avoided by
the definition in Figure \ref{CODE:safe-layout}: it simply forbids that
optional layout is followed by layout characters.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Layout

exports
  lexical syntax
    [\ \t\n] -> LAYOUT

  context-free restrictions
    LAYOUT? -/- [\ \t\n]
\end{boxedverbatim}
\end{IncCode}
\caption{Safe way of defining {\tt LAYOUT}}\label{CODE:safe-layout}
\end{figure}

The last example is shown in Figure~\ref{CODE:c-comment} and illustrates the
use of restrictions to extend the previous layout definition with C-style
comments. For readability we give here \emph{two} restrictions whereas the
first one is already imported from module {\tt Layout}
(Figure~\ref{CODE:safe-layout}).  The repetition of this first restriction is
redundant and could be eliminated.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Comment

imports Layout

exports
  sorts ComWord Comment
  lexical syntax
    ~[\ \n\t\/]+ -> ComWord

  context-free syntax
    "/*" ComWord * "*/" -> Comment
    Comment             -> LAYOUT

  context-free restrictions
    LAYOUT? -/- [\ \t\n]
    LAYOUT? -/- [\/].[\*]
\end{boxedverbatim}
\end{IncCode}
\caption{Definition of C comments}\label{CODE:c-comment}
\end{figure}

One frequently asked question is when to use lexical restrictions
and when to use context-free restrictions. 
The lexical restrictions on {\tt let} and {\tt in} in
Figure \ref{CODE:functional} can not be defined context-free 
because these keywords do not "live" at the lexical level.
Is it possible to put a lexical restriction on {\tt Var}?
Yes, but it will have no effect, because internally the
lexical {\tt Var} is injected in the context-free {\tt Var}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module RestrictedExpressions

imports Layout

exports
  sorts Expr

  lexical syntax
    [a-z]+ -> Expr

  context-free syntax
    Expr Expr    -> Expr {left}
    "(" Expr ")" -> Expr {bracket}

  context-free restrictions
    Expr -/- [a-z]
\end{boxedverbatim}
\end{IncCode}
\caption{Erroneous use of restrictions in the definition of simple expressions}\label{CODE:restrictedexpressions}
\end{figure}   

Figure \ref{CODE:restrictedexpressions} shows an erroneous use of
context-free expressions, because it prevents the recognition of
{\tt (abc)def}. If we want to enforce the correct restriction, it
is necessary to transform this context-free restriction into
a lexical restriction.

\subsubsection{Preferring, Avoiding or Rejecting Parses} 
\label{PreferAvoidReject}

Priorities can be used to define a priority between two functions or between
two groups of functions. In both cases the functions involved have to be
listed explicitly in the priority declaration. In certain cases, however, it
is desirable to define that a single rule has higher or lower priority than
all other functions or to explicitly reject certain syntactic constructs.
The former is achieved by the attributes {\tt prefer} and {\tt avoid}. The
latter by the attribute {\tt reject}.

If a function $F$ is attributed with {\tt prefer} and there is a syntactic
ambiguity in which it is involved, only the parse using $F$ will remain.

If a function $F$ is attributed with {\tt avoid} and there is no ambiguity,
then $F$ will be used. If there is an ambiguity, then $F$ will be immediately
removed from the set of ambiguities.

If a function $F$ is attributed with {\tt reject}, then independent of the
number of ambiguities, the parse using $F$ will be removed.  While
restrictions (Section~\ref{Restrictions}) only impose restrictions on the
immediate lookahead that follows a symbol, the reject mechanism can be used
to eliminate complicated syntactic structures.

Examples of the use of {\tt prefer}, {\tt avoid} and {\tt reject} in order
to solve lexical ambiguities will be discussed in
Section~\ref{lex-ambiguity}.
In Section~\ref{cf-ambiguity} we will give examples of how to use
these attributes to solve context-free ambiguities, such as the
famous dangling else problem.

\subsection{Disambiguation} \label{Disambiguation}

\subsubsection{Lexical Ambiguities}
\label{lex-ambiguity}

\sdf\ provides a number of elementary lexical disambiguation features but does
not offer {\em fully automated} lexical disambiguation.
As a result, the specification writer has to be aware of lexical ambiguities
and has to specify disambiguation rules explicitly.
We will discuss various
approaches to lexical disambiguation and illustrate them by means of examples.

\paragraph{Prefer Longest Match per Sort} Reject all interpretations of 
the input text that are included in a longer interpretation of the same 
sort. Given a standard definition of identifiers, the input `{\tt xyz}' 
will thus lead to recognition of the identifier `{\tt xyz}' and not to 
either `{\tt x}' or `{\tt xy}'.

This is achieved by defining a restriction on this lexical sort. This
can be done in either the lexical restrictions or the context-free restrictions 
section (see Section \ref{Restrictions}).
Figure \ref{CODE:restrict-id} shows how to enforce the longest match
for the sort {\tt Id}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Identifiers-restrict

imports Layout

exports
  sorts Id
  lexical syntax
    [a-zA-Z][a-zA-Z0-9]* -> Id

  context-free restrictions
    Id -/- [a-zA-Z0-9]  
\end{boxedverbatim}
\end{IncCode}
\caption{Using context-free restrictions to define a longest match for identifiers}\label{CODE:restrict-id}
\end{figure}   

\paragraph{Prefer Literals} 

In the left-hand side of a context-free syntax rule literals (keywords
and/or operators) may be used.  If these literals overlap with  more
general lexical tokens (such as identifier) this causes ambiguities.

The strategy \emph{Prefer Literals} gives preference to interpretation
as a literal, over interpretation as a more general lexical token.
For instance, the keyword {\tt begin} may be recognized as an identifier
given the lexical definition of Figure \ref{CODE:restrict-id}.  There are
two approaches to implement Prefer Literals.

In the first approach, we can explicitly forbid the recognition of
literals as tokens of a specific sort using the reject mechanism (see
Section~\ref{PreferAvoidReject}).  The idea is to define context-free
grammar rules for all literals with the undesired lexical sort (e.g.,
{\tt Id}) in the right-hand side followed by the attribute {\tt reject},
Figure \ref{CODE:reject-id} illustrates this.  The {\tt reject} attribute
indicates that the recognizition of a keyword as a literal of the sort
{\tt Id} should be rejected. This approach has the major disadvantage
that the addition of a literal in any context-free rule also requires
the addition of a new reject rule for that literal.

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Identifiers-reject

imports Layout

exports
  lexical syntax
    [a-zA-Z][a-zA-Z0-9]* -> Id

  context-free restrictions
    Id -/- [a-zA-Z0-9]  

  context-free syntax
    "begin" -> Id {reject}
\end{boxedverbatim}
\end{IncCode}
\caption{Using {\tt reject} to implement Prefer Literals}\label{CODE:reject-id}
\end{figure}   

The second approach is more attractive. The lexical definition of the general
notion that interferes with our literals is written in such a way that it is
only used as a last resort. In other words, it is avoided as much as possible
and is only used when no alternative exists.  The attribute {\tt avoid}
defines precisely this behaviour (Section~\ref{PreferAvoidReject}).  Figure
\ref{CODE:avoid-id} shows how the lexical definition of {\tt Id} is attributed
with {\tt avoid}.

Although the first approach is more tedious, it allows more flexibility
than the second one.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Identifiers-avoid

imports Layout

exports
  lexical syntax
    [a-zA-Z][a-zA-Z0-9]* -> Id {avoid}

  context-free restrictions
    Id -/- [a-zA-Z0-9]  
\end{boxedverbatim}
\end{IncCode}
\caption{Using {\tt avoid} to implement Prefer Literals}\label{CODE:avoid-id}
\end{figure}   

\paragraph{Prefer Non-Layout} If there are interpretations of the text as
layout symbol and as non-layout symbol, eliminate all interpretations as layout
symbol. This is built-in behaviour of \asfsdf.

\paragraph{Prefer Variables} Give preference to interpretation as a 
variable (as defined in a variables section) over interpretation as a lexical
token. Thus built-in behaviour of \asfsdf. It is achieved by automatically
extending each variable declaration with the attribute {\tt prefer}
(Section~\ref{PreferAvoidReject}).

\subsubsection{Context-free Ambiguities} \label{cf-ambiguity}

Context-free grammars may be ambiguous and, as a result, the parser may yield
different parses of a text. More precisely, the result of a parse is a single
tree in which the ambiguities are explicitly marked. Each marked ambiguity
consists of a set of different parse trees for that ambiguity.  Many---but not
all!--- of these different parses can be eliminated by the following
strategies that are built-in the \ASmetaenv. These strategies use the
priorities and associativities as defined in the specification. In addition,
some standard heuristics are used.

\paragraph{Associativity filtering} The associativity filtering
is performed during the generation of the parse table. Based
on the associativity relations certain entries in the parse table
are removed.

\paragraph{Removing Trees containing Conflicts}

The simplest application of priority and associativity declarations is the
elimination of trees that contain conflicts:

\begin{itemize}

\item A parent node has a child with a lower priority than the parent itself.
  
\item A parent has a first or last child that is in conflict with
  an associativity relation between this parent and child.

\end{itemize}

Given the example of Figure \ref{CODE:complex-prio} we will give a
number of example sentences and the interpretation given the
language definition in this example.

\begin{center}
\begin{tabular}{ll}
  Sentence   & Interpretation \\
\verb"1^2^3" & \verb"1^(2^3)" \\
\verb"1^2*3" & \verb"(1^2)*3" \\
\verb"1*2*3" & \verb"(1*2)*3" \\
\verb"1/2/3" & error \\
\verb"1*2/3" & error \\
\verb"1-2-3" & error \\
\verb"1+2+3" & \verb"(1+2)+3" \\
\verb"1-2+3" & \verb"(1-2)+3" \\
\verb"1+2-3" & \verb"(1+2)-3"\\
\end{tabular}
\end{center}

\paragraph{Removing Trees using {\tt prefer}/{\tt avoid} Attributes at the Root}

The priority declarations are used to eliminate trees 
in three phases: 

\begin{enumerate}
  
\item If there are trees of which the syntax rule at the top node has a {\tt
    prefer} attribute, all other trees are removed.

\item  If there are trees of which the syntax rule at the top node has an {\tt
    avoid} attribute and there are other trees without an {\tt avoid} attribute at
  the root node, then all trees with {\tt avoid} attribute are removed.
  
\end{enumerate}

\paragraph{Removing Trees containing {\tt prefer}/{\tt avoid} Attributes}

After removing all trees containing conflicts, more than one tree may
still remain.  To further reduce this set of remaining trees, the number
of context-free functions with {\tt prefer}/{\tt avoid} attributes is
calculated and compared.  A tree in the set is then rejected if there
is another tree in the set with more {\tt prefer}s and less or equal
{\tt avoid}s, or with equal {\tt prefer}s and more {\tt avoid}s.

\paragraph{Injection count}
Finally, the number of injections in each of the resulting trees
is calculated, the tree with the smallest number of injections
is prefered.

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Eqn

imports Layout

exports
  sorts E 

  context-free syntax
    E "sub" E         -> E {left}
    E "sup" E         -> E {left}
    E "sub" E "sup" E -> E {prefer}
    "{" E "}"         -> E {bracket}
    "a"               -> E 
\end{boxedverbatim}
\end{IncCode}
\caption{Syntax definition of EQN expressions}\label{CODE:eqn-exprs}
\end{figure}

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module DanglingElse

imports Layout

exports
  sorts E S

  context-free syntax
    "a"                      -> E 
    "if" E "then" S          -> S
    "if" E "then" S "else" S -> S {avoid}
    "s"                      -> S
\end{boxedverbatim}
\end{IncCode}
\caption{Syntax definition of conditionals}\label{CODE:dangling-else}
\end{figure}

\paragraph{Example}

The following example shows how the interaction (and resulting ambiguities)
between general context-free functions and special case functions can be
described using {\tt prefer} attribute. It concerns expressions for 
describing subscripts
and superscripts in the typesetting language EQN. The crucial point is that,
for typesetting reasons, we want to treat a subscript followed by a
superscript in a special way. Therefore, the special case `{\tt E sub E sup E
  -> E}' is introduced, which is prefered over a combination of the two
functions `{\tt E sub E -> E}' and `{\tt E sup E -> E}'.
See, Figure \ref{CODE:eqn-exprs} for the \sdf\ definition.

In Figure~\ref{CODE:dangling-else} the {\tt avoid} attribute is used to
solve the dangling else problem in a nice way.

\subsection{Variables} \label{Variables}

Variables are declared in the `{\tt variables}' section of a module.  Like all
other entities in a module---except equations---variables may be exported
(Section \ref{modules}). A variables section consists of a list of variable
names followed by their sort. In fact, a variable declaration can define an
infinite collection of variables by using a \emph{naming scheme} instead of a
simple variable name.  A naming scheme is a regular expression like the ones
allowed in the lexical syntax (Section \ref{lexical-syntax}) except that sorts
are not allowed. A variable may represent any symbol. 

In Figure \ref{CODE:variables}, `{\tt Id}', `{\tt Type3}', and `{\tt Id-list}'
are examples of variables declared by the naming schemes in the {\tt
  variables} section.
Strings that occur in the left-hand side of variable declarations
should {\em always} be quoted.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module VarDecls

imports Layout

exports
  sorts ID DECL TYPE 

  lexical syntax
    [a-z]+ -> ID
 
  context-free syntax
    "decl" {ID ","}+ ":" TYPE -> DECL 
    "integer"                 -> TYPE 
    "real"                    -> TYPE 

hiddens
  variables
    "Id"           -> ID 
    "Type"[0-9]*   -> TYPE 
    "Id-list"[\']* -> {ID ","}* 
    "Id-ne-list"   -> {ID ","}+
\end{boxedverbatim}
\end{IncCode}
\caption{Variable declarations using naming schemes}\label{CODE:variables}
\end{figure}         

Declared variables can only be used when defining equations. It is not
allowed to use them in terms.

Ambiguities due to variables are resolved by the {\em Prefer Variables}
strategy that was discussed in Section \ref{lex-ambiguity}.

\subsubsection{Set Operator}

The set operator (see Section~\ref{SetOperator}) is an abbrevation for
representing sets. However, to obtain access to the elements of such
a set an auxilary variable has to be defined, see
Figure~\ref{CODE:set-variable} for an example.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Set

imports Layout

exports
  sorts Id IdSet

  lexical syntax
    [a-z]+ -> Id
 
  context-free syntax
    Set[ID] -> IdSet 

hiddens
  variables
    "Id"[0-9]*     -> Id
    "Id*"[0-9]*    -> {Id ","}* 
    "IdSet"[0-9]*  -> IdSet
\end{boxedverbatim}
\end{IncCode}
\caption{Variable declarations for Set}\label{CODE:set-variable}
\end{figure}         

\subsection{Equations} \label{Equations}

With equations a meaning or semantics may be added to functions declared in
the lexical and context-free syntax sections. 
In particular, equations consist of two \emph{open terms}, i.e.
terms possibly containing variables.

In the context of \asfsdf, an open term is any string that can be parsed
according to one of the sorts in the specification (possibly including
variables).  Recalling the {\tt Bool-example} specification in
Section~\ref{Bool-example}, examples of open terms are `{\tt true}', 
`{\tt not(false)}' and `{\tt true | Bool}'.

\subsubsection{Unconditional Equations}

An equality then consists of two open $L$ (lefthand side) and $R$ (righthand
side) such that:

\begin{itemize}

\item $L$ and $R$ are of the same sort.

\item $L$ is not a single variable.
  
\item The variables that occur in $R$ also occur in $L$.

\end{itemize}

It is assumed that the
variables occurring in the equation are universally quantified. In other
words, the equality holds for all possible values of the variables.

The equality of two terms $L$ and $R$ is defined  in \asfsdf\ by the following
\emph{unconditional} equation:

\begin{quote}
{\tt [$TagId$] $L$ = $R$} 
\end{quote}

\noindent where $TagId$ is a sequence of letters, digits, and/or minus
signs ({\tt -}) starting with a letter or a digit.


\subsubsection{Conditional Equations}

An unconditional equation is a special case of a \emph{conditional equation},
i.e., an equality with one or more associated conditions (premises).  The
equality is sometimes called the \emph{conclusion} of the conditional
equation.

In \asfsdf\ a conditional equaltion can be written in two (syntactically
different) ways:

\begin{tabbing}
(a) \= {\tt [$TagId$]} \= {\tt $L$ = $R$ when $C_1$, $C_2$, ...} \\\\
%(b) \> {\tt [$TagId$]} \> {\tt $C_1$, $C_2$, ... ====> $L$ = $R$} \\ 
(b) \> {\tt [$TagId$]} \> {\tt $C_1$, $C_2$, ...} \\
             \>    \> {\tt =================} \\
             \>    \> \ \ \ \ \ {\tt $L$ = $R$}
\end{tabbing}


\noindent where $C_1$, $C_2$, ...  are conditions which may be 
either positive (and have  the form `{\tt $S$ = $T$}'), 
or negative (and have the form `{\tt $S$ != $T$}').

The conditions of an equation are evaluated from left to right. Let,
initially, $V$ be the set of variables occurring in the left-hand side $L$ of the
conclusion of the equation. For the evaluation of each positive condition we
distinguish the following cases:

\begin{itemize}
  
\item The condition contains only variables in $V$. Reduce both sides of the
  condition to normal form and the condition succeeds if both normal forms are
  identical.
  
\item One of the sides of the condition contains new variables not in $V$.
  Reduce the side that does not contain new variables to normal form and the
  condition succeeds if this normal form and the other (unnormalized) side of
  the condition match. The new variables resulting from this match are added
  to $V$.

\end{itemize}
  
The evaluation of negative conditions is described by replacing in the above
description `identical' and `match' by `not identical' and `do not
match', respectively. 
However, it is not allowed to introduce new variables in a negative condition.
%%A warning is appropriate here: a negative condition that introduces
%%new variables nearly always succeeds (unless the sort has exactly one
%% element) and this is almost certainly not what you want.

After the successful evaluation of the conditions, all variables occurring in
the right-hand side of the conclusion of the equation should be in $V$.

New variables (see above) should therefore {\bf not} occur on \emph{both} 
sides of a positive condition, in a negative condition, 
or in the right-hand side of the conclusion.


\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Split

imports Integers

exports
  sorts El List
  lexical syntax
    [a-z]+ -> El
  context-free syntax
    {El ","}* -> List
    "length" "(" List ")"       -> Int
    "split-in-two" "(" List ")" -> List

hiddens
  variables
    "El"[0-9]*  -> El
    "El*"[0-9]* -> {El ","}* 

equations

  [l-1] length() = 0

  [l-2] length(El, El*) = 1 + length(El*)

  [s-1] length(El*1) = length(El*2)
        ===========================
        split-in-two(El*1, El*2) = El*1

  [s-1] length(El*1) = length(El*2)
        ===========================
        split-in-two(El*1, El, El*2) = El*1 
\end{boxedverbatim}
\end{IncCode}
\caption{Split-in-two specification}\label{CODE:split}
\end{figure}  

\subsubsection{Executing Equations} \label{ExecutingEquations}

In the \ASmetaenv, equations can be executed as \emph{rewrite rules}.  The
above equation is thus executed as the rewrite rule $L \rightarrow R$. This
can be used to reduce some initial closed term (i.e., not containing
variables) to a \emph{normal form} (i.e., a term that is not reducible any
further) by repeatedly applying rules from the specification.

A term is always reduced in the context of a certain module, say $M$. The
rewrite rules that may be used for the reduction of the term are the rules
declared in $M$ itself and in the modules that are (directly or indirectly)
imported by $M$.

The search for an applicable rule is determined by the reduction strategy,
that is, the procedure used to select a subterm for possible reduction. In our
case the \emph{leftmost-innermost} reduction strategy is used. This means that a
left-to-right, depth-first traversal of the term is performed and that for
each subterm encountered an attempt is made to reduce it.

Next, the rules are traversed one after the other.  The textual order of the
rules is irrelevant.  Instead they are ordered according to their
\emph{specificity}: more specific rules come before more general rules and
default rules (see Section~\ref{DefaultEquations}) come last.  
If the selected subterm and the left-hand side of a
rule (more precisely: of the left-hand side of its conclusion) match, we say
that a \emph{redex} has been found and the following happens. The conditions
of the rule are evaluated and if the evaluation of a condition fails, other
rules (if any) with matching left-hand sides are tried.  If the evaluation of
all conditions succeeds, the selected subterm is replaced by the right-hand
side of the rule (more precisely: the right-hand side of the conclusion of the
rule) after performing proper \emph{substitutions}. Substitutions come into
existence by the initial matching of the rule and by the evaluation of its
conditions.  For the resulting term the above process is repeated until no
further reductions are possible and a normal form is reached (if any).

\subsubsection{List Matching}

List matching, also known as associative matching, is a powerful
mechanism to describe complex functionality in a compact way.  See Figure
\ref{CODE:sets} for a compact specification to remove double elements
from a set.

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Sets

imports Layout

exports
  sorts Elem Set

  lexical syntax
    [a-z]+ -> Elem

  context-free syntax
    Set[Elem] -> Set

hiddens
  variables
    "Elem"[0-9]*  -> Elem
    "Elem*"[0-9]* -> {Elem ","}*

equations
  
  [set] {Elem*1, Elem, Elem*2, Elem, Elem*3} = {Elem*1, Elem, Elem*2, Elem*3} 
\end{boxedverbatim}
\end{IncCode}
\caption{Set specification}\label{CODE:sets}
\end{figure}   

Unlike the matching of ordinary (non-list) variables, the matching of a list
variable may have more than one solution since the variable can match lists of
arbitrary length.

As a result, backtracking is needed. For instance, to match {\tt X Y} (a list
expression containing the two list variables {\tt X} and {\tt Y} indicating
the division of a list into two sublists) with the list {\tt ab} (a list
containing two elements) the following three alternatives have to be
considered:

\begin{quote}
{\tt X = (empty), Y = ab, \\
X = a, Y = b, \\
X = ab, Y = (empty)}.
\end{quote}

In the unconditional case, backtracking occurs only during matching. When
conditions are present, the failure of a condition following the match of a
list variable leads to the trial of the next possible match of the list
variable and the repeated evaluation of following conditions.

This is exemplified by the Set specification presented in 
Figure \ref{CODE:sets}.
Yet another example of list matching in combination with the evaluation
of conditions is shown in Figure \ref{CODE:split}.
A list of elements is split into two parts of equal length, if the list
has an even number of elements. In case of a list of uneven length
the middle element is ignored. The first part of the list is returned
as result.

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Nats

imports Layout

exports
  sorts NAT-CON
  lexical syntax
    [0-9]+ -> NAT-CON 

hiddens
  variables
    "Char+"[0-9]* -> CHAR+

equations

  [1] nat-con("0" Char+) = nat-con(Char+)  
\end{boxedverbatim}
\end{IncCode}
\caption{Use of lexical constructor functions}\label{CODE:lcfs}
\end{figure}    

\subsubsection{Lexical Constructor Functions}

The only way to access the actual characters of a lexical token is
by means of the so-called {\em lexical constructor functions}.
For each lexical sort $LEX$ a lexical constructor function is automatically
derived, the corresponding syntax definition is:
{\tt "lex" "(" CHAR* ")" -> LEX}.
The sort {\tt CHAR} is a predefined sort to access the characters.

Characters can be directly addressed by the representation or via
variables which are either of the sort {\tt CHAR}, {\tt CHAR*}, or
{\tt CHAR+}, where the latter two represent lists of characters.
In Figure \ref{CODE:lcfs} a lexical constructor function is used
to remove the leading zeros for a number.

\noindent Note: there is no check on the use of characters within the
lexical constructor function.

\subsubsection{Default Equations} \label{DefaultEquations}

The evaluation strategy for normalizing terms given the equations is
based on innermost rewriting. All equations have the same priority.
Given the outermost function symbol of a redex the set of equations with
this outermost function symbol in the left-hand side is selected and all
these rules will be tried.  However, sometimes a specification writer
would like to write down a rule with a special status ``{\em try this rule
if all other rules fail}''.  A kind of default behaviour is needed. \asf\
offers functionality in order to obtain this behaviour. If the $TagId$
of an equation starts with {\tt default-} this equation is considered to
be a special equation which will only be applied if no other rule matches.
Figure \ref{CODE:types} shows an example of the use of a default equation.

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Types

imports Booleans

exports
  sorts Type
  context-free syntax
    "natural"     -> Type
    "string"      -> Type
    "nil-type"    -> Type
    "compatible" "(" Type "," Type ")" -> Bool 

hiddens
  variables
    "Type"[0-9]*  -> Type

equations

  [Type-1]  compatible(natural, natural) = true

  [Type-2]  compatible(string, string) = true

  [default-Type] compatible(Type1,Type2) = false
\end{boxedverbatim}
\end{IncCode}
\caption{Use of a default equation}\label{CODE:types}
\end{figure}   

\subsubsection{Memo Functions} \label{MemoFunctions}

Rewriting as execution model is not very efficient. Memo functions
improve the efficiency of \asfsdf\ specifications.  Given a set of
argument values for some function the normal form can be obtained via
rewriting. It is possible that some function is called with the same set
of arguments over and over again. Each time the function is rewritten to
obtain the same normal again. By means of adding the {\tt memo} attribute,
this behaviour is improved by storing the set of argument values and the
derived normal form in a memo-table.  For each set of argument values it
is checked whether there exists a normal form in the memo-table. If so,
this normal form is returned.  If not, the function given this set of
argument values is normalized and stored in the memo-table.

There is some overhead involved in accessing the memo-table.  Therefore,
it is not wise to add the memo attribute to each function.  With respect
to the operational behaviour adding a memo attribute does not have any
effect.  In Figure~\ref{CODE:fib} the Fibonacci function is decorated with
the memo attribute, improving its efficiency. See Table~\ref{TABLE:fibn}
for some figures on the performance improvement.

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|c|c|} \hline
fib(n) & Time without memo (sec) & Time with memo (sec) \\ \hline \hline
fib(16)          & \ 2.0 & 0.7  \\ \hline
fib(17)          & \ 3.5 & 1.1  \\ \hline
fib(18)          & \ 5.9 & 1.8  \\ \hline
fib(19)          &  10.4 & 3.3  \\ \hline
\end{tabular}
\vspace{\baselineskip}
\caption{The execution times for the evaluation of $\mbox{\em fib}(n)$}
\label{TABLE:fibn}
\end{center}
\end{table}


\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Fib

imports Layout

exports
  sorts Int
  context-free syntax
    "0"             -> Int
    "s" "(" Int ")" -> Int

  context-free syntax
    add(Int, Int) -> Int

    fib(Int)      -> Int {memo}

hiddens
  variables
    [xy][0-9]* -> Int

equations

  [add-s] add(s(x), y) = s(add(x, y))
  [add-z] add(0, y) = y

  [fib-z] fib(0) = s(0)
  [fib-o] fib(s(0)) = s(0)
  [fib-x] fib(s(s(x))) = add(fib(s(x)), fib(x))

\end{boxedverbatim}
\end{IncCode}
\caption{Use of the memo attribute when defining Fibonacci
functionality}\label{CODE:fib}
\end{figure}   

%%\subsubsection{Delaying} \label{Delay}
%%
%%Both the compiler and the evaluator are based on innermost rewriting.
%%In some cases it is more efficient to overrule this rewriting
%%strategy. The disadvantage of innermost rewriting are redundant work
%%in some cases and in some cases even non-termination. An example
%%of the first point and indirectly of the second point is the
%%evaluation of a conditional (see Figure~\ref{CODE:conditional}), 
%%using innermost rewriting the evaluation of an conditional 
%%amounts to evaluating the expression, then part, and else part, independent
%%of the result of the evaluation of the expression.
%%
%%\begin{figure}
%%\begin{IncCode}
%%\begin{boxedverbatim}
%%module Conditional
%%
%%imports Layout Expr Booleans
%%
%%exports
%%  context-free syntax
%%    "if" Bool "then" Expr "else" Expr "fi" -> Expr
%%
%%hiddens
%%  variables
%%    "Bool"[0-9]* -> Bool
%%    "Expr"[0-9]* -> Expr
%%
%%equations
%%
%%  [if-t] if true then Expr1 else Expr2 fi = Expr1
%%  [if-f] if false then Expr1 else Expr2 fi = Expr2
%%\end{boxedverbatim}
%%\end{IncCode}
%%\caption{Equations for conditions}\label{CODE:conditional}
%%\end{figure}   

\subsubsection{Traversal Functions} \label{Traversal}

Program analysis and program transformation usually take the syntax
tree of a program as starting point.  One common
problem that one encounters is how to express the \emph{traversal} of
the tree: visit all the nodes of the tree and extract information from
some nodes or make changes to certain other nodes.

The kinds of nodes that may appear in a program's syntax tree are
determined by the grammar of the language the program is written
in. Typically, each rule in the grammar corresponds to a node category
in the syntax tree. Real-life languages are described by grammars which
can easily contain several hundred, if not thousands of grammar rules.
This immediately reveals a hurdle for writing tree traversals: a naive
recursive traversal function should consider many node categories and
the size of its definition will grow accordingly.  This becomes even
more dramatic if we realize that the traversal function will only do
some real work (apart from traversing) for very few node categories.

Traversal functions in \asfsdf~\cite{BKV01} solve this 
problem\footnote{Traversal functions are currently only supported 
by the evaluator and not by the compiler.}.
We distinguish three kinds of traversal functions, defined as follows.

\begin{description}

\item[Transformer:] a sort-preserving transformation that 
will traverse its first argument. Possible extra arguments may contain
additional data that can be used (but not modified) during the traversal.
A transformer is declared as follows:

\transformerprod{n} \{{\tt \traversal(\trafo)}\}

Because a transformer always returns the same sort, it is type-safe. A
transformer is used to transform a tree\footnote{The additions {\tt trafo}
and {\tt accu} are not yet supported by the parser, so in the example you
will only see the attribute {\tt traversal}}.

\item[Accumulator:] a mapping of all node types to a single type. 
It will traverse its first argument, while the second
argument keeps the accumulated value. An accumulator is declared as follows:

\accumulatorprod{n} \{{\tt \traversal(\accu)}\}

After each application of an accumulator, the accumulated argument is updated.
The next application of the accumulator, possibly somewhere else in the term,
will use the \emph{new} value of the accumulated argument. In other words,
the accumulator acts as a global, modifiable, state during the traversal.

An accumulator function never changes the tree, only its accumulated argument.
Furthermore, the type of the second argument has to be equal to the result
type. The end-result of an accumulator is the value of the accumulated
argument. By these restrictions, an accumulator is also type-safe for every
instantiation.

An accumulator is meant to be used to extract information from a tree.

\item[Accumulating transformer:] a sort preserving transformation
that accumulates information while traversing its first argument. The
second argument maintains the accumulated value. The return value of
an accumulating transformer is a tuple consisting of the transformed
first argument and accumulated value. An accumulating transformer
is declared as follows:

\combinationprod{n}  \{{\tt \traversal(\accutrafo)}\}

An accumulating transformer is used to simultaneously extract information from a
tree and transform it.
\end{description}

Having these three types of traversals, they must be provided with visiting
strategies. Visiting strategies determine the order of traversal and the
``depth'' of the traversal. We provide the following two strategies for each
type of traversal:

\begin{description}
\item[Bottom-up:] the traversal visits \emph{all} the subtrees of a node where
  the visiting function applies in an \emph{bottom-up}\ fashion.  The
  annotation \innerm\ selects this behavior.  A traversal function without an
  explicit indication of a visiting strategy also uses the bottom-up strategy.
  
\item[Top-down:] the traversal visits the subtrees of a node in an top-down
  fashion and stops recurring at the first node where the visiting function
  applies and does not visit the subtrees of that node. The annotation
  \outerm\ selects this behavior.
\end{description}

We give two simple examples of traversal functions that are both
based on the tree language defined in Figure~\ref{FIG:tree-language}.
Our first example transforms a given tree into a new tree in which
all numbers have been incremented (Figure~\ref{FIG:inc}).  Our second
example computes the sum of all numbers in a tree (Figure~\ref{FIG:sum}).
For many more examples and a detailed description of traversal functions
see~\cite{BKV01}.

Implicitly, each traversal function corresponds to a set of functions,
one for each sort that can be reached during a traversal.
There are two approaches how to define these functions:

\begin{itemize}

\item They are explicitly defined by the specification writer
(this is the default).

\item They  are generated automatically. This is achived with
  the attribute {\tt generate-syntax}.

\end{itemize}

Experience learns that the automatic generation of syntax is nice for
smaller examples but becomes a burden for real-life specifications:
generation times become much longer and the generated function definitions
may yield undesired, and hard to resolve syntactic ambiguities.


\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Tree-syntax
imports Naturals
exports
  sorts TREE
  contextfreesyntax
    NAT           -> TREE
    f(TREE, TREE) -> TREE
    g(TREE, TREE) -> TREE
    h(TREE, TREE) -> TREE
\end{boxedverbatim}
\end{IncCode}
\caption{A simple tree language}\label{FIG:tree-language}
\end{figure}     

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Tree-inc
imports Tree-syntax
exports
  contextfreesyntax
    inc(TREE) -> TREE {traversal}
equations
[1] inc(N) = N + 1  
\end{boxedverbatim}
\end{IncCode}
\caption{The transformer {\tt inc} increments all numbers in a tree} \label{FIG:inc}
\end{figure}  

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Tree-sum
imports Tree-syntax
exports
contextfreesyntax
   sum(TREE, NAT) -> NAT {traversal}
equations
[1] sum(N1, N2) = N1 + N2
\end{boxedverbatim}
\end{IncCode}
\caption{The accumulator {\tt sum} that sums all numbers in a tree.}\label{FIG:sum} 
\end{figure}   



\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module ItemSet

imports Layout

exports
  sorts Item Set 

  lexical syntax
    [a-z]+ -> Item 

  context-free syntax
    Set[Item] -> Set

hiddens
  variables
    "i"[0-9]* -> Item
    "l"[0-9]* -> {Item ","}* 

equations

  [1] {l1, i, l2, i, l3}   = {l1, i, l2, l3} 
  [2] {l1, i1, l2, i2, l3} = {l1, i2, l2, i1, l3}
\end{boxedverbatim}
\end{IncCode}
\caption{Non-executable specification for sets}\label{CODE:itemsets}
\end{figure}   

\subsubsection{Which Specifications are Executable?}

Which \asfsdf\ specifications can be executed? The specification of sets
in Figure~\ref{CODE:itemsets} illustrates a non-executable specification,
since equation {\tt [2]}, which expresses that two elements in a set may
be exchanged, will lead to an infinite rewriting loop.

\subsubsection{Common Errors when Executing Specifications}

\begin{itemize}

\item When using the inequality operator {\tt !=} in a condition,
no new variables may be introduced in either side of the inequality.

\item If the normal form of a term still contains function symbols
that should have been removed during rewriting, you probably have
forgotten one or more equations that define the function.

A typical situation is that you have given an \emph{incomplete} set of equations
defining the function.

\item The rewriting process does not stop. Your equations probably contain
an infinite loop.

\item Be careful when a condition contains both instantiated and 
uninstantiated variables.

\end{itemize}

\newpage
\section{Examples of \asfsdf\ Specifications}
\label{large-examples}

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Diff 

imports Layout

exports
  sorts NAT VAR EXP
 
  lexical syntax
    [0-9]+   -> NAT 
    [XYZ]    -> VAR

  context-free syntax
    NAT                 -> EXP 
    VAR                 -> EXP 
    EXP "+" EXP         -> EXP {left} 
    EXP "*" EXP         -> EXP {left} 
    "(" EXP ")"         -> EXP {bracket} 
    "d" EXP "/" "d" VAR -> EXP

 context-free priorities
   EXP "*" EXP -> EXP > EXP "+" EXP -> EXP

hiddens
  variables
    "N"       -> NAT 
    "V"[0-9]* -> VAR 
    "E"[0-9]* -> EXP 

equations 
  [ 1] dN/dV = 0            [ 2] dV/dV = 1 
  [ 3] V1 != V2 ==> dV1/dV2 = 0 
  [ 4] d(E1+E2)/dV = dE1/dV + dE2/dV 
  [ 5] d(E1*E2)/dV = dE1/dV * E2 + E1 * dE2/dV 
  [ 6] E + 0 = E            [ 7] 0 + E = E 
  [ 8] E * 1 = E            [ 9] 1 * E = E 
  [10] 0 * E = 0            [11] E * 0 = 0
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for differentiation}\label{CODE:diff}
\end{figure}   


Here are some examples of \asfsdf\ specifications, which are selected to
illustrate specific features of the formalism. Larger examples can be found in
the online specifications.

\newpage
\subsection{Symbolic Differentiation}

Computing the derivative of an expression with respect to some variable is a
classical problem we discuss in this example. 
Computing the derivative of {\tt X * (X + Y + Z)} with respect to X gives:
\begin{quote}
{\tt d(X * (X + Y + Z)) / dX ) X + Y + Z + X}
\end{quote}

\noindent Differentiation is defined in three stages. First, the sorts {\tt NAT}
(natural numbers), {\tt VAR} (variables), and {\tt EXP} (expressions) are
introduced. Next, a differentiation operator of the form {\tt d $E$/d$V$} is
defined. Then, the differentiation rules are defined (equations [1]-[5]).
Finally, some rules for simplifying expressions are given. As the above
example shows, further simplification rules could have been added to collect
multiple occurrences of a variable (giving {\tt 2*X + Y + Z}) or to 
compute constant expressions.

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module Flag

imports Layout

exports
  sorts Color Flag 

  context-free syntax
    "red"          -> Color 
    "white"        -> Color 
    "blue"         -> Color 
    "{" Color+ "}" -> Flag 

hiddens
  variables
    "Cs"[0-9]* -> Color* 
    "C"[0-9]*  -> Color 

equations

  [1] {Cs1 white red Cs2}  = {Cs1 red white Cs2}

  [2] {Cs1 blue white Cs2} = {Cs1 white blue Cs2} 

  [3] {Cs1 blue red Cs2}   = {Cs1 red blue Cs2}
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for sorting}\label{CODE:flag}
\end{figure} 


\newpage
\subsection{Sorting}

The use of list structures is illustrated by the specification of
the "Dutch National Flag" problem presented in Figure \ref{CODE:flag}: 
given an arbitrary list of the colours red,
white and blue, sort them in the order as they appear in the Dutch National
Flag. We want:

{\tt \{white blue red blue red white red\} $\Rightarrow$ \{red red red white
 white blue blue\}}

In the specification of Figure~\ref{CODE:flag}, 
the list variables {\tt Cs1} and {\tt Cs2}
permit a succinct formulation of the search for adjacent colours that are in
the wrong order.

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module BasicNotions 
exports
  sorts Nat Id 
  lexical syntax
    [0-9]+         -> Nat 
    [a-z][a-z0-9]* -> Id
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for BasicNotions}\label{CODE:basicnotions}
\end{figure}

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Expressions 

imports BasicNotions

exports
  sorts Exp 
  context-free syntax
    Nat         -> Exp 
    Id          -> Exp 
    Exp "+" Exp -> Exp {left}
    Exp "-" Exp -> Exp {left}
    Exp "*" Exp -> Exp {left}

  context-free priorities
    Exp "*" Exp -> Exp > {left: Exp "+" Exp -> Exp
                                Exp "-" Exp -> Exp}
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for Expressions}\label{CODE:expressions}
\end{figure}

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module Statements 

imports Expressions 

exports
  sorts Stat Stats 
  context-free syntax
    Id ":=" Exp                 -> Stat 
    "if" Exp "then" Stats "fi"  -> Stat 
    "while" Exp "do" Stats "od" -> Stat
    {Stat ";"}+                 -> Stats 
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for Statements}\label{CODE:statements}
\end{figure}

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module AssemblyLanguage

imports BasicNotions 
exports 
  sorts Label Instr Instrs
  lexical syntax
    [a-z0-9]+ -> Label 
  context-free syntax
    "push" Nat      -> Instr
    "rvalue" Id     -> Instr 
    "lvalue" Id     -> Instr 
    "assign"        -> Instr 
    "add"           -> Instr 
    "sub"           -> Instr 
    "mul"           -> Instr 
    "label" Label   -> Instr 
    "goto" Label    -> Instr 
    "gotrue" Label  -> Instr 
    "gofalse" Label -> Instr
    {Instr ";"}+    -> Instrs 
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for AssemblyLanguage}\label{CODE:assemblylanguage}
\end{figure}

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module NextLabel
  
imports AssemblyLanguage
  
exports
  context-free syntax
    "nextlabel" "(" Label ")" -> Label 

hiddens
  variables
    "Char*"[0-9]* -> CHAR*

equations

 [1] nextlabel(label(Char*)) = label(Char* "x")
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for NextLabel}\label{CODE:nextlabel}
\end{figure}

\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
module CodeGenerator

imports Statements AssemblyLanguage NextLabel 
  
exports
  context-free syntax
    "tr" "(" Stats ")" -> Instrs
  
hiddens
  sorts Instrs-lab 
  context-free syntax
    Instrs # Label               -> Instrs-lab 
    "tr" "(" Stats "," Label ")" -> Instrs-lab 
    "tr" "(" Exp ")"             -> Instrs

hiddens
  variables
    "Exp"[0-9\']*        -> Exp 
    "Id"[0-9\']*         -> Id
    "Instr"[0-9\']*      -> Instr 
    "Instr-list"[0-9\']* -> {Instr ";"}+
    "Label"[0-9\']*      -> Label
    "Nat"[0-9\']*        -> Nat 
    "Stat"[0-9\']*       -> Stat
    "Stat+"[0-9\']*      -> {Stat ";"}+
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for
CodeGenerator}\label{CODE:codegenerator.sdf}
\end{figure}

    
\begin{figure}
\begin{IncCode}
\begin{boxedverbatim}
equations 

[1] tr(Stat-list, x) = <Instr-list, Label>
    ======================================
    tr(Stat-list) = Instr-list

[2] tr(Stat, Label) = <Instr-list1, Label'>, 
    tr(Stat-list, Label') = <Instr-list2, Label''>
    ================================================================== 
    tr(Stat ; Stat-list, Label) = <Instr-list1 ; Instr-list2, Label''>

[3] tr(Exp) = Instr-list
    ============================================================= 
    tr(Id := Exp, Label) = <lvalue Id; Instr-list; assign, Label>

[4] tr(Exp) = Instr-list1, tr(Stat-list, Label) = <Instr-list2, Label'>,
    Label'' = nextlabel(Label') 
    ================================================================ 
    tr(if Exp then Stat-list fi, Label) =
    <Instr-list1; gofalse Label''; Instr-list2; label Label'', Label''>

[5] tr(Exp) = Instr-list1, tr(Stat-list, Label) = <Instr-list2, Label'>,
    Label'' = nextlabel(Label'), Label''' = nextlabel(Label'') 
    ================================================================ 
    tr(while Exp do Stat-list od, Label) =
    <label Label''; Instr-list1; gofalse Label'''; Instr-list2;
     goto Label''; label Label''', Label'''>

[6] tr(Exp1) = Instr-list1, tr(Exp2) = Instr-list2
    =============================================== 
    tr(Exp1 + Exp2) = Instr-list1; Instr-list2; add

[7] tr(Exp1) = Instr-list1, tr(Exp2) = Instr-list2
    =============================================== 
    tr(Exp1 - Exp2) = Instr-list1; Instr-list2; sub

[8] tr(Exp1) = Instr-list1, tr(Exp2) = Instr-list2
    =============================================== 
    tr(Exp1 * Exp2) = Instr-list1; Instr-list2; mul

[9] tr(Nat) = push Nat 
[10] tr(Id) = rvalue Id
\end{boxedverbatim}
\end{IncCode}
\caption{\asfsdf\ specification for
CodeGenerator}\label{CODE:codegenerator.asf}
\end{figure}

\newpage
\subsection{Code Generation}

Consider a simple statement language (with assignment, if-statement and
while-statement) and suppose we want to compile this language to the following
stack machine code:

\begin{description}
\item{\tt push $N$} Push the number $N$. 
\item{\tt rvalue $I$} Push the contents of data location $I$.
\item{\tt lvalue $I$} Push the address of data location $I$. 
\item{\tt pop} Remove the top of the stack. 
\item{\tt copy} Push a copy of the top value on the stack. 
\item{\tt assign} The r-value on top of the stack is stored in the l-value
  below it and both are popped.
\item{\tt add, sub, mul}
Replace the two values on top of the stack by their sum (difference,
product). 
\item{\tt label $L$} Place a label (target of jumps). 
\item{\tt goto $L$} Next instruction is taken from statement following label
  $L$. 
\item{\tt gotrue $L$} Pop the top value; jump if it is nonzero.
\item{\tt  gofalse $L$}
Pop the top value; jump if it is zero.
\end{description}

\noindent The statement:
\begin{quote}
{\tt while a do a := a - 1; b := a * c od }
\end{quote}
will now be translated to the following instruction sequence:
\begin{verbatim}
label xx ; 
rvalue a ; 
gofalse xxx ; 
lvalue a ; 
rvalue a ; 
push 1 ;
sub ; 
assign ;
lvalue b ; 
rvalue a ; 
rvalue c ; 
mul ; 
assign ; 
goto xx ; 
label xxx
\end{verbatim}

\noindent Figure \ref{CODE:basicnotions} defines the 
sorts {\tt Nat} (numbers) and {\tt Id} (identifiers).
Given these basic notions the expressions and statements of our little 
source language are defined, see Figure \ref{CODE:expressions} and
\ref{CODE:statements}, respectively.






The instructions of the stack machine are defined in 
Figure \ref{CODE:assemblylanguage}.
Next, we define a function to construct a next label given the
previous one. It is defined on the lexical notion of labels ({\tt Label}). 
The scheme of appending the character `{\tt x}' to the previous label is, of
course, naive and will in real life be replaced by a more sophisticated one,
see Figure \ref{CODE:nextlabel} for the actual definition.





It remains to define a function `{\tt tr}' that translates statements into
instructions. During code generation we should generate new label names for
the translation of if- and while-statements. This is an instance of a
frequently occurring problem: how do we maintain global information (in this
case: the last label name generated)? A standard solution is to introduce an
auxiliary sort ({\tt Instrs-Lab}) that contains both the generated instruction
sequence and the last label name generated so far.
The Figures \ref{CODE:codegenerator.sdf} and \ref{CODE:codegenerator.asf} 
show the module containing this actual translation function.

This completes the specification of our code generator.


\subsection{Large \asfsdf\ Specifications}

There are a quite a few very large \asfsdf\ specifications around:

\begin{itemize}

\item The \asfsdf2C compiler.

\item A part of the parse table generator for \sdf.

\item The syntax and type checking of a domain specific language for
describing financial products.

\item A compiler from UML diagrams to various target languages (Progress, Java, DB2).

\item Transformation system for improving Cobol programs.

\item A system for Java refactoring.

\item Tooling for Action Semantics.

\item Tooling for Casl.

\end{itemize}

\newpage
\section{Compiling Specifications}
\label{compiling-specs}

By means of the button {\tt Compile ...} in the pop up menu for module
operations (Figure \ref{FIG:module-menu}) or the button pane the \asfsdf2C
compiler can be activated. C code is generated for the selected module
(exactly {\em one} module can be selected for compilation) including
the imported modules.

There are a number of requirements that have to be fulfilled in order
to be able to generate this C code:
\begin{itemize}
\item There should exist a directory in which the C code can be generated.
The name of this directory can be communicated via
\begin{itemize}
\item The environment variable {\tt COMPILER\_OUTPUT}.
\item The {\tt -c} option when starting the \ASmetaenv\ via {\tt meta}.
\item The dialog box presented in Figure \ref{FIG:compiler-dir}.
\end{itemize}

\item The specification should be complete, i.e. no missing modules are
allowed, and the equations sections should be error free.

\item Not all \sdf\ syntax features are yet supported by the compiler.
This poses several extra restrictions:

\begin{itemize}
\item the result symbols in the lexical syntax rules are restricted to
      plain basic sorts.
\item In the context-free syntax rules only literals, basic sorts, and
      lists (without the minimal repetition) are allowed. Furthermore
      lists are not allowed as result symbol.
\item Aliases are not supported by the compiler.
\end{itemize}

\end{itemize}

If these requirements are satisfied, C code can be generated.
This process of C code generation is described in detail in \cite{BKO99}.
We shall give a brief description of the main steps performed by the
\asfsdf2C compiler.

In \asfsdf\ there is {\em no} restriction in which module an equation
is defined, except that all applied syntax rules should be defined.
This freedom asks for a `reshuffling' of the equations given the set of
defined syntax rules. This reshuffling is needed because for each syntax
rule a separate C function is generated. This C function must contain
the C code for {\em all} equations with the C function as outermost
function symbol in the left-hand side.  By having all equations with the
same outermost function symbol in the left-hand side together an optimal
matching automaton can be generated.  Equations with the same outermost
function symbol are moved to the corresponding syntax rule.  This syntax
rule together with the equations is `stored' in an intermediate module
`$\mbox{\tt AUX-M}_{ij}$' where $\mbox{\tt M}_i$ is the module which
contained the definition of the syntax rule and the corresponding
equations, see Figure~\ref{CODE:reshuffled-module}.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
module AUX-Booleans2
exports               
  context-free syntax
    BOOL "|" BOOL -> BOOL {left}
equations
  [B1]   true | Bool   = true
  [B2]   false | Bool  = Bool
\end{boxedverbatim}
\end{IncCode}
\caption{Module generated after reshuffling}\label{CODE:reshuffled-module}
\end{figure}

These intermediate modules are input to the actual \asfsdf2C compiler.
For each $\mbox{\tt AUX-M}_{ij}$ module a separate C file is generated
in the {\tt COMPILER\_OUTPUT} directory, see
Figure~\ref{CODE:generated-c-code}.
The code presented in this figure is only a small part of the generated
code, there are some extra functions generated which take care of 
the administration of used functions in the code and how such a 
generated function can be used by other functions.

\begin{figure}[tb]
\begin{IncCode}
\begin{boxedverbatim}
ATerm lf_AUX_Booleans2_1 ( ATerm arg0 , ATerm arg1 ) {
FUNC_ENTRY ( lf_AUX_Booleans2_1sym , 
ATmakeAppl ( lf_AUX_Booleans2_1sym , arg0 , arg1 ) ) ;
if ( check_sym ( arg0 , ef1sym ) ) { FUNC_EXIT ( arg1 ) ; }
if ( check_sym ( arg0 , ef2sym ) ) { FUNC_EXIT ( arg0 ) ; }
FUNC_EXIT ( make_nf2 ( lf_AUX_Booleans2_1sym , arg0 , arg1 ) ) ;
}                       
\end{boxedverbatim}
\end{IncCode}
\caption{Generated C code for the reshuffled module of
Figure~\ref{CODE:reshuffled-module}}\label{CODE:generated-c-code}
\end{figure}

\newpage
\section{Technology and Architecture of the \ASmetaenv}\label{SEC:TechnologyandArchitecture}

So far, we have explained the functionality of the \ASmetaenv\ as an
interactive development environment for \asfsdf\ specifications.
There are, however, good reasons to have a look under the hood and
understand the architecture and technologies that have been used:

\begin{itemize}

\item Both architecture and technologies are very innovative
and it is worthwhile to learn about them.

\item The \ASmetaenv\ has been constructed as a collection of
cooperating components. All of these components have merits of their
own and can be used independently of the \ASmetaenv.

\end{itemize}

If you want to reuse components of the \ASmetaenv\ or want to build
variants of it, then the following information is for you.

\subsection{Technological Background}

\paragraph{ToolBus}

A hallmark of legacy systems in general and the old \ASmetaenv\ in particular is the 
entangling of control flow and actual computation.  To separate coordination from 
computation we use the
ToolBus coordination architecture~\cite{BK98}, a programmable software bus based
on process algebra.  Coordination is expressed by a formal description of the
cooperation protocol between components while computation is expressed in
components that may be written in any language. We thus obtain 
interoperability of heterogeneous components in a (possibly) distributed
system.

\paragraph{ATerms}

Coordination protocol and components have to share data. We use
ATerms~\cite{BJKO00} for this purpose. These are trees with optional annotations on
each node.  The annotations are used to store tool-specific information
like text coordinates or color attributes. The implementation of ATerms
has two essential properties: terms are stored using maximal subterm
sharing (reducing memory requirements and making deep equality tests
very efficient) and they can be exchanged using a very dense binary encoding that
preserves sharing. As a result very large terms (with over $1,000,000$ nodes)
can be processed.

\paragraph{SGLR}

In our language-centric approach the parser is an essential tool.  We
use scannerless, generalized-LR parsing~\cite{Vis97}.  In this way we
can parse arbitrary context-free grammars, an essential property when
combining and parsing large grammars for (dialects of) real-world
languages.

\paragraph{Term rewriting}

\asfsdf\ specifications are executed as (conditional) rewrite rules. Both
interpretation and compilation (using the ASF2C compiler~\cite{BKO99}) of
these rewrite rules are supported. The compiler generates very efficient
C code that implements pattern matching and term traversal. The generated
code uses ATerms as its main data representation, and ensures a minimal
use of memory during normalization of terms.



\subsection{Architecture}

\begin{figure}[tb]
\centerline{\psfig{file=ETAPS01/newarch.eps,width=8cm}}
\caption{\label{FIG:architecture}Architecture of the \ASmetaenv}
\vspace{-1.0\baselineskip}                  
\end{figure}

The architecture of the \ASmetaenv\ is shown in Figure~\ref{FIG:architecture}.
It consists of a ToolBus that interconnects the following components:

\begin{itemize}
\item {\bf User interface}: the top level user-interface of the system.
      It consists primarily of a graph browser for the import graph
      of the current specification.

\item {\bf Text Editor}: a customized version of \xemacs\ for text editing.
\item {\bf Structure Editor}: a syntax-directed editor that closely cooperates
with the Text Editor.

\item {\bf Parser}:  scannerless, generalized-LR parser (SGLR) that is parametrized 
with a parse table.
\item {\bf Parsetable generator}: takes an SDF syntax definition as input and 
generates a parse table for SGLR.
\item {\bf Tree Repository}: stores all terms corresponding to specification 
modules, parse tables, user-defined terms, etc.
\item {\bf Compiler}: the ASF2C compiler.
\item {\bf Interpreter}: executes specifiations by direct interpretation.
\item {\bf Unparser generator}: generates prettyprinters.
\end{itemize}

\newpage
\section{Selected Components of the \ASmetaenv}\label{SEC:Components}

Now we will describe a selection of components that are useful for
building stand-alone tools. \emph{Be aware that the information in the
following pages is still very much in a state of flux.}

Note that all components mentioned have a Unix-style manual page,
e.g., {\tt man sglr} gives the manual page for the command {\tt sglr}.
All components also have a {\tt -h} options that gives an overview of
their command line options.

\subsection{Parse Table Generation}

Before you can parse terms over a module {\tt M} outside the
\ASmetaenv\ you have to obtain a parse table for this module.  There
are currently two ways to achieve this:

\begin{itemize}

\item Interactively using the \ASmetaenv:

\begin{itemize}

\item Open a term editor for module {\tt M}.

\item Parse the contents of the editor by pressing the button {\tt
Parse Buffer}. If necessary this will generate the parse table.  Note
that all modules imported by {\tt M} are also used for generating the
parse table.

\item Press the {\tt Save} button on the file menu (see Figure~\ref{FIG:file-menu})
of module {\tt M}. The parse table is now saved as {\tt M.trm.tbl} in
the same directory as {\tt M.sdf}.

\end{itemize}

\item From the command line using the command {\tt sdf2table}.  You should
be aware that {\tt sdf2table} needs as input the \emph{complete} grammar
of {\tt M}. This means in particular that all imports of {\tt M} should
be expanded before {\tt sdf2table} can be used. Command line tools are
available to do this, but we will not describe them here.

For further information see the manual page for {\tt sdf2table}.

\end{itemize}


\subsection{Parsing} \label{SEC:parsingofterms}

Given a parse table {\tt M.trm.tbl} for module {\tt M},
terms can be parsed by using the command {\tt sglr}:

\begin{verbatim}
   sglr -m -p M.trm.tbl -i term.txt -o term.tree
\end{verbatim}
or, alternatively, via

\begin{verbatim}
   sglr -m -p M.trm.tbl < term.txt > term.tree
\end{verbatim}

The output of {\tt sglr} is either an error message (if the input
contains a syntax error) or a parse tree in a format called AsFixMe.\
Note that {\tt sglr} can generate parse trees in several formats:

\begin{itemize}
\item Use the flag {\tt -1} to generate AsFix1: this is an old parse
tree format that is in the process of being replaced.  \emph{Be aware that
compiled specifications currently still use this format: add the
option {\tt -1} if the term is to be rewritten using a compiled
specification (also see Section \ref{SEC:reducingofterms})}.

\item Use the flag {\tt -2} to generate AsFix2: this is the format
introduced for the latest version of \sdf.  This format is only in use
by some tools outside the \ASmetaenv.  AsFix2 is very verbose: even
lexical tokens and layout are represented as trees.  For efficiency
reasons, the \ASmetaenv\ uses the more concise format AsFixMe.

\item Use the flag {\tt -m} to generate AsFixMe: this is the preferred
parse tree format that is becoming the standard inside the \ASmetaenv.

\end{itemize}

For further information see the manual page for {\tt sglr}.

\subsection{Rewriting a Term using the Evaluator}
\label{SEC:interpretationgofterms}

In order to rewrite a term {\tt term.txt} using the \asfsdf\ evaluator {\tt asfe}
two inputs are required:

\begin{itemize}

\item The parse tree of {\tt term.txt}. See the previous paragraph how to do
this. The result is {\tt term.tree}.

\item The equations to be used for rewriting.  Select the desired
module and use {\tt Dump Equations...} (see Section~\ref{ButtonPane})
to dump the equations of the selected module and all its imports.
The results is a file named {\tt M.eqs}.

\end{itemize}

\begin{verbatim}
   asfe -e M.eqs <term.tree >reduct.tree 
\end{verbatim}  

Section~\ref{SEC:unparsing} explains how {\tt reduct.tree} can be
converted to a textual representation.

\subsection{Rewriting a Term using a Compiled Specification}
\label{SEC:reducingofterms}

In Section~\ref{compiling-specs} we explained how a given module {\tt M}
can be compiled into C code. Here we describe how to compile and
use this generated C code. The steps are as follows:

\begin{itemize}

\item Go to the directory where the \asfsdf2C compiler has generated
the C code.

\item Check whether there is a {\tt Makefile}. If this is not the case
or you expect it to be invalid, generate a new one as follows:
\begin{verbatim}
   genmakefile -m M > Makefile
\end{verbatim}
where {\tt M} is the name of the top module for which the C code was
generated.  There should also exist a file {\tt
ModuleName.module-list} in the directory with generated C code.


\item Use {\tt make} to compile the generated C code.
The result is both a library {\tt libM.a}
and an executable {\tt M}. This library can be used 
when several compiled specifications have to be combined into a single executable
or when compiled specifications have to be combined with with hand-written C code.

\item

After parsing the term that has to be reduced (as explained in
Section \ref{SEC:parsingofterms}), this term can
be reduced via the compiled code as follows:

\begin{verbatim}
COMPILER_OUTPUT/ModuleName < term.tree > reduct.tree
\end{verbatim}

Section~\ref{SEC:unparsing} explains how {\tt reduct.tree} can be
converted to a textual representation.
\end{itemize}

\subsection{Unparsing a (Parsed/Normalized) Term} \label{SEC:unparsing}

The unparsing of parsed/normalized terms is currently quite primitive.
It is achieved as follows:

\begin{verbatim}
   unparsePT < reduct.tree > reduct.txt
\end{verbatim}

\subsection{Applying a Function to a Term}

In many applications it is desirable to apply a function to a given term
before reducing it. A typical example is the type checking of a program: given
a parse tree {\tt T} for a program we first want to apply the typecheck
function {\tt tc} to it
before reduction. In the context of term rewriting this means first
constructing the term {\tt tc(T)} and then reducing it.

The construction of this new term is achieved by the following command:
\begin{verbatim}
   apply-function -f <name> -s <sort> -m <modulename> -i <ifile> -o <ofile> 
\end{verbatim}

A term is constructed consisting of an application of function {\tt <name>}
with result sort {\tt <sort>} defined in module {\tt <modulename>} to a term
{\tt <ifile>}.  The resulting term is written to {\tt <ofile> }.
In order to actually \emph{apply} {\tt <name>}, the term {\tt ofile} has to
be reduced.

\newpage 
\section{Examples of Stand-alone Tools}

\subsection{A Stand-alone Boolean Tool}

A stand-alone tool for parsing and reducing Boolean terms can be created in
the following steps:
\begin{itemize}

\item Goto the directory {\tt demo/pico}.

\item Start the \ASmetaenv.

\item Create a parse table for {\tt Pico-Booleans}:
  \begin{itemize}
  \item Right click on {\tt Pico-Booleans} and select {\tt New Term}.
    
  \item Enter the text {\tt true | false}.
  \item Save this text as {\tt term.txt}.
    
  \item Push the {\tt Parse} button in the {\tt Meta-Environment} menu of the
    editor.
  \item Push the {\tt Save} button of the {\tt File} menu of the \ASmetaenv.
  \end{itemize}
  The parse table {\tt Pico-Booleans.trm.tbl} has now been created.

\item Dump the equations for {\tt Pico-Booleans}:
  \begin{itemize}
  \item Right click on  {\tt Pico-Booleans} and select {\tt Dump Equations}.
  \end{itemize}
   The equations file {\tt Pico-Booleans.eqs} has now been created.

\item Parse {\tt term.txt}:
\begin{verbatim}
  sglr -p Pico-Booleans.trm.tbl -i term.txt -o term.tree
\end{verbatim}
The result is the parse tree {\tt term.tree}

\item Reduce {\tt term.tree}:
\begin{verbatim}
  asfe -e Pico-Booleans.eqs  <term.tree >reduct.tree   
\end{verbatim}

\item Unparse {\tt reduct.tree}:
\begin{verbatim}
  unparsePT <reduct.tree >reduct.txt
\end{verbatim}

The result (in textual form) of reducing {\tt term.txt} is now {\tt reduct.txt}
\end{itemize}

Of course, the last steps can be written more concisely in a pipeline:
\begin{verbatim}
  sglr -p Pico-Booleans.trm.tbl -i term.txt | \
  asfe -e Pico-Booleans.eqs | asource > reduct.txt
\end{verbatim}

\subsection{A Stand-alone Pico Typechecker}

Now we show how to create a stand-alone typechecker for the Pico language.  We
follow the same steps as in the previous example, but there is one additional
step required: given a parsed Pico program, we have to wrap the function
symbol {\tt tcp( )} around the Pico program, before we reduce the term.  The
steps are as follows:

\begin{itemize}
\item As before, generate a parse table for {\tt Pico-syntax} (result: {\tt
    Pico-syntax.trm.tbl}) and equations for {\tt Pico-typecheck} (result: {\tt
    Pico-typecheck.eqs}).
\item Parse the input term {\tt term.txt} (result: {\tt term.tree}).
\item Wrap  {\tt tcp(  )} around {\tt term.tree}:
\begin{verbatim}
  apply-function -f tcp                       \
                 -s PICO-BOOL                 \
                 -m Pico-typecheck            \
                 -i term.tree -o tcterm.tree
\end{verbatim}
\item Reduce {\tt tcterm.tree} and unparse the result as before.

\end{itemize}


\newpage
\section{Future Developments}
The \ASmetaenv\ is a system that is continuously evolving.  In this
section, we give a brief sketch of the developments that we expect in
the near future. Don't hesitate to give us feedback on these plans!

\subsection{Near Future (within 6 months)}

\begin{itemize}

\item Conversion of all tools to work with the new internal format AsFixMe.
      This will simplify several components since only support for a single
      format is needed.

\item Face lift of the toplevel user-interface. The current interface is written
      in Tcl/Tk. The new interface will be written in Java/Swing.
      Probably, we will detach the import graph viewer from the toplevel interface,
      but add it as an optional, separate view on the specification.

\item Integration of the TIDE debugging framework.  This will provide
      very sophisticated support for the debugging of, amongst others,
      \asfsdf\ specifications.

\item Integration of the pretty printer generator.

\item Overall optimizations.

\end{itemize}

\subsection{Medium Term (within 1.5 years)}

\begin{itemize}

\item Support for traversal functions in the compiler.

\item Support for origin tracking in evaluator and compiler.

\item New lexical primitives in \asfsdf\ that allow the detailed
      control of the output of transformations.

\item User-interface tailoring: make it possible to add button definitions to editors
      that can activate either specified functions or external tools.

\item Generation of stand-alone environments.

\item Support for representing and manipulating graphs.

\item Introduction of generic services like error messaging and online help.

\end{itemize}

\subsection{Long Term (more than 1.5 years)}

The long term ambition is to build a very flexible tool kit that can
be used to build many kinds of interactive systems for program
analysis and program transformation.  We currently target
domain-specific languages and software renovation as our main
application areas. Emphasis will be on:

\begin{itemize}

\item Enhancing the functionality, efficiency and reuasability of components.

\item Gradually defining a framework in which all these applications can be fit.
     
\item Carrying out case studies with as many different languages as possible
      to get feedback on the emerging framework.

\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{manual} 

\end{document}
