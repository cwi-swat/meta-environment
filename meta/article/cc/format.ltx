\documentclass{llncs}

\usepackage{latexsym}
\usepackage{a4}
\usepackage{ASF+SDF}
\usepackage{ASF+SDF-options}
%\usepackage{epsfig}
\usepackage{psfig}

\title{Compilation and Memory Management for ASF+SDF}
\author{Mark van den Brand\inst{1} and 
        Paul Klint\inst{1,2} and
        Pieter Olivier\inst{2}}
 
\institute{CWI,
           Department of Software Engineering,
           Kruislaan 413, NL-1098 SJ Amsterdam, The Netherlands
           \and
           University of Amsterdam,
           Programming Research Group,
           Kruislaan 403, NL-1098 SJ Amsterdam, The Netherlands
           \email{Mark.van.den.Brand@cwi.nl,Paul.Klint@cwi.nl,olivierp@wins.uva.nl}
}

\newcommand{\muasf}{$\mu\mbox{\sc Asf}$}
\newcommand{\asdf}{{\sc Asf+Sdf}}
\newcommand{\asf}{{\sc Asf}}
\newcommand{\sdf}{{\sc Sdf}}
\newcommand{\asfix}{{\sc AsFix}}
\newcommand{\epic}{{\sc Epic}}
\newcommand{\arm}{{\sc Arm}}
\newcommand{\rnx}{{\sc RNx}}
\newcommand{\asfc}{{\sc Asf+Sdf2c}}

\begin{document}
\maketitle

\begin{abstract}
Can formal specification techniques be scaled-up to industrial
problems such as the development of domain-specific languages and the
renovation of large COBOL systems?

We have developed a compiler for the specification formalism 
\asdf\ that has been used successfully to meet such industrial challenges.
This result is achieved in two ways: the compiler performs a variety
of optimizations and generates efficient C code, and the compiled code
uses a run-time memory management system based on maximal subterm
sharing and mark-and-sweep garbage collection.

We present an overview of these techniques and evaluate their
effectiveness in several benchmarks. It turns out that execution speed
of compiled \asdf\ specifications is at least as good as that of
comparable systems, while memory usage is in many cases an order
of magnitude smaller.

\end{abstract}

\section{Introduction}

Efficient implementation based on mainstream technology is a
prerequisite for the application and acceptance of declarative
languages or specification formalisms in real industrial settings.
The main characteristic of industrial applications is their
\emph{size} and the predominant implementation consideration should
therefore be the ability to handle huge problems.

In this paper we take the specification formalism \asdf\
\cite{BHK89,HHKR89.update,DHK96} as point of departure.  Its main
focus is on language prototyping and on the development of language
specific tools.  \asdf\ is based on general context-free grammars for
describing syntax and on conditional equations for describing
semantics.  In this way, one can easily describe the syntax of a (new
or existing) language and specify operations on programs in that
language such as static type checking, interpretation, compilation or
transformation.  \asdf\ has been applied successfully in a number of
industrial projects \cite{BDKKM96,VanDenBrandKlintVerhoef:98}, such as
the development of a domain-specific language for describing interest
products (in the financial domain) \cite{ADR95} and a renovation
factory for restructuring of COBOL code \cite{BSV97b}.  In such
industrial applications, the execution speed is very important, but
when processing huge COBOL programs memory usage becomes a critical
issue as well.  Other applications of \asdf\ include the development
of a GLR parser generator \cite{Vis97}, an unparser generator
\cite{BV96}, program transformation tools \cite{Bru96}, and the
compiler discussed in this paper.  Other components, such as parsers,
structure editors, and interpreters, are developed in \asdf\ as well
but are not (yet) compiled to C.

What are the performance standards one should strive for when writing
a compiler for, in our case, an algebraic specification formalism?
Experimental, comparative, studies are scarce, one notable exception
is \cite{HF96} where measurements are collected for various
declarative programs solving a single real-world problem.  In other
studies it is no exception that the units of measurement (rewrite
steps/second, or logical inferences/second) are ill-defined and that
memory requirements are not considered due to the small size of the
input problems.

In this paper, we present a compiler for \asdf\ that performs a
variety of optimizations and generates efficient C code. The compiled
code uses a run-time memory management system based on maximal subterm
sharing and mark-and-sweep garbage collection.  The contribution of
this paper is to bring the performance of executable specifications
based on term rewriting into the realm of industrial applications.

In the following two subsections we will now first give a quick
introduction to \asdf\ (the input language of the compiler to be
described) and to \muasf (the abstract intermediate representation
used internally by the compiler).  Next, we describe the generation of
C code (Section~\ref{Codegeneration}) as well as memory management
(Section~\ref{MemoryManagement}).  Section~\ref{Benchmarks} is devoted
to benchmarking. A discussion in Section~\ref{Conclusions} concludes
the paper.

\subsection{Specification Language: \asdf}

The specification formalism \asdf\ \cite{BHK89,HHKR89.update} is a
combination of the algebraic specification formalism \asf\ and the
syntax definition formalism \sdf. An overview can be found in
\cite{DHK96}. As an illustration, Figure~\ref{boolexmpl} presents the
definition of the Boolean datatype in \asdf.  \asdf\ specifications
consist of modules, each module has an \sdf-part (defining lexical and
context-free syntax) and an \asf-part (defining equations).  The \sdf\
part corresponds to signatures in ordinary algebraic specification
formalisms. However, syntax is not restricted to plain prefix notation
since arbitrary context-free grammars can be defined.  The syntax
defined in the \sdf-part of a module can be used immediately when
defining equations, the syntax in equations is thus \emph{user-defined}.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
{\NOAUTOHEADER  \input{./Booleans.mtx}}
\vspace{-1.7\baselineskip}
\end{minipage}
}
}
\caption{\label{boolexmpl} \asdf\ specification of the Booleans.}
\end{figure}

The emphasis in this paper will be on the compilation of the equations
appearing in a specification. They have the following distinctive features:

\begin{itemize}
\item Conditional equations with positive and negative conditions.
\item Non left-linear equations.
\item List matching.
\item Default equations.
\end{itemize}

It is possible to execute specifications by interpreting the equations
as conditional rewrite rules.  The semantics of \asdf\ is based on
innermost rewriting.  Default equations are tried when all other
applicable equations have failed, because either the arguments did not
match or one of the conditions failed.

One of the powerful features of the \asdf\ specification language is
list matching.  Figure \ref{setexmpl} shows a single equation which
removes multiple occurrences of identifiers from a set. In this
example, variables with a $^*$-superscript are list-variables that may
match zero or more identifiers.  The implementation of list matching
may involve backtracking to find a match that satisfies the left-hand side
of the rewrite rule as well as all its conditions.
There is only backtracking within the scope of a rewrite rule, 
so if the right-hand side of the rewrite rule is normalized and this
normalization fails {\em no} backtracking is performed to find a new match.

The development of \asdf\ specifications is supported by an interactive
programming environment, the \asdf\ Meta-Environment \cite{Kli93.meta}.
In this environment specifications can be developed and tested.
It provides syntax-directed editors, a parser generator, and a rewrite engine.
Given this rewrite engine terms can be reduced by interpreting the equations
as rewrite rules.
For instance, the term
\begin{quote}
{\tt true \& ( false | true )}
\end{quote}
reduces to {\tt true} when applying the equations of Figure
\ref{boolexmpl}.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
{\NOAUTOHEADER  \input{./Sets.mtx}}
\vspace{-1.7\baselineskip}
\end{minipage}
}
}
\caption{\label{setexmpl} \asdf\ specification of the Set equation.}
\end{figure}

\subsection{Intermediate Representation Language: \muasf}

The user-defined syntax that may be used in equations
poses two major  implementation challenges.

First, how do we represent \asdf\ specifications as parse trees?
Recall that there is no fixed grammar since the basic \asdf-grammar
can be extended by the user.  The solution we have adopted is to
introduce the intermediate format \asfix\ (\asdf\ fixed format) which
is used to represent the parse trees of the \asdf\ modules in a
machine processable format. The user-defined syntax is replaced by
prefix functions. The parse trees in the \asfix\ format are self
contained.

Second, how do we represent \asdf\ specifications in a more abstract
form that is suitable as compiler input?
We use a simplified
language \muasf\ as an intermediate representation to ease the
compilation process and to perform various transformations before
generating C code.
\muasf\ is in fact a single sorted (algebraic) specification formalism
using only prefix notation.
\muasf\ can be considered as the abstract syntax representation
of \asdf.
\asfix\ and \muasf\ live on different levels, \muasf\ is only visible
within the compiler whereas \asfix\ serves as exchange format between
the various components, such as structure editor, parser, and compiler.

A module in \muasf\ consists of a module name, a list of functions, and
a set of equations. The main differences
between \muasf\ and \asdf\ are:

\begin{itemize}

\item Only prefix functions are used.

\item The syntax is fixed (eliminating lexical and context-free definitions,
      priorities, and the like).

\item Lists are represented by binary list constructors instead
      of the built-in list construct as in \asdf; associative matching
      is used to implement list matching.

\item Functions are untyped, only their arity is declared.

\item Identifiers starting with capitals are variables;
      variable declarations are not needed.
\end{itemize}

Figure~\ref{boolexmpl2} shows the \muasf\ specification
corresponding to the \asdf\ specification of the Booleans
given earlier in Figure~\ref{boolexmpl}\footnote{To
increase the readability of the generated code in this paper, we have
consistently renamed generated names by more readable ones, like {\tt
true}, {\tt false}, etc.}.
Figure \ref{setexmpl2} shows the \muasf\ specification of sets given
earlier in Figure~\ref{setexmpl}. Note that this specification is not
left-linear since the variable \texttt{Id} appears twice on the
left-hand side of the equation.
The \texttt{\{list\}} function is used to mark that a term is a list.
This extra function is needed to distinguish between a single element list
and an ordinary term, e.g., \texttt{\{list\}(a)} \emph{versus} \texttt{a}
or \texttt{\{list\}(V)} \emph{versus} \texttt{V}.
An example of a transformation on \muasf\ specifications is shown in
Figure~\ref{setexmpl3}, where the non-left-linearity 
has been removed from  the specification in Figure \ref{setexmpl2}
by introducing new variables and an auxiliary condition.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt module Booleans}\\
{\tt si}\={\tt gnature}\\
\>{\tt true;}\\
\>{\tt false;}\\
\>{\tt and(\_,\_);}\\
\>{\tt or(\_,\_);}\\
\>{\tt xor(\_,\_);}\\
\>{\tt not(\_);}\\
{\tt rules}\\
\>{\tt and(true,B) = B;}\\
\>{\tt and(false,B) = false;}\\
\>{\tt or(true,B) = true;}\\
\>{\tt or(false,B) = B;}\\
\>{\tt not(true) = false;}\\
\>{\tt not(false) = true;}\\
\>{\tt xor(true,B) = not(B);}\\
\>{\tt xor(false,B) = B;}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{boolexmpl2} \muasf\ specification of the Booleans.}
\end{figure}

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt module Set}\\
{\tt si}\={\tt gnature}\\
\>{\tt \{list\}(\_);}\\
\>{\tt set(\_);}\\
\>{\tt conc(\_,\_);}\\
{\tt rules}\\
\>{\tt se}\={\tt t(\{list\}(conc(*Id0,conc(Id,conc(*Id1,conc(Id,*Id2)))))) =}\\
\>\>{\tt set(\{list\}(conc(*Id0,conc(Id,conc(*Id1,*Id2)))));}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{setexmpl2} \muasf\ specification of Set.}
\end{figure}

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt module Set}\\
{\tt si}\={\tt gnature}\\
\>{\tt \{list\}(\_);}\\
\>{\tt set(\_);}\\
\>{\tt conc(\_,\_);}\\
\>{\tt t;}\\
\>{\tt term-equal(\_,\_);}\\
{\tt rules}\\
\>{\tt term-equal(Id1,Id2) == t} \\
\>{\tt ==>}\\
\>{\tt se}\={\tt t(\{list\}(conc(*Id0,conc(Id1,conc(*Id1,conc(Id2,*Id2)))))) =}\\
\>\>{\tt set(\{list\}(conc(*Id0,conc(Id1,conc(*Id1,*Id2)))));}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{setexmpl3} Left-linear \muasf\ specification of Set.}
\end{figure}

\section{\label{Codegeneration}C Code Generation}

The \asf\ compiler uses \muasf\ as intermediate representation format
and generates C code as output.  The compiler consists of several
independent phases that gradually simplify and transform the \muasf\
specification and finally generate C code.

A number of transformations is performed to eliminate ``complex''
features such as removal of non left-linear rewrite rules,
simplification of matching patterns, and the introduction of
``assignment'' conditions (conditions that introduce new variable
bindings).  Some of these transformations are performed to improve the
efficiency of the resulting code whereas others are performed to
simplify code generation.

In the last phase of the compilation process C code is generated which
implements the rewrite rules in the specification using adaptations of
known techniques~\cite{Kap87a,Dik:89}.  Care is taken in constructing
an efficient matching automaton, identifying common and reusable
(sub)expressions, and efficiently implementing list matching.  For
each \muasf\ function (even the constructors) a separate C function is
generated. The right-hand side of an equation is directly translated
to a function call, if necessary. A detailed description of the construction
of the matching automaton is beyond the scope of this paper,
a full description of the construction of the matching automaton can
be found in \cite{BHKO99}. 
Each generated C function contains a small part of the matching automaton,
so instead of building one big automaton, the automaton is split over
the functions. The matching automaton respects the syntactic specificity
of the arguments from left to right in the left-hand sides of the equations.
Non-variable arguments are tried before the variable ones.

The datatype {\tt ATerm} (for Annotated Term) is the most important
datatype used in the generated C code. It is provided by a run-time
library which takes care of the creation, manipulation, and storage of
terms.
ATerms consist of a function symbol and
zero or more arguments, e.g., {\tt and(true,false)}.
The library provides predicates, such as {\tt check\_sym}
to check whether the function symbol of a term corresponds
to the given function symbol, and functions, like {\tt make\_nf}$i$ to
construct a term (normal form)
given a function symbol and $i$ arguments ($i \geq 0$).
There are also access functions to obtain the $i$-th argument ($i\geq 0$)  of
a term, e.g., {\tt arg\_1(and(true,false))} yields {\tt false}.

The usage of these term manipulation functions can be seen in
Figures~\ref{boolexmpl3} and~\ref{setexmpl4}.  Figure~\ref{boolexmpl3}
shows the C code generated for the \texttt{and} function of the
Booleans (also see Figures ~\ref{boolexmpl} and~\ref{boolexmpl2}).
This C code also illustrates the detection of reusable subexpressions.
In the second \texttt{if}-statement a check is made whether the first
argument of the \texttt{and}-function is equal to the term
\texttt{false}.  If the outcome of this test is positive, the first
argument \texttt{arg0} of the \texttt{and}-function is returned rather
than building a new normal form for the term {\tt false} or calling
the function {\tt false()}.
The last statement in Figure~\ref{boolexmpl3} is necessary to catch the
case that the first argument is neither a \texttt{true} or \texttt{false}
symbol, but some other Boolean normal form.

\begin{figure}[tb]
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt ATe}\={\tt rm}\=\ {\tt and(ATerm arg0, ATerm arg1) \{}\\
\>{\tt if (check\_sym(arg0, truesym))}\\
\>\>{\tt return arg1;}\\
\>{\tt if (check\_sym(arg0, falsesym))}\\
\>\>{\tt return arg0;}\\
\>{\tt return make\_nf2(andsym,arg0,arg1);}\\
{\tt \}}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{boolexmpl3} Generated C code for the {\tt and} function of the Booleans.}
\end{figure}


Figure \ref{setexmpl4} shows the C code generated for the Set example
of Figure \ref{setexmpl}.  List matching is translated into nested
while loops, this is possible because of the restricted nature of
the backtracking in list matching. 
The functions {\tt not\_empty\_list}, {\tt list\_head},
{\tt list\_tail}, {\tt conc}, and {\tt slice} are library functions
which give access to the C data structure which represents the \asdf\
lists.  In this way the generated C code needs no knowledge of the
internal list structure. We can even change the internal
representation of lists \emph{without adapting the generated C code}, by just
replacing the library functions.  The function {\tt term\_equal}
checks the equality of two terms.

\begin{figure}[tb]
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt ATe}\={\tt rm set(ATerm arg0) \{}\\
\>{\tt if}\={\tt (check\_sym(arg0,listsym)) \{}\\
\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 0}}$\ {\tt = arg\_0(arg0);}\\
\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [2];}\\
\>\>$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [0] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>{\tt wh}\={\tt ile(not\_empty\_list(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt )) \{}\\
\>\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 3}}$\ {\tt = list\_head(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}$\ {\tt = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [2];}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [0] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>\>{\tt wh}\={\tt ile(not\_empty\_list(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt )) \{}\\
\>\>\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 4}}$\ {\tt = list\_head(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt  = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>\>{\tt if}\={\tt (term\_equal(}$\mbox{\tt tmp}_{\mbox{\tt 3}}${\tt ,}$\mbox{\tt tmp}_{\mbox{\tt 4}}${\tt )) \{}\\
\>\>\>\>\>{\tt return set(list(conc(}\={\tt slice(}$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [0],}$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1]),}\\
\>\>\>\>\>\>{\tt conc(}\={\tt }$\mbox{\tt tmp}_{\mbox{\tt 3}}${\tt ,}\\
\>\>\>\>\>\>\>{\tt conc(slice(}$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [0],}$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1]),}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt )))));}\\
\>\>\>\>{\tt \}}\\
\>\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1] = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1]);}\\
\>\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt  =}\ $\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1];}\\
\>\>\>{\tt \}}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1] = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1]);}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt  =}\ $\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1];}\\
\>\>{\tt \}}\\
\>{\tt \}}\\
\>{\tt return make\_nf1(setsym,arg0);}\\
{\tt \}}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{setexmpl4} C code for the Set specification.}
\end{figure}

When specifications grow larger, \emph{separate compilation} becomes
mandatory.  There are two issues related to the separate compilation
of \asdf\ specifications that deserve special attention.
The first issue concerns the identification and linking of names
appearing in separately compiled modules. Essentially, this amounts to
the question how to translate the \asdf\ names into C names.  This
problem arises since a direct translation would generate names that
are too long for C compilers and linkage editors.  We have opted for a
solution in which each generated C file contains a ``register''
function which stores at run-time for each defined function defined in
this C file a
mapping between the address of the generated function 
and the original \asdf\ name. 
In addition, each C file contains a ``resolve'' function which connects
local function calls to the corresponding definitions based on their
\asdf\ names.
An example of registering and resolving can be found in Figure \ref{boolexmpl4}.

\begin{figure}[tb]
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt voi}\={\tt d register\_xor() \{}\\
\>{\tt xorsym = "prod(Bool "xor" Bool -> Bool \{left\})";}\\
\>{\tt register\_prod(}\={\tt "prod(Bool "xor" Bool -> Bool \{left\})",}\\
\>\>{\tt xor, xorsym);}\\
{\tt \}}\\
\\
{\tt voi}\={\tt d resolve\_xor() \{}\\
\>{\tt true = lookup\_func("prod("true" -> Bool)");}\\
\>{\tt truesym = lookup\_sym("prod("true" -> Bool)");}\\
\>{\tt false = lookup\_func("prod("false" -> Bool)");}\\
\>{\tt falsesym = lookup\_sym("prod("false" -> Bool)");}\\
\>{\tt not = lookup\_func("prod("not" Bool -> Bool)");}\\
\>{\tt notsym = lookup\_sym("prod("not" Bool -> Bool)");}\\
{\tt \}}\\
\\
{\tt ATe}\={\tt rm}\=\ {\tt xor(ATerm arg0, ATerm arg1) \{}\\
\>{\tt if (check\_sym(arg0, truesym))}\\
\>\>{\tt return (*not)(arg1);}\\
\>{\tt if (check\_sym(arg0, falsesym))}\\
\>\>{\tt return arg1;}\\
\>{\tt return make\_nf2(xorsym,arg0,arg1);}\\
{\tt \}}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{boolexmpl4} Generated C code for the {\tt xor} function of the Booleans.}
\end{figure}

The second issue concerns the choice of a unit for separate
compilation.  In most programming language environments, the basic
compilation unit is a file. For example, a C source file can be
compiled into an object file and several object files can be joined
by the linkage editor into a single executable.  If we change a
statement in one of the source files, that complete source file has to
be recompiled and linked with the other object files.

In the case of \asdf, the natural compilation unit would be the
module.  However, we want to generate a single C function for each
function in the specification (for efficiency reasons) but 
\asdf\ functions can be defined in specifications using multiple equations
occurring in several modules.  The solution is to use a single
function as compilation unit and to {\em re-shuffle} the equations
before translating the specification.  Equations are thus stored
depending on the module they occur in as well as on their outermost
function symbol.  When the user changes an equation, only those
functions that are actually affected have to be recompiled into C code. The
resulting C code is then compiled, and linked together with all other
previously compiled functions.

\section{\label{MemoryManagement}Memory Management}

At run-time, the main activities of compiled \asdf\ specifications are
the creation and matching of large amounts of terms.  Some of these
terms may even be very big (more than $10^6$ nodes).  The amount of
memory used during rewriting depends entirely on the number of terms
being constructed and on the amount of storage each term occupies.  In
the case of innermost rewriting a lot of redundant (intermediate)
terms are constructed.

At compile time, we can take various measures to
avoid redundant term creation (only the last two have been implemented
in the \asdf\ compiler):

\begin{itemize}

\item Postponing term construction.  Only the (sub)terms of the normal
form must be constructed, {\em all} other (sub)terms are only needed
to direct the rewriting process.  By transforming the specification
and extended it with rewrite rules that reflect the steering effect of
the intermediate terms, the amount of term construction can be
reduced. In the context of functional languages this technique is
known as \emph{deforestation}~\cite{Wadler1990}.  Its benefits for
term rewriting are not yet clear.

\item Local sharing of terms, only those terms are shared that result
from non-linear right-hand sides, e.g., {\tt f(X) = g(X,X)}.  Only
those terms will be shared of which the sharing can be established at
compile-time; the amount of sharing will thus be limited.  This
technique is also applied in ELAN~\cite{BorovanskyEtAl:96}.

\item Local reuse of terms, i.e., common subterms are only reduced
once and their normal form is reused several times.  Here again, the
common subterm has to be determined at compile-time.

\end{itemize}


\noindent At run-time, there are various other mechanisms to reduce the amount of work:

\begin{itemize}

\item Storage of all original terms to be rewritten and their
resulting normal forms, so that if the same term must be rewritten
again its normal form is immediately available.  The most obvious way
of storing this information is by means of pairs consisting of the
original term and the calculated normal form. However, even for small
specifications and terms an explosion of pairs may occur. The amount
of data to be manipulated makes this technique useless.

A more feasible solution is to store only the results of
functions that have been explicitly annotated by the user as
``memo-function'' (see Section \ref{Conclusions}).

\item Dynamic sharing of (sub)terms. This is the primary technique
we use and it is discussed in the next subsection.

\end{itemize}

\subsection{Maximal Sharing of Subterms}

Our strategy to minimize memory usage during rewriting is simple but
effective: we only create terms that are \emph{new}, i.e., that do not
exist already.  If a term to be constructed already exists, that term
is reused thus ensuring maximal sharing.  This strategy fully exploits
the redundancy that is typically present in the terms to be build
during rewriting.  The library functions to construct normal forms
take care of building shared terms whenever possible.  The sharing of
terms is invisible, so no extra precautions are necessary in the code
generated by the compiler.

Maximal sharing of terms can only be maintained when we check at every
term creation whether a particular term already exists or not. This
check implies a search through all existing terms but must nonetheless
be executed {\em extremely fast} in order not to impose an
unacceptable penalty on term creation.  Using a hash function that
depends on the internal code of the function symbol and the addresses
of its arguments, we
can quickly search for a function application before creating it.  The
(modest but not negligible) costs at term creation time are hence
one hash table lookup.

Fortunately, we get two returns on this investment.  First, the
considerably reduced memory usage also leads to reduced (real-time)
execution time.  Second, we gain substantially since the equality
check on terms ({\tt term\_equal}) becomes very cheap: it reduces from
an operation that is linear in the number of subterms to be compared
to a constant operation (pointer equality).  Note that the compiler
generates calls to {\tt term\_equal} in the translation of
patterns and conditions.

The idea of subterm sharing is known in the LISP community as
\emph{hash consing} and will be discussed below.

\subsection{Shared Terms versus Destructive Updates}

Terms can be shared in a number of places at the same time, therefore
they cannot be modified without causing unpredictable side-effects.
This means that all operations on terms should be \emph{functional}
and that terms should effectively be \emph{immutable} after creation.

During rewriting of terms by the generated code this restriction
causes no problems since terms are created in a fully functional way.
Normal forms are constructed bottom-up and there is no need to perform
destructive updates on a term once it has been constructed.
When normalizing an input term, this term is not modified, the normal
form is constructed independent of the input term. If we would modify
the input term we would get graph rewriting instead of (innermost) term
rewriting. Furthermore, the term library is very general and {\em not}
strictly devoted to rewriting, destructive updates would cause unwanted
side effects in components based on this term library.

However, destructive operations on lists, like list concatenation and
list slicing, become expensive. For instance, the most efficient way
to concatenate two lists is to physically replace one of the lists by
the concatenation result.  In our case, this effect can only be
achieved by taking the second list, prepending the elements of the
first list to it, and return the new list as result.

In LISP, the success of hash consing~\cite{Alan:78} has been limited
by the existence of the functions \texttt{rplaca} and \texttt{rplacd}
that can destructively modify a list structure. To support destructive
updates, one has to support two kinds of list structures ``mono copy''
lists with maximal sharing and ``multi copy'' lists without maximal
sharing.  Before destructively changing a mono copy list, it has to be
converted to a multi copy list. In the 1970's, E. Goto has
experimented with a Lisp dialect (HLisp) supporting hash consing and
list types as just sketched.  See~\cite{TerashimaKanada:90} for a
recent overview of this work and its applications.

In the case of the \asdf\ compiler, we \emph{generate} the code that
creates and manipulates terms and we can selectively generate code
that copies subterms in cases where the effect of a destructive update
is needed (as sketched above). This explains why we can apply the
technique of subterm sharing with more success.

\subsection{Reclaiming Unused Terms}

During rewriting, a large number of terms is created, most of which
will not appear in the end result. These terms are used as intermediate
results to guide the rewriting process. This means that terms that are
no longer used have to be reclaimed in some way.

After experimentation with various alternatives (reference counting,
mark-and-compact garbage collection) we have finally opted for a
mark--and-sweep garbage collection algorithm to reclaim unused terms.
Mark-and-sweep collection is more efficient,
both in time and space than reference counting \cite{JL96}.
The typical space overhead for a mark-sweep garbage collection algorithm is
only 1 bit per object.

Mark-and-sweep garbage collection works using three (sometimes two) phases.
In the first phase, all the objects on the heap are marked as `dead'.
In the second phase, all objects reachable from the known set of root
objects are marked as `live'. In the third phase, all `dead' objects
are swept into a list of free objects.

Mark-and-sweep garbage collection is also attractive, because it can be
implemented efficiently in C and can work without support from the
programmer or compiler \cite{BW88}. We have implemented a specialized
version of Boehm's conservative garbage collector~\cite{Boe93} that
exploits the fact that we are managing ATerms.

\section{\label{Benchmarks}Benchmarks}

Does maximal sharing of subterms lead to reductions in memory usage?
How does it affect execution speed?  Does the combination of techniques
presented in this paper indeed lead to an implementation of term
rewriting that scales-up to industrial applications?

To answer these questions, we present in Section~\ref{SmallBenchmarks}
three relatively simple benchmarks to compare our work with that of
other efficient functional and algebraic language implementations.
In Section~\ref{BigBenchmarks} we give measurements for some larger
\asdf\ specifications.

\subsection{\label{SmallBenchmarks}Three Small Benchmarks}
All three benchmarks are
based on symbolic evaluation of expressions $2 ^ n~{\bf mod}~17$, with
$14 \leq n \leq 30$.  A nice aspect of these expressions is that there
are many ways to calculate their value, giving ample opportunity to
validate the programs in the benchmark.  The actual source of the
benchmarks can be obtained at
\begin{quote}
{\tt
http://adam.wins.uva.nl/$\sim$olivierp/benchmark/index.html}.
\end{quote}

Note that these benchmarks were primarily designed to evaluate
\emph{specific} implementation aspects such as the effect of sharing,
lazy evaluation, and the like. They cannot (yet) be used to give an
overall comparison between the various systems.  Also note that some
systems failed to compute results for the complete range $14 \leq n
\leq 30$ in some benchmarks. In those cases, the corresponding graph
also ends prematurely.  Measurements were performed on an ULTRA
SPARC-IIi (300 MHz) with 576 Mb of memory.

So far we have used the following implementations
in our benchmarks:

\begin{itemize}

\item The \asdf\ compiler as discussed in this paper.
\item The Clean compiler developed at the University of Nijmegen~\cite{clean94}.
\item The ELAN compiler developed at INRIA, Nancy~\cite{BorovanskyEtAl:96}.
\item The Opal compiler developed at the Technische Universit\"{a}t
Berlin~\cite{DidrichEtAl:94}.
\item The Glasgow Haskell compiler~\cite{ghc93}.
\item The Standard ML compiler~\cite{AppelMacQueen:87}.
\end{itemize}


%%\begin{figure}[tb]
%%  \centerline{\psfig{figure=evalsym-tlog.eps}}
%%  \caption{\label{evalsym-tlog}Execution times for the {\tt evalsym} benchmark}
%%\end{figure}

\subsubsection{The {\tt evalsym} Benchmark}

The first benchmark is called {\tt evalsym} and uses an algorithm that
is CPU intensive, but does not use a lot of memory.  This benchmark is
a worst case for our implementation, because little can be gained by
maximal sharing.  The results are shown in 
Table \ref{evalsym-table}.
%%Figure \ref{evalsym-tlog}.
The differences between the various systems are indeed small.
Although, \asdf\ (with sharing) cannot benefit from maximal sharing, it does not
loose much either.

\begin{table}[tb]
\begin{center} 
\begin{tabular}{|l|r|} \hline
Compiler & Time (sec)\\ \hline \hline
Clean (strict)          & 32.3 \\ \hline
Sml                     & 32.9 \\ \hline 
Clean (lazy)            & 36.9 \\ \hline
\asdf{} (with sharing)  & 37.7 \\ \hline 
Haskell                 & 42.4 \\ \hline
Opal                    & 75.7 \\ \hline
\asdf{} (without sharing) & 190.4 \\ \hline 
Elan                    & 287.0 \\ \hline 
\end{tabular}
\vspace{\baselineskip}
\caption{\label{evalsym-table} The execution times for the evaluation of $2^{23}$.}
\end{center}
\end{table} 

\begin{figure}[tb]
  \centerline{\psfig{figure=evalexp-mlin.eps}}
  \caption{\label{evalexp-mem}Memory usage for the {\tt evalexp} benchmark}
\end{figure}

\begin{figure}[tb]
  \centerline{\psfig{figure=evalexp-tlog.eps}}
  \caption{\label{evalexp-tlog}Execution times for the {\tt evalexp} benchmark}
\end{figure}

\subsubsection{The {\tt evalexp} Benchmark}

The second benchmark is called {\tt evalexp} and is based on an algorithm
that uses a lot of memory when a typical eager (strict) implementation
is used. Using a lazy implementation, the amount of memory needed is
relatively small.


Memory usage is shown in Figure \ref{evalexp-mem}.  Clearly, normal
strict implementations cannot cope with the excessive memory
requirements of this benchmark. Interestingly, \asdf\ (with sharing)
has no problems whatsoever due to the use of maximal sharing, although
it is also based on strict evaluation

Execution times are plotted in Figure \ref{evalexp-tlog}.  Opal and
Clean (lazy) are faster than \asdf\ (with sharing) but the differences
are small.  The Haskell implementation is a bit slower, but Elan is
really lagging behind.

\begin{figure}[tb]
  \centerline{\psfig{figure=evaltree-mlin.eps}}
  \caption{\label{evaltree-mlin}Memory usage for the {\tt evaltree} benchmark}
\end{figure}

\begin{figure}[tb]
  \centerline{\psfig{figure=evaltree-tlog.eps}}
  \caption{\label{evaltree-tlog}Execution times for the {\tt evaltree} benchmark}
\end{figure}

\subsubsection{The {\tt evaltree} Benchmark}

The third benchmark is called {\tt evaltree} and is based on an
algorithm that uses a lot of memory both with lazy and eager
implementations.  Figure \ref{evaltree-mlin} shows that neither the
lazy nor the strict implementations can cope with the memory
requirements of this benchmark.  Only \asdf\ (with sharing) can keep
the memory requirements at an acceptable level due to its maximal
sharing.

The execution times plotted in Figure \ref{evaltree-tlog}
show that only \asdf\ scales-up for $n > 20$.

\subsection{\label{BigBenchmarks}Compilation Times of Larger \asdf{} Specifications}

Table \ref{ASF2Ctable} gives an overview of the compilation times of
four non-trivial \asdf{} specifications and their sizes in number of
equations, lines of \asdf{} specification, and generated C code. 
The \asdf{} compiler is the
specification of the \asdf{} to C compiler discussed in this paper.
The parser generator is an \asdf{} specification which generates a
parse table for an GLR-parser \cite{Vis97}. The COBOL formatter
is a pretty-printer for COBOL, this formatter is used within a renovation
factory for COBOL \cite{BSV97b}.
The Risla expander is an \asdf{} specification of a domain-specific language
for interest products, it expands modular Risla specifications into
``flat'' Risla specifications. These flat Risla specifications are later
compiled into COBOL code by an auxiliary tool.
The compilation times in the last column are produced by a
native C compiler (SUN's \texttt{cc}) with maximal optimizations.

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|} \hline
Specification & \asdf\      & \asdf    & Generated C  	& \asdf\	& C compiler\\
	      & (equations) & (lines)  & code (lines) 	& compiler (sec)& (sec)\\ \hline \hline
\asdf{} compiler & 1876     & 8699     & 85185    	& 216  		& 323 \\ \hline
Parser generator & 1388     & 4722     & 47662    	& 106  		& 192 \\ \hline
COBOL formatter  & 2037     & 9205     & 85976    	& 208  		& 374 \\ \hline
Risla expander   & 1082     & 7169     & 46787    	& 168  		& 531 \\ \hline
\end{tabular}
\vspace{\baselineskip}
\caption{\label{ASF2Ctable}Measurements of the \asdf\ compiler.}
\end{center}
\end{table}

Table \ref{sharingtable} gives an impression of the effect of maximal
sharing on execution time and memory usage of compiled 
\asdf\ specifications.  We show the results (with and without sharing) for
the compilation of the \asdf{} to C compiler itself and for the
expansion of a non-trivial Risla specification.

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|r|r|} \hline
Application 				& Time (sec) 	& Memory (Mb) \\ \hline \hline
\asdf{} compiler (with sharing)    	& 216		& 16	\\ \hline
\asdf{} compiler (without sharing) 	& 661  		& 117 	\\ \hline 
Risla expansion (with sharing)   	& 9  		& 8 	\\ \hline
Risla expansion (without sharing)  	& 18 		& 13	\\ \hline
\end{tabular}
\vspace{\baselineskip}
\caption{\label{sharingtable}Performance with and without maximal sharing.}
\end{center}
\end{table}

\section{Concluding Remarks}
\label{Conclusions}

We have presented the techniques for the compilation of \asdf\ to C,
with emphasis on memory management issues.  We conclude that
compiled \asdf\ specifications run with speeds comparable to that of
other systems, while memory usage is in some cases an order of
magnitude smaller.  We have mostly used and adjusted existing
techniques but their combination in the \asdf\ compiler turns out to
be very effective.

It is striking that our benchmarks show results that seem to
contradict previous observations in the context of
Sml~\cite{AppelGoncalves:93} where sharing resulted in slightly
increased execution speed and only marginal space savings.  On closer
inspection, we come to the conclusion that both methods for term
sharing are different and can not be compared easily.  We share terms
immediately when they are created: the costs are a table lookup and
the storage needed for the table while the benefits are space savings
due to sharing and a fast equality test (one pointer comparison).
In~\cite{AppelGoncalves:93} sharing of subterms is \emph{only}
determined during garbage collection in order to minimize the overhead
of a table lookup at term creation. This implies that local terms that
have not yet survived one garbage collection are not yet shared thus
loosing most of the benefits (space savings and fast equality test) as
well. The different usage patterns of terms in Sml and \asdf\ may also
contribute to these seemingly contradicting observations.

There are several topics that need further exploration.  First, we
want to study the potential of compile-time analysis for reducing the
amount of garbage that is generated at run-time.  Second, we have just
started exploring the implementation of \emph{memo-functions}.
Although the idea of memo-functions is rather old, they have not be
used very much in practice due to their considerable memory
requirements.  We believe that our setting of maximally shared
subterms will provide a new perspective on the implementation of
memo-functions. Finally, our ongoing concern is to achieve an even
further scale-up of prototyping based on term rewriting.

\section*{Acknowledgments}
The discussions with Jan Heering on \asdf\ compilation are much
appreciated.  The idea for the benchmarks in
Section~\ref{SmallBenchmarks} originates from Jan Bergstra.
Reference~\cite{AppelGoncalves:93} was pointed out to us by one of the
referees.

\bibliographystyle{plain}
\bibliography{metabib}

\end{document}
