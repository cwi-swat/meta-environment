\documentclass{llncs}

\usepackage{latexsym}
\usepackage{a4}
\usepackage{ASF+SDF}
\usepackage{ASF+SDF-options}
%\usepackage{epsfig}
\usepackage{psfig}

\title{Compilation and Memory Management for ASF+SDF}
\author{Mark van den Brand\inst{1} and 
        Paul Klint\inst{1,2} and
        Pieter Olivier\inst{2}}
 
\institute{CWI,
           Department of Software Engineering,
           Kruislaan 413, NL-1098 SJ Amsterdam, The Netherlands
           \and
           University of Amsterdam,
           Programming Research Group,
           Kruislaan 403, NL-1098 SJ Amsterdam, The Netherlands
           \email{Mark.van.den.Brand@cwi.nl,Paul.Klint@cwi.nl,olivierp@wins.uva.nl}
}

\newcommand{\muasf}{$\mu\mbox{\sc Asf}$}
\newcommand{\asdf}{{\sc Asf+Sdf}}
\newcommand{\asf}{{\sc Asf}}
\newcommand{\sdf}{{\sc Sdf}}
\newcommand{\asfix}{{\sc AsFix}}
\newcommand{\epic}{{\sc Epic}}
\newcommand{\arm}{{\sc Arm}}
\newcommand{\rnx}{{\sc RNx}}
\newcommand{\asfc}{{\sc Asf+Sdf2c}}

\begin{document}
\maketitle

\begin{abstract}
Can formal specification techniques be scaled-up to industrial
problems such as the development of domain-specific languages and the
renovation of large COBOL systems?

We have developed a compiler for the specification formalism 
\asdf\ that has been used successfully to meet such industrial challenges.
This result is achieved in two ways: the compiler performs a variety
of optimizations and generates efficient C code, and the compiled code
uses a run-time memory management system based on maximal subterm
sharing and mark-and-sweep garbage collection.

We present an overview of these techniques and evaluate their
effectiveness in several benchmarks. It turns out that execution speed
of compiled \asdf\ specifications is at least as good as that of
comparable systems, while memory usage is in many cases an order
of magnitude smaller.

\end{abstract}

\section{Introduction}

Efficient implementation based on mainstream technology is a
prerequisite for the application and acceptance of declarative
languages or specification formalisms in real industrial settings.
The main characteristic of industrial applications is their
\emph{size} and the predominant implementation consideration should
therefore be the ability to handle huge problems.

In this paper we take the specification formalism \asdf\
\cite{BHK89,HHKR89.update} as point of departure.  Its main focus is
on language prototyping \cite{DHK96} and on the development of
language specific tools, such as unparsers \cite{BV96} and program
transformation tools \cite{Bru96}.  \asdf\ is based on general
context-free grammars for describing syntax and on conditional
equations for describing semantics.  In this way, one can easily
describe the syntax of a (new or existing) language and specify
operations on programs in that language such as static type checking,
interpretation, compilation or transformation.  \asdf\ has been
applied successfully in a number of industrial projects
\cite{BDKKM96,VanDenBrandKlintVerhoef:98}, such as the development of
a domain-specific language for describing interest products (in the
financial domain) \cite{ADR95} and a renovation factory for
restructuring of COBOL code \cite{BSV97b}.  In such industrial
applications, the execution speed is very important, but when
processing huge COBOL programs memory usage becomes a critical issue
as well.  Other applications of \asdf\ include the development of a
GLR parser generator \cite{Vis97}, an unparser generator \cite{BV96},
and the compiler discussed in this paper.

What are the performance standards one should strive for when writing
a compiler for, in our case, an algebraic specification formalism?
Experimental, comparative, studies are scarce, one notable exception
is \cite{HF96} where measurements are collected for various
declarative programs solving a single real-world problem.  In other
studies it is no exception that the units of measurement (rewrite
steps/second, or logical inferences/second) are ill-defined and that
memory requirements are not considered due to the small size of the
input problems.

In this paper, we present a compiler for \asdf\ that performs a
variety of optimizations and generates efficient C code. The compiled
code uses a run-time memory management system based on maximal subterm
sharing and mark-and-sweep garbage collection.  The contribution of
this paper is to bring the performance of executable specifications
based on term rewriting into the realm of industrial applications.

In the following two subsections we will now first give a quick
introduction to \asdf\ (the input language of the compiler to be
described) and to \muasf (the abstract intermediate representation
used internally by the compiler).  Next, we describe the generation of
C code (Section~\ref{Codegeneration}) as well as memory management
(Section~\ref{MemoryManagement}).  Section~\ref{Benchmarks} is devoted
to benchmarking. A discussion in Section~\ref{Conclusions} concludes
the paper.

\subsection{Specification Language: \asdf}

The specification formalism \asdf\ \cite{BHK89,HHKR89.update} is a
combination of the algebraic specification formalism \asf\ and the
syntax definition formalism \sdf. An overview can be found in
\cite{DHK96}. As an illustration, Figure~\ref{boolexmpl} presents the
definition of the Boolean datatype in \asdf.  \asdf\ specifications
consist of modules, each module has an \sdf-part (defining lexical and
context-free syntax) and an \asf-part (defining equations).  The \sdf\
part corresponds to signatures in ordinary algebraic specification
formalisms. However, syntax is not restricted to plain prefix notation
since arbitrary context-free grammars can be defined.  The syntax
defined in the \sdf-part of a module can be used immediately when
defining equations, the syntax in equations is thus \emph{user-defined}.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
{\NOAUTOHEADER  \input{./Booleans.mtx}}
\vspace{-1.7\baselineskip}
\end{minipage}
}
}
\caption{\label{boolexmpl} \asdf\ specification of the Booleans.}
\end{figure}

The emphasis in this paper will be on the compilation of the equations
appearing in a specification. They have the following distinctive features:

\begin{itemize}
\item Conditional equations with positive and negative conditions.
\item Non left-linear equations.
\item List matching.
\item Default equations.
\end{itemize}

It is possible to execute specifications by interpreting the equations
as conditional rewrite rules.  The semantics of \asdf\ is based on
innermost rewriting.  Default equations are tried when all other
applicable equations have failed, because either the arguments did not
match or one of the conditions failed.

One of the powerful features of the \asdf\ specification language is
list matching.  Figure \ref{setexmpl} shows a single equation which
removes multiple occurrences of identifiers from a set. In this
example, variables with a $^*$-superscript are list-variables that may
match zero or more identifiers.  The implementation of list matching
may involve backtracking to find a match.

The development of \asdf\ specifications is supported by an interactive
programming environment, the \asdf\ Meta-Environment \cite{Kli93.meta}.
In this environment specifications can be developed and tested.
It provides syntax-directed editors, a parser generator, and a rewrite engine.
Given this rewrite engine terms can be reduced by interpreting the equations
as rewrite rules.
For instance, the term
\begin{quote}
{\tt true \& ( false | true )}
\end{quote}
reduces to {\tt true} when applying the equations of Figure
\ref{boolexmpl}.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
{\NOAUTOHEADER  \input{./Sets.mtx}}
\vspace{-1.7\baselineskip}
\end{minipage}
}
}
\caption{\label{setexmpl} \asdf\ specification of the Set equation.}
\end{figure}

\subsection{Intermediate Representation Language: \muasf}

The user-defined syntax that may be used in equations
poses two major  implementation challenges.

First, how do we represent \asdf\ specifications as parse trees?
Recall that there is no fixed grammar since the basic \asdf-grammar
can be extended by the user.  The solution we have adopted is to
introduce the intermediate format \asfix\ (\asdf\ fixed format) which
is used to represent the parse trees of the \asdf\ modules in a
machine processable format. The user-defined syntax is replaced by
prefix functions. The parse trees in the \asfix\ format are self
contained.

Second, how do we represent \asdf\ specifications in a more abstract
form that is suitable as compiler input?
We use a simplified
language \muasf\ as an intermediate representation to ease the
compilation process and to perform various transformations before
generating C code.
\muasf\ is in fact a single sorted (algebraic) specification formalism
using only prefix notation.
\muasf\ can be considered as the abstract syntax representation
of \asdf.

A module in \muasf\ consists of a module name, a list of functions, and
a set of equations. The main differences
between \muasf\ and \asdf\ are:

\begin{itemize}

\item Only prefix functions are used.

\item The syntax is fixed (eliminating lexical and context-free definitions,
      priorities, and the like).

\item Lists are represented by binary list constructors instead
      of the built-in list construct as in \asdf; associative matching
      is used to implement list matching.

\item Functions are untyped, only their arity is declared.

\item Identifiers starting with capitals are variables;
      variable declarations are not needed.
\end{itemize}

Figure~\ref{boolexmpl2} shows the \muasf\ specification
corresponding to the \asdf\ specification of the Booleans
given earlier in Figure~\ref{boolexmpl}\footnote{To
increase the readability of the generated code in this paper, we have
consistently renamed generated names by more readable ones, like {\tt
true}, {\tt false}, etc.}.
Figure \ref{setexmpl2} shows the \muasf\ specification of sets given
earlier in Figure~\ref{setexmpl}. Note that this specification is not
left-linear since the variable \texttt{Id} appears twice on the
left-hand side of the equation.
The \texttt{\{list\}} function is used to mark that a term is a list.
This extra function is needed to distinguish bewteen a single element list
and an ordinary term, e.g., \texttt{\{list\}(a)} vs.\ \texttt{a}
or \texttt{\{list\}(V)} vs.\ \texttt{V}.
An example of a transformation on \muasf\ specifications is shown in
Figure~\ref{setexmpl3}, where the non-left-linearity 
has been removed from  the specification in Figure \ref{setexmpl2}
by introducing new variables and an auxiliary condition.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt module Booleans}\\
{\tt si}\={\tt gnature}\\
\>{\tt true;}\\
\>{\tt false;}\\
\>{\tt and(\_,\_);}\\
\>{\tt or(\_,\_);}\\
\>{\tt not(\_);}\\
{\tt rules}\\
\>{\tt and(true,B) = B;}\\
\>{\tt and(false,B) = false;}\\
\>{\tt or(true,B) = true;}\\
\>{\tt or(false,B) = B;}\\
\>{\tt not(true) = false;}\\
\>{\tt not(false) = true;}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{boolexmpl2} \muasf\ specification of the Booleans.}
\end{figure}

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt module Set}\\
{\tt si}\={\tt gnature}\\
\>{\tt \{list\}(\_);}\\
\>{\tt set(\_);}\\
\>{\tt cons(\_,\_);}\\
{\tt rules}\\
\>{\tt se}\={\tt t(\{list\}(cons(*Id0,cons(Id,cons(*Id1,cons(Id,*Id2)))))) =}\\
\>\>{\tt set(\{list\}(cons(*Id0,cons(Id,cons(*Id1,*Id2)))));}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{setexmpl2} \muasf\ specification of Set.}
\end{figure}

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt module Set}\\
{\tt si}\={\tt gnature}\\
\>{\tt \{list\}(\_);}\\
\>{\tt set(\_);}\\
\>{\tt cons(\_,\_);}\\
\>{\tt t;}\\
\>{\tt term-equal(\_,\_);}\\
{\tt rules}\\
\>{\tt term-equal(Id1,Id2) == t} \\
\>{\tt ==>}\\
\>{\tt se}\={\tt t(\{list\}(cons(*Id0,cons(Id1,cons(*Id1,cons(Id2,*Id2)))))) =}\\
\>\>{\tt set(\{list\}(cons(*Id0,cons(Id1,cons(*Id1,*Id2)))));}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{setexmpl3} Left-linear \muasf\ specification of Set.}
\end{figure}

\section{\label{Codegeneration}C Code Generation}

The \asf\ compiler uses \muasf\ as intermediate representation format
and generates C code as output.  The compiler consists of several
independent phases that gradually simplify and transform the \muasf\
specification and finally generate C code.

A number of transformations is performed to eliminate ``complex''
features such as removal of non left-linear rewrite rules,
simplification of matching patterns, and the introduction of
``assignment'' conditions (conditions that introduce new variable
bindings).  Some of these transformations are performed to improve the
efficiency of the resulting code whereas others are performed to
simplify code generation.

In the last phase of the compilation process C code is generated which
implements the rewrite rules in the specification using adaptations of
known techniques~\cite{Kap87a,Dik:89}.  Care is taken in constructing
the most efficient matching automaton, identifying common and reusable
(sub)expressions, and efficiently implementing list matching.  For
each \muasf\ function (even the constructors) a separate C function is
generated. The right-hand side of an equation is directly translated
to a function call, if necessary.

The datatype {\tt ATerm} (for Annotated Term) is the most important
datatype used in the generated C code. It is provided by a run-time
library which takes care of the creation, manipulation, and storage of
terms.
ATerms consist of a function symbol and
zero or more arguments, e.g., {\tt and(true,false)}.
The library provides predicates, such as {\tt check\_sym}
to check whether the function symbol of a term corresponds
to the given function symbol, and functions, like {\tt make\_nf}$i$ to
construct a term (normal form)
given a function symbol and $i$ arguments ($i \geq 0$).
There are also access functions to obtain the $i$-th argument ($i\geq 0$)  of
a term, e.g., {\tt arg\_1(and(true,false))} yields {\tt false}.

The usage of these term manipulation functions can be seen in
Figures~\ref{boolexmpl3} and~\ref{setexmpl4}.  Figure~\ref{boolexmpl3}
shows the C code generated for the \texttt{and} function of the
Booleans (also see Figures ~\ref{boolexmpl} and~\ref{boolexmpl2}).
This C code also illustrates the detection of reusable subexpressions.
In the second \texttt{if}-statement a check is made whether the first
argument of the \texttt{and}-function is equal to the term
\texttt{false}.  If the outcome of this test is positive, the first
argument \texttt{arg1} of the \texttt{and}-function is returned rather
than building a new normal form for the term {\tt false} or calling
the function {\tt false()}.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt ATe}\={\tt rm}\=\ {\tt and(ATerm arg0, ATerm arg1) \{}\\
\>{\tt if (check\_sym(arg0, truesym))}\\
\>\>{\tt return arg1;}\\
\>{\tt if (check\_sym(arg0, falsesym))}\\
\>\>{\tt return arg0;}\\
\>{\tt return make\_nf2(andsym,arg0,arg1);}\\
{\tt \}}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{boolexmpl3} Generated C for {\tt and} function of the Booleans.}
\end{figure}


Figure \ref{setexmpl4} shows the C code generated for the Set example
of Figure \ref{setexmpl}.  List matching is translated into nested
while loops.  The functions {\tt not\_empty\_list}, {\tt list\_head},
{\tt list\_tail}, {\tt cons}, and {\tt slice} are library functions
which give access to the C data structure which represents the \asdf\
lists.  In this way the generated C code needs no knowledge of the
internal list structure. We can even change the internal
representation of lists \emph{without adapting the C code}, by just
replacing the library functions.  The function {\tt term\_equal}
checks the equality of two terms.

\begin{figure}
\centerline{\fbox{
\begin{minipage}[tb]{\textwidth}
\begin{tabbing}
{\tt ATe}\={\tt rm set(ATerm arg0) \{}\\
\>{\tt if}\={\tt (check\_sym(arg0,listsym)) \{}\\
\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 0}}$\ {\tt = arg\_0(arg0);}\\
\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [2];}\\
\>\>$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [0] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>{\tt wh}\={\tt ile(not\_empty\_list(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt )) \{}\\
\>\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 3}}$\ {\tt = list\_head(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}$\ {\tt = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [2];}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [0] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1] =}\ $\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt ;}\\
\>\>\>{\tt wh}\={\tt ile(not\_empty\_list(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt )) \{}\\
\>\>\>\>{\tt ATerm }$\mbox{\tt tmp}_{\mbox{\tt 4}}$\ {\tt = list\_head(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt  = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt );}\\
\>\>\>\>{\tt if}\={\tt (term\_equal(}$\mbox{\tt tmp}_{\mbox{\tt 3}}${\tt ,}$\mbox{\tt tmp}_{\mbox{\tt 4}}${\tt )) \{}\\
\>\>\>\>\>{\tt return set(list(cons(}\={\tt slice(}$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [0],}$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1]),}\\
\>\>\>\>\>\>{\tt cons(}\={\tt }$\mbox{\tt tmp}_{\mbox{\tt 3}}${\tt ,}\\
\>\>\>\>\>\>\>{\tt cons(slice(}$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [0],}$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1]),}$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt )))));}\\
\>\>\>\>{\tt \}}\\
\>\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1] = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1]);}\\
\>\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt  =}\ $\mbox{\tt tmp}_{\mbox{\tt 2}}${\tt [1];}\\
\>\>\>{\tt \}}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1] = list\_tail(}$\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1]);}\\
\>\>\>$\mbox{\tt tmp}_{\mbox{\tt 0}}${\tt  =}\ $\mbox{\tt tmp}_{\mbox{\tt 1}}${\tt [1];}\\
\>\>{\tt \}}\\
\>{\tt \}}\\
\>{\tt return make\_nf1(setsym,arg0);}\\
{\tt \}}
\end{tabbing}
\end{minipage}
}
}
\caption{\label{setexmpl4} C code for the Set specification.}
\end{figure}

When specifications grow larger, \emph{separate compilation} becomes
mandatory.  There are two issues related to the separate compilation
of \asdf\ specifications that deserve special attention.
The first issue concerns the identification and linking of names
appearing in separately compiled modules. Essentially, this amounts to
the question how to translate the \asdf\ names into C names.  This
problem arises since a direct translation would generate names that
are too long for C compilers and linkage editors.  We have opted for a
solution in which each generated C file contains a ``register''
function which stores at run-time for each locally defined function a
mapping between the derived C name and the original \asdf\ name. In
addition, each C file contains a ``resolve'' function which connects
local function calls to the corresponding definitions based on their
\asdf\ names.

The second issue concerns the choice of a unit for separate
compilation.  In most programming language environments, the basic
compilation unit is a file. For example, a C source file can be
compiled into an object file and several object files can be joined
by the linkage editor into a single executable.  If we change a
statement in one of the source files, that complete source file has to
be recompiled and linked with the other object files.

In the case of \asdf, the natural compilation unit would be the
module.  However, we want to generate a single C function for each
function in the specification (for efficiency reasons) but in \asdf\
specification functions can be defined using multiple equations
occurring in several modules.  The solution is to use a single
function as compilation unit and to {\em re-shuffle} the equations
before translating the specification.  Equations are thus stored
depending on the module they occur in as well as on their outermost
function symbol.  When the user changes an equation, only those
functions that are actually affected can be recompiled to C code. The
resulting C code is then compiled, and linked together with all other
compiled functions.

\section{\label{MemoryManagement}Memory Management}

At run-time, the main activities of compiled \asdf\ specifications are
the creation and matching of large amounts of terms.  Some of these
terms may even be very big (more than $10^6$ nodes).  The amount of
memory used during rewriting depends entirely on the number of terms
being constructed and on the amount of storage each term occupies.  In
the case of innermost rewriting a lot of redundant (intermediate)
terms are constructed.

At compile time, we can take various measures to
avoid redundant term creation (only the last two have been implemented
in the \asdf\ compiler):

\begin{itemize}

\item Postponing term construction.  Only the (sub)terms of the normal
form must be constructed, {\em all} other (sub)terms are only needed
to direct the rewriting process.  By transforming the specification
and extended it with rewrite rules that reflect the steering effect of
the intermediate terms, the amount of term construction can be
reduced. In the context of functional languages this technique is
known as \emph{deforestation}~\cite{Wadler1990}.  Its benefits for
term rewriting are not yet clear.

\item Local sharing of terms, only those terms are shared that result
from non-linear right-hand sides, e.g., {\tt f(X) = g(X,X)}.  Only
those terms will be shared of which the sharing can be established at
compile-time; the amount of sharing will thus be limited.  This
technique is also applied in ELAN~\cite{BorovanskyEtAl:96}.

\item Local reuse of terms, i.e., common subterms are only reduced
once and their normal form is reused several times.  Here again, the
common subterm has to be determined at compile-time.

\end{itemize}


\noindent At run-time, there are various other mechanisms to reduce the amount of work:

\begin{itemize}

\item Storage of all original terms to be rewritten and their
resulting normal forms, so that if the same term must be rewritten
again its normal form is immediately available.  The most obvious way
of storing this information is by means of pairs consisting of the
original term and the calculated normal form. However, even for small
specifications and terms an explosion of pairs may occur. The amount
of data to be manipulated makes this technique useless.

A more feasible solution is to store only the results of
functions that have been explicitly annotated by the user as
``memo-function'' (see Section \ref{Conclusions}).

\item Dynamic sharing of (sub)terms. This is the primary technique
we use and it is discussed in the next subsection.

\end{itemize}

\subsection{Maximal Sharing of Subterms}

Our strategy to minimize memory usage during rewriting is simple but
effective: we only create terms that are \emph{new}, i.e., that do not
exist already.  If a term to be constructed already exists, that term
is reused thus ensuring maximal sharing.  This strategy fully exploits
the redundancy that is typically present in the terms to be build
during rewriting.  The library functions to construct normal forms
take care of building shared terms whenever possible.  The sharing of
terms is invisible, so no extra precautions are necessary in the code
generated by the compiler.

Maximal sharing of terms can only be maintained when we check at every
term creation whether a particular term already exists or not. This
check implies a search through all existing terms but must nonetheless
be executed {\em extremely fast} in order not to impose an
unacceptable penalty on term creation.  Using a hash function that
depends on the addresses of the function symbol and its arguments, we
can quickly search for a function application before creating it.  The
(modest but not negligible) costs at term creation time are hence
one hash table lookup.

Fortunately, we get two returns on this investment.  First, the
considerably reduced memory usage also leads to reduced (real-time)
execution time.  Second, we gain substantially since the equality
check on terms ({\tt term\_equal}) becomes very cheap: it reduces from
an operation that is linear in the number of subterms to be compared
to a constant operation (pointer equality).  Note that the compiler
generates calls to {\tt term\_equal} in the translation of
patterns and conditions.

The idea of subterm sharing is known in the LISP community as
\emph{hash consing}~\cite{Alan:78} and will be discussed below.

\subsection{Shared Terms versus Destructive Updates}

Terms can be shared in a number of places at the same time, therefore
they cannot be modified without causing unpredictable side-effects.
This means that all operations on terms should be \emph{functional}
and that terms should effectively be \emph{immutable} after creation.

During rewriting of terms by the generated code this restriction
causes no problems since terms are created in a fully functional way.
Normal forms are constructed bottom-up and there is no need to perform
destructive updates on a term once it has been constructed.

However, destructive operations on lists, like list concatenation and
list slicing, become expensive. For instance, the most efficient way
to concatenate two lists is to physically replace one of the lists by
the concatenation result.  In our case, this effect can only be
achieved by taking the second list, prepending the elements of the
first list to it, and return the new list as result.

In LISP, the success of hash consing has been limited by the existence
of the functions \texttt{rplaca} and \texttt{rplacd} that can
destructively modify a list structure. To support destructive updates,
one has to support two kinds of lists structures ``mono copy'' lists
with maximal sharing and ``multi copy'' lists without maximal sharing.
Before destructively changing a mono copy list, it has to be converted
to a multi copy list.

In the case of the \asdf\ compiler, we \emph{generate} the code that
creates and manipulates terms and we can selectively generate code
that copies subterms in cases where the effect of a destructive update
is needed (as sketched above). This explains why we can apply the
technique of subterm sharing with more success.

\subsection{Reclaiming Unused Terms}

During rewriting, a large number of terms is created, most of which
will not appear in the end result. These terms are used as intermediate
results to guide the rewriting process. This means that terms that are
no longer used have to be reclaimed in some way.

After experimentation with various alternatives (reference counting,
mark-and-compact garbage collection) we have finally opted for a
mark--and-sweep garbage collection algorithm to reclaim unused terms.
Mark-and-sweep collection is more efficient,
both in time and space than reference counting \cite{JL96}.
The typical space overhead for a mark-sweep garbage collection algorithm is
only 1 bit per object.

Mark-and-sweep garbage collection works using three (sometimes two) phases.
In the first phase, all the objects on the heap are marked as `dead'.
In the second phase, all objects reachable from the known set of root
objects are marked as `live'. In the third phase, all `dead' objects
are swept into a list of free objects.

Mark-and-sweep garbage collection is also attractive, because it can be
implemented efficiently in C and can work without support from the
programmer or compiler \cite{BW88}. We have implemented a specialized
version of Boehm's conservative garbage collector~\cite{Boe93} that
exploits the fact that we are managing ATerms.

\section{\label{Benchmarks}Benchmarks}

 
To give an impression of the actual performance of the \asfc\ compiler 
the sizes and the compilation times of a number of non-trivial \asdf\
specifications are given in Table \ref{ASF2Ctable}.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|} \hline
Specification      & Nr of  & Lines of & Lines of & \asdf    & C \\
                 & eqs    & \asdf  & C code   & compiler   & compiler \\ \hline \hline
Compiler         & 1748   & 7972     & 75578    & 186 s  & 263 s \\ \hline
Parser generator & 1365   & 4586     & 49649    & 203 s  & 168 s \\ \hline
COBOL formatter  & 1857   & 8387     & 76788    & 371 s  & 339 s \\ \hline
%%Risla flattener  & 1258   & 16453    & 76664    & 324 s  & 759 s \\ \hline
\end{tabular}
\caption{\label{ASF2Ctable}Measurements of the \asfc\ compiler.}
\end{table}

Does maximal sharing of subterms lead to reductions in memory usage?
How does it affect execution speed?  Does the combination of techniques
presented in this paper indeed lead to an implementation of term
rewriting that scales-up to industrial applications?

To answer these questions, we have developed three relatively simple
benchmarks to compare our work with that of other efficient functional
and algebraic language implementations.  All three benchmarks are
based on symbolic evaluation of expressions $2 ^ n~{\bf mod}~17$, with
$14 \leq n \leq 30$.  A nice aspect of these expressions is that there
are many ways to calculate their value, giving ample opportunity to
validate the programs in the benchmark.  The actual source of the
benchmarks can be obtained at
\begin{quote}
{\tt
http://adam.wins.uva.nl/$\sim$olivierp/benchmark/index.html}.
\end{quote}

Note that these benchmarks were primarily designed to evaluate
\emph{specific} implementation aspects such as the effect of sharing,
lazy evaluation, and the like. They cannot (yet) be used to give an
overall comparison between the various systems.  Also note that some
systems failed to compute results for the complete range $14 \leq n
\leq 30$ in some benchmarks. In those cases, the corresponding graph
also ends prematurely.  Measurements were performed on an ULTRA
SPARC-IIi (300 MHz) with 576 Mb of memory.

So far we have used the following implementations
in our benchmarks:

\begin{itemize}

\item The \asdf\ compiler as discussed in this paper.
\item The Clean compiler developed at the University of Nijmegen~\cite{clean94}.
\item The ELAN compiler developed at INRIA, Nancy~\cite{BorovanskyEtAl:96}.
\item The Opal compiler developed at the Technische Universit\"{a}t
Berlin~\cite{DidrichEtAl:94}.
\item The Glasgow Haskell compiler~\cite{ghc93}.
\end{itemize}





\begin{figure}[tb]
  \centerline{\psfig{figure=evalsym-tlog.eps}}
  \caption{\label{evalsym-tlog}Execution times for the {\tt evalsym} benchmark}
\end{figure}

\subsection{The {\tt evalsym} benchmark}

The first benchmark is called {\tt evalsym} and uses an algorithm that
is CPU intensive, but does not use a lot of memory.  This benchmark is
a worst case for our implementation, because little can be gained by
maximal sharing.  The results are shown in Figure \ref{evalsym-tlog}.
The differences between the various systems are indeed small.
Although, \asdf\ cannot benefit from maximal sharing, it does not
loose much either.

\begin{figure}[tb]
  \centerline{\psfig{figure=evalexp-mlin.eps}}
  \caption{\label{evalexp-mem}Memory usage for the {\tt evalexp} benchmark}
\end{figure}

\begin{figure}[tb]
  \centerline{\psfig{figure=evalexp-tlog.eps}}
  \caption{\label{evalexp-tlog}Execution times for the {\tt evalexp} benchmark}
\end{figure}

\subsection{The {\tt evalexp} benchmark}

The second benchmark is called {\tt evalexp} and is based on an algorithm
that uses a lot of memory when a typical eager (strict) implementation
is used. Using a lazy implementation, the amount of memory needed is
relatively small.


Memory usage is shown in Figure \ref{evalexp-mem}.  Clearly, normal
strict implementations cannot cope with the excessive memory
requirements of this benchmark. Interestingly, \asdf\ has no problems
whatsoever due to the use of maximal sharing, although it is also
based on strict evaluation

Execution times are plotted in Figure \ref{evalexp-tlog}.  Opal and
Clean(lazy) are faster than \asdf\ but the differences are small.  The
Haskell implementation is somewhat lagging behind.

\begin{figure}[tb]
  \centerline{\psfig{figure=evaltree-mlin.eps}}
  \caption{\label{evaltree-mlin}Memory usage for the {\tt evaltree} benchmark}
\end{figure}

\begin{figure}[tb]
  \centerline{\psfig{figure=evaltree-tlog.eps}}
  \caption{\label{evaltree-tlog}Execution times for the {\tt evaltree} benchmark}
\end{figure}

\subsection{The {\tt evaltree} benchmark}

The third benchmark is called {\tt evaltree} and is based on an
algorithm that uses a lot of memory both with lazy and eager
implementations.  Figure \ref{evaltree-mlin} shows that neither the
lazy nor the strict implementations can cope with the memory
requirements of this benchmark.  Only \asdf\ can keep the memory
requirements at an acceptable level due to its maximal sharing.

The execution times plotted in Figure \ref{evaltree-tlog}
show that only \asdf\ scales-up for $n > 20$.

\section{Concluding remarks}
\label{Conclusions}

We have presented the techniques for the compilation of \asdf\ to C,
with emphasis on memory management issues.  We conclude that
compiled \asdf\ specifications run with speeds comparable to that of
other systems, while memory usage is in some cases an order of
magnitude smaller.  We have mostly used and adjusted existing
techniques but their combination in the \asdf\ compiler turns out to
be very effective.

There are several topics that need further exploration.  First, we
want to study the potential of compile-time analysis for reducing the
amount of garbage that is generated at run-time.  Second, we have just
started exploring the implementation of \emph{memo-functions}.
Although the idea of memo-functions is rather old, they have not be
used very much in practice due to their considerable memory
requirements.  We believe that our setting of maximally shared
subterms will provide a new perspective on the implementation of
memo-functions. Finally, our ongoing concern is to achieve an even
further scale-up of prototyping based on term rewriting.

{\bf PM}: The maximal sharing prevents a cheap clean up of local variables
when exiting a function. This is an observation made by Appel (according
to Jan Heering). He does not share the local variables, but this prevents
a cheap implementation of the \texttt{term-equal}.

\section*{Acknowledgments}
The discussions with Jan Heering on \asdf\ compilation are much
appreciated.  The idea for the benchmarks in Section~\ref{Benchmarks}
originates from Jan Bergstra.


\bibliographystyle{plain}
\bibliography{/home/gipe/lib/tex/LanguageProto,metabib}
%\bibliography{metabib}

\end{document}
